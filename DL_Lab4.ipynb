{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DL_Lab4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericburdett/cs474_labs_f2019/blob/master/DL_Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LjLT357VGND",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericburdett/cs474_labs_f2019/blob/master/DL_Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# Lab 4: Cancer Detection\n",
        "\n",
        "## Objective\n",
        "* To build a dense prediction model\n",
        "* To begin reading current papers in DNN research\n",
        "\n",
        "## Deliverable\n",
        "For this lab, you will turn in a notebook that describes your efforts at creating\n",
        "a pytorch radiologist. Your final deliverable is a notebook that has (1) deep network,\n",
        "(2) cost function, (3) method of calculating accuracy,\n",
        "(4) an image that shows the dense prediction produced by your network on the pos_test_000072.png image.\n",
        "This is an image in the test set that your network will not have seen before.\n",
        "This image, and the ground truth labeling, is shown below.\n",
        "(And is contained in the downloadable dataset below).\n",
        "\n",
        "![](http://liftothers.org/dokuwiki/lib/exe/fetch.php?w=200&tok=a8ac31&media=cs501r_f2016:pos_test_000072_output.png)\n",
        "<img src=\"http://liftothers.org/dokuwiki/lib/exe/fetch.php?media=cs501r_f2016:pos_test_000072.png\" width=\"200\">\n",
        "\n",
        "\n",
        "## Grading standards\n",
        "Your notebook will be graded on the following:\n",
        "* 40% Proper design, creation and debugging of a dense prediction network\n",
        "* 40% Proper implementation of a loss function and train/test set accuracy measure\n",
        "* 10% Tidy visualizations of loss of your dense predictor during training\n",
        "* 10% Test image output\n",
        "\n",
        "\n",
        "## Data set\n",
        "The data is given as a set of 1024×1024 PNG images. Each input image (in \n",
        "the ```inputs``` directory) is an RGB image of a section of tissue,\n",
        "and there a file with the same name (in the ```outputs``` directory) \n",
        "that has a dense labeling of whether or not a section of tissue is cancerous\n",
        "(white pixels mean “cancerous”, while black pixels mean “not cancerous”).\n",
        "\n",
        "The data has been pre-split for you into test and training splits.\n",
        "Filenames also reflect whether or not the image has any cancer at all \n",
        "(files starting with ```pos_``` have some cancerous pixels, while files \n",
        "starting with ```neg_``` have no cancer anywhere).\n",
        "All of the data is hand-labeled, so the dataset is not very large.\n",
        "That means that overfitting is a real possibility.\n",
        "\n",
        "## Description\n",
        "For a video including some tips and tricks that can help with this lab: [https://youtu.be/Ms19kgK_D8w](https://youtu.be/Ms19kgK_D8w)\n",
        "For this lab, you will implement a virtual radiologist.\n",
        "You are given images of possibly cancerous tissue samples, \n",
        "and you must build a detector that identifies where in the tissue cancer may reside.\n",
        "\n",
        "---\n",
        "\n",
        "### Part 0\n",
        "Watch and follow video tutorial:\n",
        "\n",
        "https://youtu.be/Ms19kgK_D8w\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "**DONE:**\n",
        "* Watch tutorial\n",
        "\n",
        "### Part 1\n",
        "Implement a dense predictor\n",
        "\n",
        "In previous labs and lectures, we have talked about DNNs that classify an \n",
        "entire image as a single class. Here, however, we are interested in a more nuanced classification: \n",
        "given an input image, we would like to identify each pixel that is possibly cancerous. \n",
        "That means that instead of a single output, your network should output an “image”, \n",
        "where each output pixel of your network represents the probability that a pixel is cancerous.\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Create a Network that classifies each pixel as a 1 or 0 for cancerous / not cancerous\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "___\n",
        "\n",
        "### Part 1a\n",
        "Implement your network topology\n",
        "\n",
        "\n",
        "Use the “Deep Convolution U-Net” from this paper: [(U-Net: Convolutional Networks for Biomedical Image Segmentation)](https://arxiv.org/pdf/1505.04597.pdf) \n",
        "\n",
        "![(Figure 1)](https://lh3.googleusercontent.com/qnHiB3B2KRxC3NjiSDtY08_DgDGTDsHcO6PP53oNRuct-p2QXCR-gyLkDveO850F2tTAhIOPC5Ha06NP9xq1JPsVAHlQ5UXA5V-9zkUrJHGhP_MNHFoRGnjBz1vn1p8P2rMWhlAb6HQ=w2400)\n",
        "\n",
        "You should use existing pytorch functions (not your own Conv2D module), such as ```nn.Conv2d```;\n",
        "you will also need the pytorch function ```torch.cat``` and ```nn.ConvTranspose2d```\n",
        "\n",
        "```torch.cat``` allows you to concatenate tensors.\n",
        "```nn.ConvTranspose2d``` is the opposite of ```nn.Conv2d```.\n",
        "It is used to bring an image from low res to higher res.\n",
        "[This blog](https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0) should help you understand this function in detail.\n",
        "\n",
        "Note that the simplest network you could implement (with all the desired properties)\n",
        "is just a single convolution layer with two filters and no relu! \n",
        "Why is that? (of course it wouldn't work very well!)\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Understand the U-Net architecture\n",
        "* Understand concatenation of inputs from multiple prior layers\n",
        "* Understand ConvTranspose\n",
        "* Answer Question / Reflect on simplest network with the desired properties\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "\n",
        "___\n",
        "The intention of this lab is to learn how to make deep neural nets and implement loss function.\n",
        "Therefore we'll help you with the implementation of Dataset.\n",
        "This code will download the dataset for you so that you are ready to use it and focus on network\n",
        "implementation, losses and accuracies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wQOefmcZVgTl",
        "pycharm": {
          "is_executing": false
        },
        "outputId": "6b47646b-2668-4fe1-cf09-e5a7011e6100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!pip3 install torch\n",
        "!pip3 install torchvision\n",
        "!pip3 install tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils, datasets\n",
        "from tqdm import tqdm\n",
        "from torch.nn.parameter import Parameter\n",
        "import pdb\n",
        "import torchvision\n",
        "import os\n",
        "import gzip\n",
        "import tarfile\n",
        "import gc\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n",
        "\n",
        "assert torch.cuda.is_available(), \"You need to request a GPU from Runtime > Change Runtime\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.16.5)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Il_53HLSWPTY",
        "colab": {}
      },
      "source": [
        "class CancerDataset(Dataset):\n",
        "  def __init__(self, root, download=True, size=512, train=True):\n",
        "    if download and not os.path.exists(os.path.join(root, 'cancer_data')):\n",
        "      datasets.utils.download_url('http://liftothers.org/cancer_data.tar.gz', root, 'cancer_data.tar.gz', None)\n",
        "      self.extract_gzip(os.path.join(root, 'cancer_data.tar.gz'))\n",
        "      self.extract_tar(os.path.join(root, 'cancer_data.tar'))\n",
        "    \n",
        "    postfix = 'train' if train else 'test'\n",
        "    root = os.path.join(root, 'cancer_data', 'cancer_data')\n",
        "    self.dataset_folder = torchvision.datasets.ImageFolder(os.path.join(root, 'inputs_' + postfix) ,transform = transforms.Compose([transforms.Resize(size),transforms.ToTensor()]))\n",
        "    self.label_folder = torchvision.datasets.ImageFolder(os.path.join(root, 'outputs_' + postfix) ,transform = transforms.Compose([transforms.Resize(size),transforms.ToTensor()]))\n",
        "\n",
        "  @staticmethod\n",
        "  def extract_gzip(gzip_path, remove_finished=False):\n",
        "    print('Extracting {}'.format(gzip_path))\n",
        "    with open(gzip_path.replace('.gz', ''), 'wb') as out_f, gzip.GzipFile(gzip_path) as zip_f:\n",
        "      out_f.write(zip_f.read())\n",
        "    if remove_finished:\n",
        "      os.unlink(gzip_path)\n",
        "  \n",
        "  @staticmethod\n",
        "  def extract_tar(tar_path):\n",
        "    print('Untarring {}'.format(tar_path))\n",
        "    z = tarfile.TarFile(tar_path)\n",
        "    z.extractall(tar_path.replace('.tar', ''))\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    img = self.dataset_folder[index]\n",
        "    label = self.label_folder[index]\n",
        "    return img[0],label[0][0]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.dataset_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSovbvhAMfeB",
        "colab_type": "code",
        "outputId": "58e98f2c-f3e7-4ef2-dccf-504eae95e3ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "data = CancerDataset('/tmp/cancer_data')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://liftothers.org/cancer_data.tar.gz to /tmp/cancer_data/cancer_data.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 2749825024/2750494655 [01:35<00:00, 28383305.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/cancer_data/cancer_data.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r2750496768it [01:50, 28383305.83it/s]                                "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Untarring /tmp/cancer_data/cancer_data.tar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WVIbOlwzM_B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "outputId": "0c260b69-eb74-409d-9c9f-1f5f97eceacc"
      },
      "source": [
        ""
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-6afff3f1686d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mo1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mo2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mo2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    336\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    337\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 338\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: conv2d(): argument 'input' (position 1) must be Tensor, not tuple"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDv3nG8rX5M-",
        "colab_type": "code",
        "outputId": "f999cff6-3617-4156-da34-b2fd256b7699",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(torch.cuda.memory_allocated(0)/ 1e9) # How much memory is the GPU using in GB\n",
        "\n",
        "gc.collect() # Force the garbage collector to release memory\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6iCBoiCespm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  pass\n",
        "except: # Without specifying the exception, this will catch all exceptions\n",
        "  __ITB__() # prints a stack trace\n",
        "  \n",
        "# Place code in a function, rather than keeping global scope. This will prevent memory problems"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2H1SLoon8c1",
        "colab_type": "code",
        "outputId": "cbc2c7ad-09ad-4e85-ef63-6c5405e67360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Sep 25 22:52:03 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.40       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    60W / 149W |   1257MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCEc1WJgiZyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Concatenate multiple tensors\n",
        "# torch.cat()\n",
        "\n",
        "# Use Convolution Transpose\n",
        "# There should be a module in nn that does this\n",
        "\n",
        "# In loaders...\n",
        "# modify \"num_workers\" parameter\n",
        "# somewhere between 2 - 6\n",
        "# mostly applicable to the training loaders, but you could do it on validation loaders as well\n",
        "\n",
        "# See GPU Utilization\n",
        "# GPUTIL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUaDr7Vya9J2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(ConvBlock, self).__init__()\n",
        "    \n",
        "    self.net = nn.Sequential(\n",
        "      nn.Conv2d(in_channels, out_channels, (3, 3), padding=(1, 1)),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(out_channels, out_channels, (3, 3), padding=(1, 1)),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(out_channels, out_channels, (3, 3), padding=(1, 1)),\n",
        "      nn.ReLU()\n",
        "    )\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "  \n",
        "class ConvNetwork(nn.Module):\n",
        "  def __init__(self, dataset):\n",
        "    super(ConvNetwork, self).__init__()\n",
        "    x, y = dataset[0]\n",
        "    c, h, w = x.size()\n",
        "    \n",
        "    padding = (1, 1)\n",
        "    \n",
        "    # Setting up objects\n",
        "    \n",
        "    # Downwards\n",
        "    self.max_pool = nn.MaxPool2d(2)\n",
        "    \n",
        "    self.conv1 = ConvBlock(c, 64) # For a 3x3 kernel, the padding is always (1, 1)\n",
        "    self.conv2 = ConvBlock(64, 128)\n",
        "    self.conv3 = ConvBlock(128, 256)\n",
        "    self.conv4 = ConvBlock(256, 512)\n",
        "    self.conv5 = ConvBlock(512, 1024)\n",
        "    \n",
        "    # Questions to Answer\n",
        "    # - What am I expecting after an Up-Convolution?\n",
        "    # - What am I concatenating together and how does the cat function work\n",
        "    \n",
        "    # Upwards\n",
        "    self.upconv1 = nn.ConvTranspose2d(1024, 512, 3, stride=2, padding=1)\n",
        "    self.conv6 = ConvBlock(1024, 512)\n",
        "    \n",
        "    self.upconv2 = nn.ConvTranspose2d(512, 256, 3, stride=2, padding=1)\n",
        "    self.conv7 = ConvBlock(512, 256)\n",
        "    \n",
        "    self.upconv3 = nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1)\n",
        "    self.conv8 = ConvBlock(256, 128)\n",
        "    \n",
        "    self.upconv4 = nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1)\n",
        "    self.conv9 = ConvBlock(128, 64)\n",
        "    \n",
        "    # 1x1 conv with 2 output channels\n",
        "    self.outconv = nn.Conv2d(64, 2, (1, 1), padding=(0, 0))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    n, c, h, w = x.size()\n",
        "    \n",
        "    # Downwards\n",
        "    o1 = self.conv1(x)\n",
        "    mp1 = self.max_pool(o1)\n",
        "    o2 = self.conv2(mp1)\n",
        "    mp2 = self.max_pool(o2)\n",
        "    o3 = self.conv3(mp2)\n",
        "    mp3 = self.max_pool(o3)\n",
        "    o4 = self.conv4(mp3)\n",
        "    mp4 = self.max_pool(o4)\n",
        "    o5 = self.conv5(mp4)\n",
        "    \n",
        "    up1 = self.upconv1(o5)\n",
        "    \n",
        "    #pdb.set_trace()\n",
        "    o6 = self.conv6(torch.cat((o4, up1), 0))\n",
        "    \n",
        "    up2 = self.upconv2(o6)\n",
        "    o7 = self.conv7(torch.cat((o3, up2), 0))\n",
        "    \n",
        "    up3 = self.upconv3(o7)\n",
        "    o8 = self.conv8(torch.cat((o2, up3), 0))\n",
        "    \n",
        "    up4 = self.upconv4(o8)\n",
        "    o9 = self.conv9(torch.cat((o1, up4), 0))\n",
        "    \n",
        "    #o10 = self.outconv(o9)\n",
        "    \n",
        "    return o9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sv8g1dh0bll",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df3b305a-5413-44b7-a1e1-ec06f2ce1593"
      },
      "source": [
        "def main():\n",
        "  root = '/tmp/cancer_data'\n",
        "  num_epochs = 1\n",
        "  \n",
        "  train_dataset = CancerDataset(root, train=True)\n",
        "  val_dataset = CancerDataset(root, train=False)\n",
        "  \n",
        "  model = ConvNetwork(train_dataset)\n",
        "  model = model.cuda()\n",
        "  \n",
        "  objective = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "  \n",
        "  train_loader = DataLoader(train_dataset,\n",
        "                           batch_size=8,\n",
        "                           num_workers=4,\n",
        "                           pin_memory=True)\n",
        "  \n",
        "  val_loader = DataLoader(train_dataset,\n",
        "                         batch_size=8,\n",
        "                         num_workers=2,\n",
        "                         pin_memory=True)\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "    loop = tqdm(total=len(train_loader), position=0, leave=False)\n",
        "    \n",
        "    for batch, (x, y_truth) in enumerate(train_loader):\n",
        "      x, y_truth = x.cuda(async=True), y_truth.cuda(async=True)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      y_hat = model(x)\n",
        "      \n",
        "      loss = objective(y_hat, y_truth)\n",
        "      accuracy = .5 #(y_hat.argmax(1) == y_truth).float().mean()\n",
        "      \n",
        "      loss.backward()\n",
        "      \n",
        "      train_losses.append(loss.item())\n",
        "      train_acc.append(accuracy.item())\n",
        "      \n",
        "      loop.set_description('epoch:{}, loss:{:.4f}, accuracy:{:.3f}'.format(epoch, loss, accuracy))\n",
        "      loop.update(1)\n",
        "      \n",
        "      optimizer.step()\n",
        "      \n",
        "      if batch % 10 == 0:\n",
        "        val = np.mean([objective(model(x.cuda()), y.cuda()).item()\n",
        "                      for x, y in val_loader])\n",
        "\n",
        "        val_y_hats = np.mean([(model(x.cuda()).argmax(1) == y_truth.cuda()).float().mean().item()\n",
        "                     for x, y_truth in val_loader])\n",
        "\n",
        "        val_losses.append((len(train_losses), val))\n",
        "        val_acc.append((len(train_losses), val_y_hats))\n",
        "        \n",
        "    loop.close()\n",
        "\n",
        "try:\n",
        "  main()\n",
        "  gc.collect()\n",
        "except:\n",
        "  gc.collect()\n",
        "  __ITB__()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/168 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
            "\u001b[0;32m<ipython-input-66-fe904c9bd240>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n",
            "\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     30\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m---> 31\u001b[0;31m       \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m        \u001b[0;36my_hat\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mmodel\u001b[0m \u001b[0;34m= ConvNetwork(\n",
            "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv1): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (conv2): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (conv3): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (conv4): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (conv5): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (upconv1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(1, 1))\n",
            "  (conv6): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (upconv2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(1, 1))\n",
            "  (conv7): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(1, 1))\n",
            "  (conv8): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (upconv4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(1, 1))\n",
            "  (conv9): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (outconv): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mx\u001b[0m \u001b[0;34m= tensor([[[[0.9490, 0.9569, 0.9490,  ..., 0.4941, 0.8275, 0.9412],\n",
            "          [0.9490, 0.9529, 0.9451,  ..., 0.4510, 0.7333, 0.8980],\n",
            "          [0.9451, 0.9490, 0.9451,  ..., 0.4902, 0.6314, 0.8588],\n",
            "          ...,\n",
            "          [0.7843, 0.8980, 0.8863,  ..., 0.4980, 0.4745, 0.3804],\n",
            "          [0.8667, 0.8980, 0.8157,  ..., 0.7451, 0.5373, 0.3686],\n",
            "          [0.9686, 0.9255, 0.8392,  ..., 0.9098, 0.6824, 0.4118]],\n",
            "\n",
            "         [[0.9412, 0.9490, 0.9529,  ..., 0.4118, 0.6235, 0.7490],\n",
            "          [0.9529, 0.9490, 0.9451,  ..., 0.3804, 0.5765, 0.6000],\n",
            "          [0.9529, 0.9490, 0.9451,  ..., 0.4039, 0.4824, 0.5412],\n",
            "          ...,\n",
            "          [0.7961, 0.8863, 0.8863,  ..., 0.4980, 0.4667, 0.4235],\n",
            "          [0.8627, 0.8902, 0.8196,  ..., 0.7059, 0.5255, 0.3843],\n",
            "          [0.9490, 0.9176, 0.8353,  ..., 0.8745, 0.6510, 0.4196]],\n",
            "\n",
            "         [[0.9412, 0.9412, 0.9490,  ..., 0.5216, 0.6627, 0.7686],\n",
            "          [0.9373, 0.9529, 0.9451,  ..., 0.5059, 0.6157, 0.6588],\n",
            "          [0.9412, 0.9529, 0.9373,  ..., 0.5412, 0.5490, 0.5804],\n",
            "          ...,\n",
            "          [0.7647, 0.8510, 0.8627,  ..., 0.6118, 0.5922, 0.5490],\n",
            "          [0.8471, 0.8667, 0.7804,  ..., 0.7647, 0.6314, 0.5216],\n",
            "          [0.9412, 0.9020, 0.8196,  ..., 0.8745, 0.7333, 0.5255]]],\n",
            "\n",
            "\n",
            "        [[[0.3255, 0.3255, 0.2471,  ..., 0.8980, 0.8941, 0.8941],\n",
            "          [0.8588, 0.8588, 0.7569,  ..., 0.9098, 0.8980, 0.9216],\n",
            "          [0.9412, 0.9216, 0.9490,  ..., 0.9098, 0.9098, 0.9216],\n",
            "          ...,\n",
            "          [0.3098, 0.2902, 0.4706,  ..., 0.9137, 0.9020, 0.9137],\n",
            "          [0.3569, 0.3804, 0.6471,  ..., 0.9059, 0.9098, 0.8941],\n",
            "          [0.5098, 0.5961, 0.7176,  ..., 0.9020, 0.9098, 0.8980]],\n",
            "\n",
            "         [[0.2235, 0.2353, 0.1647,  ..., 0.8980, 0.8863, 0.8824],\n",
            "          [0.7608, 0.7608, 0.6549,  ..., 0.9020, 0.8941, 0.8980],\n",
            "          [0.9216, 0.8863, 0.9216,  ..., 0.8941, 0.8980, 0.8941],\n",
            "          ...,\n",
            "          [0.3451, 0.3451, 0.5294,  ..., 0.8980, 0.8902, 0.8824],\n",
            "          [0.3804, 0.4353, 0.6784,  ..., 0.8863, 0.9020, 0.8863],\n",
            "          [0.5333, 0.5843, 0.7098,  ..., 0.8863, 0.9020, 0.8941]],\n",
            "\n",
            "         [[0.1843, 0.1882, 0.1255,  ..., 0.8745, 0.8667, 0.8784],\n",
            "          [0.6275, 0.6510, 0.5647,  ..., 0.8980, 0.8745, 0.8980],\n",
            "          [0.8745, 0.8471, 0.8745,  ..., 0.8863, 0.8902, 0.8902],\n",
            "          ...,\n",
            "          [0.5059, 0.5098, 0.6314,  ..., 0.8784, 0.8784, 0.8824],\n",
            "          [0.5216, 0.5412, 0.7059,  ..., 0.8745, 0.8745, 0.8706],\n",
            "          [0.6196, 0.6157, 0.6784,  ..., 0.8667, 0.8627, 0.8745]]],\n",
            "\n",
            "\n",
            "        [[[0.9373, 0.9412, 0.9490,  ..., 0.8431, 0.9176, 0.6980],\n",
            "          [0.9412, 0.9412, 0.9490,  ..., 0.8000, 0.8392, 0.5647],\n",
            "          [0.9490, 0.9451, 0.9451,  ..., 0.7647, 0.7529, 0.7020],\n",
            "          ...,\n",
            "          [0.9490, 0.9490, 0.9490,  ..., 0.6235, 0.7333, 0.8431],\n",
            "          [0.9490, 0.9490, 0.9490,  ..., 0.3765, 0.4941, 0.7843],\n",
            "          [0.9490, 0.9490, 0.9490,  ..., 0.2353, 0.2706, 0.5490]],\n",
            "\n",
            "         [[0.9451, 0.9490, 0.9294,  ..., 0.8118, 0.8235, 0.5490],\n",
            "          [0.9412, 0.9490, 0.9373,  ..., 0.7137, 0.7176, 0.4824],\n",
            "          [0.9412, 0.9451, 0.9412,  ..., 0.6000, 0.5608, 0.5686],\n",
            "          ...,\n",
            "          [0.9451, 0.9451, 0.9412,  ..., 0.6118, 0.7020, 0.8078],\n",
            "          [0.9451, 0.9451, 0.9412,  ..., 0.3961, 0.5176, 0.7922],\n",
            "          [0.9412, 0.9412, 0.9451,  ..., 0.2549, 0.3176, 0.6078]],\n",
            "\n",
            "         [[0.9294, 0.9373, 0.9451,  ..., 0.8392, 0.8275, 0.6196],\n",
            "          [0.9451, 0.9451, 0.9451,  ..., 0.7686, 0.7647, 0.6000],\n",
            "          [0.9451, 0.9412, 0.9412,  ..., 0.6471, 0.5961, 0.6235],\n",
            "          ...,\n",
            "          [0.9412, 0.9412, 0.9373,  ..., 0.7020, 0.7333, 0.7961],\n",
            "          [0.9451, 0.9490, 0.9490,  ..., 0.5647, 0.6353, 0.8235],\n",
            "          [0.9451, 0.9373, 0.9412,  ..., 0.4667, 0.5176, 0.7137]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.4157, 0.3647, 0.3020,  ..., 0.9098, 0.8784, 0.8784],\n",
            "          [0.4118, 0.3529, 0.3255,  ..., 0.8902, 0.8824, 0.8980],\n",
            "          [0.4118, 0.3922, 0.3333,  ..., 0.8784, 0.8902, 0.8941],\n",
            "          ...,\n",
            "          [0.9451, 0.9451, 0.9490,  ..., 0.7608, 0.3686, 0.3098],\n",
            "          [0.9451, 0.9451, 0.9490,  ..., 0.7412, 0.2941, 0.2784],\n",
            "          [0.9529, 0.9490, 0.9451,  ..., 0.9098, 0.5843, 0.3451]],\n",
            "\n",
            "         [[0.3647, 0.3098, 0.2510,  ..., 0.8745, 0.8471, 0.8510],\n",
            "          [0.3451, 0.3020, 0.2706,  ..., 0.8471, 0.8431, 0.8588],\n",
            "          [0.3490, 0.3373, 0.2863,  ..., 0.8431, 0.8588, 0.8627],\n",
            "          ...,\n",
            "          [0.9451, 0.9412, 0.9451,  ..., 0.6902, 0.3608, 0.2902],\n",
            "          [0.9412, 0.9451, 0.9451,  ..., 0.7020, 0.3020, 0.2549],\n",
            "          [0.9373, 0.9490, 0.9412,  ..., 0.8902, 0.5647, 0.3608]],\n",
            "\n",
            "         [[0.3020, 0.2392, 0.2000,  ..., 0.8353, 0.7961, 0.8039],\n",
            "          [0.2824, 0.2510, 0.2078,  ..., 0.8118, 0.8000, 0.8118],\n",
            "          [0.2863, 0.2784, 0.2196,  ..., 0.8039, 0.8157, 0.8235],\n",
            "          ...,\n",
            "          [0.9412, 0.9333, 0.9412,  ..., 0.7176, 0.5020, 0.4627],\n",
            "          [0.9412, 0.9333, 0.9412,  ..., 0.7490, 0.4549, 0.4196],\n",
            "          [0.9333, 0.9451, 0.9373,  ..., 0.8980, 0.6627, 0.4784]]],\n",
            "\n",
            "\n",
            "        [[[0.9255, 0.9333, 0.9294,  ..., 0.8980, 0.8863, 0.8824],\n",
            "          [0.9373, 0.9412, 0.9333,  ..., 0.8784, 0.8706, 0.8863],\n",
            "          [0.9216, 0.9333, 0.9294,  ..., 0.8431, 0.8588, 0.9020],\n",
            "          ...,\n",
            "          [0.9490, 0.9412, 0.9412,  ..., 0.7843, 0.9098, 0.9451],\n",
            "          [0.9529, 0.9451, 0.9373,  ..., 0.9333, 0.9569, 0.9647],\n",
            "          [0.9490, 0.9490, 0.9412,  ..., 0.9490, 0.9608, 0.9569]],\n",
            "\n",
            "         [[0.9333, 0.9255, 0.9373,  ..., 0.8902, 0.8549, 0.8745],\n",
            "          [0.9412, 0.9412, 0.9294,  ..., 0.8471, 0.8510, 0.8627],\n",
            "          [0.9373, 0.9333, 0.9216,  ..., 0.8235, 0.8510, 0.8745],\n",
            "          ...,\n",
            "          [0.9412, 0.9451, 0.9451,  ..., 0.5843, 0.8549, 0.9059],\n",
            "          [0.9451, 0.9451, 0.9451,  ..., 0.8196, 0.9451, 0.9294],\n",
            "          [0.9490, 0.9451, 0.9451,  ..., 0.8549, 0.9216, 0.9451]],\n",
            "\n",
            "         [[0.9255, 0.9412, 0.9373,  ..., 0.8667, 0.8196, 0.8353],\n",
            "          [0.9294, 0.9294, 0.9373,  ..., 0.8235, 0.8196, 0.8196],\n",
            "          [0.9373, 0.9255, 0.9333,  ..., 0.7765, 0.8157, 0.8235],\n",
            "          ...,\n",
            "          [0.9451, 0.9451, 0.9412,  ..., 0.6353, 0.8980, 0.9137],\n",
            "          [0.9412, 0.9333, 0.9373,  ..., 0.8431, 0.9451, 0.9255],\n",
            "          [0.9490, 0.9451, 0.9451,  ..., 0.8824, 0.9255, 0.9412]]],\n",
            "\n",
            "\n",
            "        [[[0.9569, 0.9569, 0.9569,  ..., 0.9569, 0.9569, 0.9569],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.9569, 0.9569, 0.9569],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.9569, 0.9569, 0.9569],\n",
            "          ...,\n",
            "          [0.9569, 0.9529, 0.9529,  ..., 0.8824, 0.8824, 0.8824],\n",
            "          [0.9294, 0.9529, 0.9647,  ..., 0.8706, 0.8706, 0.9333],\n",
            "          [0.9451, 0.9569, 0.9608,  ..., 0.8980, 0.9059, 0.9529]],\n",
            "\n",
            "         [[0.9569, 0.9569, 0.9569,  ..., 0.9569, 0.9569, 0.9569],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.9569, 0.9569, 0.9569],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.9569, 0.9569, 0.9569],\n",
            "          ...,\n",
            "          [0.9608, 0.9686, 0.9608,  ..., 0.8706, 0.8745, 0.8863],\n",
            "          [0.9137, 0.9569, 0.9647,  ..., 0.8588, 0.8627, 0.9098],\n",
            "          [0.9451, 0.9608, 0.9608,  ..., 0.8784, 0.8863, 0.9333]],\n",
            "\n",
            "         [[0.9569, 0.9569, 0.9569,  ..., 0.9569, 0.9569, 0.9569],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.9569, 0.9569, 0.9569],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.9569, 0.9569, 0.9569],\n",
            "          ...,\n",
            "          [0.9569, 0.9569, 0.9608,  ..., 0.8392, 0.8392, 0.8471],\n",
            "          [0.9059, 0.9529, 0.9569,  ..., 0.8196, 0.8275, 0.8706],\n",
            "          [0.9373, 0.9569, 0.9608,  ..., 0.8431, 0.8627, 0.9059]]]],\n",
            "       device='cuda:0')\u001b[0m\n",
            "\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     33\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self=ConvNetwork(\n",
            "  (max_pool): MaxPool2d(kernel_size...onv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "), *input=(tensor([[[[0.9490, 0.9569, 0.9490,  ..., 0.4941,...8431, 0.8627, 0.9059]]]],\n",
            "       device='cuda:0'),), **kwargs={})\u001b[0m\n",
            "\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m        \u001b[0;36mresult\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mself.forward\u001b[0m \u001b[0;34m= <bound method ConvNetwork.forward of ConvNetwork(\n",
            "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv1): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (conv2): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (conv3): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (conv4): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (conv5): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (upconv1): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (conv6): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (upconv2): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (conv7): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (conv8): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (upconv4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (conv9): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (outconv): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            ")>\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36minput\u001b[0m \u001b[0;34m= (tensor([[[[0.9490, 0.9569, 0.9490,  ..., 0.4941, 0.8275, 0.9412],\n",
            "          [0.9490, 0.9529, 0.9451,  ..., 0.4510, 0.7333, 0.8980],\n",
            "          [0.9451, 0.9490, 0.9451,  ..., 0.4902, 0.6314, 0.8588],\n",
            "          ...,\n",
            "          [0.7843, 0.8980, 0.8863,  ..., 0.4980, 0.4745, 0.3804],\n",
            "          [0.8667, 0.8980, 0.8157,  ..., 0.7451, 0.5373, 0.3686],\n",
            "          [0.9686, 0.9255, 0.8392,  ..., 0.9098, 0.6824, 0.4118]],\n",
            "\n",
            "         [[0.9412, 0.9490, 0.9529,  ..., 0.4118, 0.6235, 0.7490],\n",
            "          [0.9529, 0.9490, 0.9451,  ..., 0.3804, 0.5765, 0.6000],\n",
            "          [0.9529, 0.9490, 0.9451,  ..., 0.4039, 0.4824, 0.5412],\n",
            "          ...,\n",
            "          [0.7961, 0.8863, 0.8863,  ..., 0.4980, 0.4667, 0.4235],\n",
            "          [0.8627, 0.8902, 0.8196,  ..., 0.7059, 0.5255, 0.3843],\n",
            "          [0.9490, 0.9176, 0.8353,  ..., 0.8745, 0.6510, 0.4196]],\n",
            "\n",
            "         [[0.9412, 0.9412, 0.9490,  ..., 0.5216, 0.6627, 0.7686],\n",
            "          [0.9373, 0.9529, 0.9451,  ..., 0.5059, 0.6157, 0.6588],\n",
            "          [0.9412, 0.9529, 0.9373,  ..., 0.5412, 0.5490, 0.5804],\n",
            "          ...,\n",
            "          [0.7647, 0.8510, 0.8627,  ..., 0.6118, 0.5922, 0.5490],\n",
            "          [0.8471, 0.8667, 0.7804,  ..., 0.7647, 0.6314, 0.5216],\n",
            "          [0.9412, 0.9020, 0.8196,  ..., 0.8745, 0.7333, 0.5255]]],\n",
            "\n",
            "\n",
            "        [[[0.3255, 0.3255, 0.2471,  ..., 0.8980, 0.8941, 0.8941],\n",
            "          [0.8588, 0.8588, 0.7569,  ..., 0.9098, 0.8980, 0.9216],\n",
            "          [0.9412, 0.9216, 0.9490,  ..., 0.9098, 0.9098, 0.9216],\n",
            "          ...,\n",
            "          [0.3098, 0.2902, 0.4706,  ..., 0.9137, 0.9020, 0.9137],\n",
            "          [0.3569, 0.3804, 0.6471,  ..., 0.9059, 0.9098, 0.8941],\n",
            "          [0.5098, 0.5961, 0.7176,  ..., 0.9020, 0.9098, 0.8980]],\n",
            "\n",
            "         [[0.2235, 0.2353, 0.1647,  ..., 0.8980, 0.8863, 0.8824],\n",
            "          [0.7608, 0.7608, 0.6549,  ..., 0.9020, 0.8941, 0.8980],\n",
            "          [0.9216, 0.8863, 0.9216,  ..., 0.8941, 0.8980, 0.8941],\n",
            "          ...,\n",
            "          [0.3451, 0.3451, 0.5294,  ..., 0.8980, 0.8902, 0.8824],\n",
            "          [0.3804, 0.4353, 0.6784,  ..., 0.8863, 0.9020, 0.8863],\n",
            "          [0.5333, 0.5843, 0.7098,  ..., 0.8863, 0.9020, 0.8941]],\n",
            "\n",
            "         [[0.1843, 0.1882, 0.1255,  ..., 0.8745, 0.8667, 0.8784],\n",
            "          [0.6275, 0.6510, 0.5647,  ..., 0.8980, 0.8745, 0.8980],\n",
            "          [0.8745, 0.8471, 0.8745,  ..., 0.8863, 0.8902, 0.8902],\n",
            "          ...,\n",
            "          [0.5059, 0.5098, 0.6314,  ..., 0.8784, 0.8784, 0.8824],\n",
            "          [0.5216, 0.5412, 0.7059,  ..., 0.8745, 0.8745, 0.8706],\n",
            "          [0.6196, 0.6157, 0.6784,  ..., 0.8667, 0.8627, 0.8745]]],\n",
            "\n",
            "\n",
            "        [[[0.9373, 0.9412, 0.9490,  ..., 0.8431, 0.9176, 0.6980],\n",
            "          [0.9412, 0.9412, 0.9490,  ..., 0.8000, 0.8392, 0.5647],\n",
            "          [0.9490, 0.9451, 0.9451,  ..., 0.7647, 0.7529, 0.7020],\n",
            "          ...,\n",
            "          [0.9490, 0.9490, 0.9490,  ..., 0.6235, 0.7333, 0.8431],\n",
            "          [0.9490, 0.9490, 0.9490,  ..., 0.3765, 0.4941, 0.7843],\n",
            "          [0.9490, 0.9490, 0.9490,  ..., 0.2353, 0.2706, 0.5490]],\n",
            "\n",
            "         [[0.9451, 0.9490, 0.9294,  ..., 0.8118, 0.8235, 0.5490],\n",
            "          [0.9412, 0.9490, 0.9373,  ..., 0.7137, 0.7176, 0.4824],\n",
            "          [0.9412, 0.9451, 0.9412,  ..., 0.6000, 0.5608, 0.5686],\n",
            "          ...,\n",
            "          [0.9451, 0.9451, 0.9412,  ..., 0.6118, 0.7020, 0.8078],\n",
            "          [0.9451, 0.9451, 0.9412,  ..., 0.3961, 0.5176, 0.7922],\n",
            "          [0.9412, 0.9412, 0.9451,  ..., 0.2549, 0.3176, 0.6078]],\n",
            "\n",
            "         [[0.9294, 0.9373, 0.9451,  ..., 0.8392, 0.8275, 0.6196],\n",
            "          [0.9451, 0.9451, 0.9451,  ..., 0.7686, 0.7647, 0.6000],\n",
            "          [0.9451, 0.9412, 0.9412,  ..., 0.6471, 0.5961, 0.6235],\n",
            "          ...,\n",
            "          [0.9412, 0.9412, 0.9373,  ..., 0.7020, 0.7333, 0.7961],\n",
            "          [0.9451, 0.9490, 0.9490,  ..., 0.5647, 0.6353, 0.8235],\n",
            "          [0.9451, 0.9373, 0.9412,  ..., 0.4667, 0.5176, 0.7137]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.4157, 0.3647, 0.3020,  ..., 0.9098, 0.8784, 0.8784],\n",
            "          [0.4118, 0.3529, 0.3255,  ..., 0.8902, 0.8824, 0.8980],\n",
            "          [0.4118, 0.3922, 0.3333,  ..., 0.8784, 0.8902, 0.8941],\n",
            "          ...,\n",
            "          [0.9451, 0.9451, 0.9490,  ..., 0.7608, 0.3686, 0.3098],\n",
            "          [0.9451, 0.9451, 0.9490,  ..., 0.7412, 0.2941, 0.2784],\n",
            "          [0.9529, 0.9490, 0.9451,  ..., 0.9098, 0.5843, 0.3451]],\n",
            "\n",
            "         [[0.3647, 0.3098, 0.2510,  ..., 0.8745, 0.8471, 0.8510],\n",
            "          [0.3451, 0.3020, 0.2706,  ..., 0.8471, 0.8431, 0.8588],\n",
            "          [0.3490, 0.3373, 0.2863,  ..., 0.8431, 0.8588, 0.8627],\n",
            "          ...,\n",
            "          [0.9451, 0.9412, 0.9451,  ..., 0.6902, 0.3608, 0.2902],\n",
            "          [0.9412, 0.9451, 0.9451,  ..., 0.7020, 0.3020, 0.2549],\n",
            "          [0.9373, 0.9490, 0.9412,  ..., 0.8902, 0.5647, 0.3608]],\n",
            "\n",
            "         [[0.3020, 0.2392, 0.2000,  ..., 0.8353, 0.7961, 0.8039],\n",
            "          [0.2824, 0.2510, 0.2078,  ..., 0.8118, 0.8000, 0.8118],\n",
            "          [0.2863, 0.2784, 0.2196,  ..., 0.8039, 0.8157, 0.8235],\n",
            "          ...,\n",
            "          [0.9412, 0.9333, 0.9412,  ..., 0.7176, 0.5020, 0.4627],\n",
            "          [0.9412, 0.9333, 0.9412,  ..., 0.7490, 0.4549, 0.4196],\n",
            "          [0.9333, 0.9451, 0.9373,  ..., 0.8980, 0.6627, 0.4784]]],\n",
            "\n",
            "\n",
            "        [[[0.9255, 0.9333, 0.9294,  ..., 0.8980, 0.8863, 0.8824],\n",
            "          [0.9373, 0.9412, 0.9333,  ..., 0.8784, 0.8706, 0.8863],\n",
            "          [0.9216, 0.9333, 0.9294,  ..., 0.8431, 0.8588, 0.9020],\n",
            "          ...,\n",
            "          [0.9490, 0.9412, 0.9412,  ..., 0.7843, 0.9098, 0.9451],\n",
            "          [0.9529, 0.9451, 0.9373,  ..., 0.9333, 0.9569, 0.9647],\n",
            "          [0.9490, 0.9490, 0.9412,  ..., 0.9490, 0.9608, 0.9569]],\n",
            "\n",
            "         [[0.9333, 0.9255, 0.9373,  ..., 0.8902, 0.8549, 0.8745],\n",
            "          [0.9412, 0.9412, 0.9294,  ..., 0.8471, 0.8510, 0.8627],\n",
            "          [0.9373, 0.9333, 0.9216,  ..., 0.8235, 0.8510, 0.8745],\n",
            "          ...,\n",
            "          [0.9412, 0.9451, 0.9451,  ..., 0.5843, 0.8549, 0.9059],\n",
            "          [0.9451, 0.9451, 0.9451,  ..., 0.8196, 0.9451, 0.9294],\n",
            "          [0.9490, 0.9451, 0.9451,  ..., 0.8549, 0.9216, 0.9451]],\n",
            "\n",
            "         [[0.9255, 0.9412, 0.9373,  ..., 0.8667, 0.8196, 0.8353],\n",
            "          [0.9294, 0.9294, 0.9373,  ..., 0.8235, 0.8196, 0.8196],\n",
            "          [0.9373, 0.9255, 0.9333,  ..., 0.7765, 0.8157, 0.8235],\n",
            "          ...,\n",
            "          [0.9451, 0.9451, 0.9412,  ..., 0.6353, 0.8980, 0.9137],\n",
            "          [0.9412, 0.9333, 0.9373,  ..., 0.8431, 0.9451, 0.9255],\n",
            "          [0.9490, 0.9451, 0.9451,  ..., 0.8824, 0.9255, 0.9412]]],\n",
            "\n",
            "\n",
            "        [[[0.9569, 0.9569, 0.9569,  ..., 0.9569, 0.9569, 0.9569],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.9569, 0.9569, 0.9569],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.9569, 0.9569, 0.9569],\n",
            "          ...,\n",
            "          [0.9569, 0.9529, 0.9529,  ..., 0.8824, 0.8824, 0.8824],\n",
            "          [0.9294, 0.9529, 0.9647,  ..., 0.8706, 0.8706, 0.9333],\n",
            "          [0.9451, 0.9569, 0.9608,  ..., 0.8980, 0.9059, 0.9529]],\n",
            "\n",
            "         [[0.9569, 0.9569, 0.9569,  ..., 0.9569, 0.9569, 0.9569],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.9569, 0.9569, 0.9569],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.9569, 0.9569, 0.9569],\n",
            "          ...,\n",
            "          [0.9608, 0.9686, 0.9608,  ..., 0.8706, 0.8745, 0.8863],\n",
            "          [0.9137, 0.9569, 0.9647,  ..., 0.8588, 0.8627, 0.9098],\n",
            "          [0.9451, 0.9608, 0.9608,  ..., 0.8784, 0.8863, 0.9333]],\n",
            "\n",
            "         [[0.9569, 0.9569, 0.9569,  ..., 0.9569, 0.9569, 0.9569],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.9569, 0.9569, 0.9569],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.9569, 0.9569, 0.9569],\n",
            "          ...,\n",
            "          [0.9569, 0.9569, 0.9608,  ..., 0.8392, 0.8392, 0.8471],\n",
            "          [0.9059, 0.9529, 0.9569,  ..., 0.8196, 0.8275, 0.8706],\n",
            "          [0.9373, 0.9569, 0.9608,  ..., 0.8431, 0.8627, 0.9059]]]],\n",
            "       device='cuda:0'),)\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mkwargs\u001b[0m \u001b[0;34m= {}\u001b[0m\n",
            "\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m<ipython-input-65-764f89d0f903>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self=ConvNetwork(\n",
            "  (max_pool): MaxPool2d(kernel_size...onv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "), x=tensor([[[[0.9490, 0.9569, 0.9490,  ..., 0.4941,...8431, 0.8627, 0.9059]]]],\n",
            "       device='cuda:0'))\u001b[0m\n",
            "\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     72\u001b[0m     \u001b[0;31m#pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mo6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mup1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m        \u001b[0;36mo6\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mself.conv6\u001b[0m \u001b[0;34m= ConvBlock(\n",
            "  (net): Sequential(\n",
            "    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU()\n",
            "  )\n",
            ")\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mtorch.cat\u001b[0m \u001b[0;34m= <built-in method cat of type object at 0x7fd77ca72b80>\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mo4\u001b[0m \u001b[0;34m= tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 4.4337e-05],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.4098e-04, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           4.2845e-04, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [1.0299e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[1.3642e-02, 1.2818e-02, 1.3777e-02,  ..., 1.4072e-02,\n",
            "           1.4456e-02, 1.3401e-02],\n",
            "          [1.4624e-02, 1.3348e-02, 1.3970e-02,  ..., 1.4071e-02,\n",
            "           1.4339e-02, 1.3327e-02],\n",
            "          [1.4641e-02, 1.3732e-02, 1.4051e-02,  ..., 1.3976e-02,\n",
            "           1.4623e-02, 1.3488e-02],\n",
            "          ...,\n",
            "          [1.4720e-02, 1.3694e-02, 1.4030e-02,  ..., 1.4098e-02,\n",
            "           1.4515e-02, 1.3406e-02],\n",
            "          [1.4510e-02, 1.3394e-02, 1.3212e-02,  ..., 1.3295e-02,\n",
            "           1.3821e-02, 1.3023e-02],\n",
            "          [1.2347e-02, 1.1778e-02, 1.1502e-02,  ..., 1.1313e-02,\n",
            "           1.1529e-02, 1.1163e-02]],\n",
            "\n",
            "         [[9.9959e-03, 8.8973e-03, 8.2789e-03,  ..., 8.3899e-03,\n",
            "           8.8546e-03, 1.0843e-02],\n",
            "          [1.5306e-02, 1.3921e-02, 1.3399e-02,  ..., 1.3433e-02,\n",
            "           1.2382e-02, 1.0783e-02],\n",
            "          [1.5713e-02, 1.3340e-02, 1.3061e-02,  ..., 1.3224e-02,\n",
            "           1.2040e-02, 1.0590e-02],\n",
            "          ...,\n",
            "          [1.5735e-02, 1.3411e-02, 1.3089e-02,  ..., 1.3207e-02,\n",
            "           1.1889e-02, 1.0517e-02],\n",
            "          [1.5657e-02, 1.4026e-02, 1.3881e-02,  ..., 1.3999e-02,\n",
            "           1.2869e-02, 1.1819e-02],\n",
            "          [1.9420e-02, 1.9364e-02, 1.9446e-02,  ..., 1.9557e-02,\n",
            "           1.8361e-02, 1.3543e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1.1131e-02, 6.2476e-03, 7.0156e-03,  ..., 6.8930e-03,\n",
            "           7.1766e-03, 6.2056e-03],\n",
            "          [9.1155e-03, 5.3831e-03, 5.7191e-03,  ..., 5.6788e-03,\n",
            "           6.7016e-03, 7.3447e-03],\n",
            "          [8.8958e-03, 5.6943e-03, 5.8123e-03,  ..., 5.6867e-03,\n",
            "           6.7989e-03, 6.8800e-03],\n",
            "          ...,\n",
            "          [8.9388e-03, 5.6471e-03, 5.9156e-03,  ..., 5.7699e-03,\n",
            "           6.9141e-03, 6.8576e-03],\n",
            "          [9.1865e-03, 6.1710e-03, 6.2512e-03,  ..., 6.2169e-03,\n",
            "           6.8727e-03, 6.5174e-03],\n",
            "          [9.0006e-03, 1.0381e-02, 9.7689e-03,  ..., 9.7872e-03,\n",
            "           1.0776e-02, 1.1170e-02]],\n",
            "\n",
            "         [[2.9525e-03, 4.0339e-03, 3.8118e-03,  ..., 3.6061e-03,\n",
            "           4.4985e-03, 1.9294e-03],\n",
            "          [3.3319e-03, 4.9663e-03, 5.5197e-03,  ..., 5.2745e-03,\n",
            "           6.2997e-03, 3.6238e-03],\n",
            "          [2.4973e-03, 3.7026e-03, 4.6197e-03,  ..., 4.3178e-03,\n",
            "           5.6546e-03, 3.1807e-03],\n",
            "          ...,\n",
            "          [2.4236e-03, 3.5474e-03, 4.4860e-03,  ..., 4.0417e-03,\n",
            "           5.4636e-03, 2.8327e-03],\n",
            "          [2.9715e-03, 4.9081e-03, 5.5463e-03,  ..., 5.3054e-03,\n",
            "           6.6470e-03, 3.4146e-03],\n",
            "          [8.5802e-04, 1.7163e-03, 2.7280e-03,  ..., 2.5792e-03,\n",
            "           3.9347e-03, 2.5805e-03]],\n",
            "\n",
            "         [[1.0273e-02, 1.3837e-02, 1.3712e-02,  ..., 1.3791e-02,\n",
            "           1.3874e-02, 1.2836e-02],\n",
            "          [1.1870e-02, 1.5559e-02, 1.4677e-02,  ..., 1.4874e-02,\n",
            "           1.4937e-02, 1.2139e-02],\n",
            "          [1.1564e-02, 1.5071e-02, 1.4169e-02,  ..., 1.4415e-02,\n",
            "           1.4590e-02, 1.2073e-02],\n",
            "          ...,\n",
            "          [1.1637e-02, 1.4818e-02, 1.3988e-02,  ..., 1.4274e-02,\n",
            "           1.4435e-02, 1.2052e-02],\n",
            "          [1.1339e-02, 1.4655e-02, 1.3921e-02,  ..., 1.4111e-02,\n",
            "           1.4403e-02, 1.2426e-02],\n",
            "          [1.1367e-02, 1.1829e-02, 1.1645e-02,  ..., 1.1700e-02,\n",
            "           1.1666e-02, 1.1316e-02]]],\n",
            "\n",
            "\n",
            "        [[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 4.5155e-05],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.3045e-04, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           4.2651e-04, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [1.0278e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[1.3644e-02, 1.2818e-02, 1.3777e-02,  ..., 1.4069e-02,\n",
            "           1.4455e-02, 1.3399e-02],\n",
            "          [1.4617e-02, 1.3349e-02, 1.3965e-02,  ..., 1.4082e-02,\n",
            "           1.4356e-02, 1.3330e-02],\n",
            "          [1.4635e-02, 1.3726e-02, 1.4037e-02,  ..., 1.3994e-02,\n",
            "           1.4634e-02, 1.3490e-02],\n",
            "          ...,\n",
            "          [1.4720e-02, 1.3687e-02, 1.4030e-02,  ..., 1.4094e-02,\n",
            "           1.4516e-02, 1.3404e-02],\n",
            "          [1.4510e-02, 1.3389e-02, 1.3203e-02,  ..., 1.3290e-02,\n",
            "           1.3817e-02, 1.3021e-02],\n",
            "          [1.2343e-02, 1.1785e-02, 1.1502e-02,  ..., 1.1313e-02,\n",
            "           1.1526e-02, 1.1163e-02]],\n",
            "\n",
            "         [[9.9962e-03, 8.8983e-03, 8.2809e-03,  ..., 8.3810e-03,\n",
            "           8.8580e-03, 1.0840e-02],\n",
            "          [1.5306e-02, 1.3921e-02, 1.3400e-02,  ..., 1.3431e-02,\n",
            "           1.2383e-02, 1.0784e-02],\n",
            "          [1.5714e-02, 1.3348e-02, 1.3068e-02,  ..., 1.3215e-02,\n",
            "           1.2036e-02, 1.0585e-02],\n",
            "          ...,\n",
            "          [1.5735e-02, 1.3414e-02, 1.3099e-02,  ..., 1.3202e-02,\n",
            "           1.1891e-02, 1.0519e-02],\n",
            "          [1.5655e-02, 1.4023e-02, 1.3879e-02,  ..., 1.4002e-02,\n",
            "           1.2869e-02, 1.1821e-02],\n",
            "          [1.9420e-02, 1.9363e-02, 1.9448e-02,  ..., 1.9559e-02,\n",
            "           1.8362e-02, 1.3542e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1.1140e-02, 6.2577e-03, 7.0291e-03,  ..., 6.8706e-03,\n",
            "           7.1610e-03, 6.1996e-03],\n",
            "          [9.1238e-03, 5.3913e-03, 5.7269e-03,  ..., 5.6523e-03,\n",
            "           6.6806e-03, 7.3402e-03],\n",
            "          [8.9003e-03, 5.6978e-03, 5.8311e-03,  ..., 5.6451e-03,\n",
            "           6.7686e-03, 6.8761e-03],\n",
            "          ...,\n",
            "          [8.9412e-03, 5.6462e-03, 5.9219e-03,  ..., 5.7626e-03,\n",
            "           6.9067e-03, 6.8581e-03],\n",
            "          [9.1899e-03, 6.1700e-03, 6.2499e-03,  ..., 6.2100e-03,\n",
            "           6.8678e-03, 6.5154e-03],\n",
            "          [8.9991e-03, 1.0383e-02, 9.7737e-03,  ..., 9.7809e-03,\n",
            "           1.0770e-02, 1.1172e-02]],\n",
            "\n",
            "         [[2.9510e-03, 4.0343e-03, 3.8118e-03,  ..., 3.6049e-03,\n",
            "           4.4965e-03, 1.9339e-03],\n",
            "          [3.3359e-03, 4.9729e-03, 5.5204e-03,  ..., 5.2604e-03,\n",
            "           6.2951e-03, 3.6173e-03],\n",
            "          [2.5026e-03, 3.7199e-03, 4.6246e-03,  ..., 4.3139e-03,\n",
            "           5.6464e-03, 3.1794e-03],\n",
            "          ...,\n",
            "          [2.4238e-03, 3.5582e-03, 4.4971e-03,  ..., 4.0414e-03,\n",
            "           5.4584e-03, 2.8334e-03],\n",
            "          [2.9688e-03, 4.9145e-03, 5.5548e-03,  ..., 5.2954e-03,\n",
            "           6.6404e-03, 3.4154e-03],\n",
            "          [8.6071e-04, 1.7202e-03, 2.7309e-03,  ..., 2.5841e-03,\n",
            "           3.9345e-03, 2.5798e-03]],\n",
            "\n",
            "         [[1.0269e-02, 1.3838e-02, 1.3708e-02,  ..., 1.3809e-02,\n",
            "           1.3880e-02, 1.2839e-02],\n",
            "          [1.1869e-02, 1.5564e-02, 1.4674e-02,  ..., 1.4880e-02,\n",
            "           1.4943e-02, 1.2141e-02],\n",
            "          [1.1566e-02, 1.5075e-02, 1.4175e-02,  ..., 1.4419e-02,\n",
            "           1.4594e-02, 1.2074e-02],\n",
            "          ...,\n",
            "          [1.1634e-02, 1.4822e-02, 1.3990e-02,  ..., 1.4270e-02,\n",
            "           1.4438e-02, 1.2053e-02],\n",
            "          [1.1345e-02, 1.4657e-02, 1.3922e-02,  ..., 1.4112e-02,\n",
            "           1.4409e-02, 1.2428e-02],\n",
            "          [1.1366e-02, 1.1835e-02, 1.1653e-02,  ..., 1.1700e-02,\n",
            "           1.1667e-02, 1.1315e-02]]],\n",
            "\n",
            "\n",
            "        [[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 4.3811e-05],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.4007e-04, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           4.3011e-04, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [1.0297e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[1.3642e-02, 1.2818e-02, 1.3777e-02,  ..., 1.4070e-02,\n",
            "           1.4453e-02, 1.3399e-02],\n",
            "          [1.4624e-02, 1.3348e-02, 1.3969e-02,  ..., 1.4076e-02,\n",
            "           1.4339e-02, 1.3327e-02],\n",
            "          [1.4641e-02, 1.3732e-02, 1.4050e-02,  ..., 1.3994e-02,\n",
            "           1.4631e-02, 1.3487e-02],\n",
            "          ...,\n",
            "          [1.4723e-02, 1.3697e-02, 1.4033e-02,  ..., 1.4101e-02,\n",
            "           1.4518e-02, 1.3405e-02],\n",
            "          [1.4512e-02, 1.3397e-02, 1.3213e-02,  ..., 1.3292e-02,\n",
            "           1.3815e-02, 1.3020e-02],\n",
            "          [1.2348e-02, 1.1778e-02, 1.1504e-02,  ..., 1.1312e-02,\n",
            "           1.1529e-02, 1.1164e-02]],\n",
            "\n",
            "         [[9.9959e-03, 8.8976e-03, 8.2792e-03,  ..., 8.3861e-03,\n",
            "           8.8622e-03, 1.0844e-02],\n",
            "          [1.5306e-02, 1.3922e-02, 1.3400e-02,  ..., 1.3430e-02,\n",
            "           1.2379e-02, 1.0782e-02],\n",
            "          [1.5713e-02, 1.3341e-02, 1.3062e-02,  ..., 1.3212e-02,\n",
            "           1.2036e-02, 1.0592e-02],\n",
            "          ...,\n",
            "          [1.5735e-02, 1.3410e-02, 1.3089e-02,  ..., 1.3202e-02,\n",
            "           1.1892e-02, 1.0519e-02],\n",
            "          [1.5656e-02, 1.4027e-02, 1.3883e-02,  ..., 1.4004e-02,\n",
            "           1.2868e-02, 1.1820e-02],\n",
            "          [1.9419e-02, 1.9365e-02, 1.9447e-02,  ..., 1.9558e-02,\n",
            "           1.8361e-02, 1.3542e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1.1131e-02, 6.2480e-03, 7.0157e-03,  ..., 6.8865e-03,\n",
            "           7.1755e-03, 6.2050e-03],\n",
            "          [9.1156e-03, 5.3832e-03, 5.7191e-03,  ..., 5.6711e-03,\n",
            "           6.6888e-03, 7.3546e-03],\n",
            "          [8.8959e-03, 5.6942e-03, 5.8115e-03,  ..., 5.6565e-03,\n",
            "           6.7784e-03, 6.8795e-03],\n",
            "          ...,\n",
            "          [8.9382e-03, 5.6446e-03, 5.9122e-03,  ..., 5.7695e-03,\n",
            "           6.9093e-03, 6.8575e-03],\n",
            "          [9.1866e-03, 6.1702e-03, 6.2489e-03,  ..., 6.2149e-03,\n",
            "           6.8751e-03, 6.5158e-03],\n",
            "          [9.0017e-03, 1.0382e-02, 9.7699e-03,  ..., 9.7851e-03,\n",
            "           1.0773e-02, 1.1171e-02]],\n",
            "\n",
            "         [[2.9526e-03, 4.0341e-03, 3.8116e-03,  ..., 3.6042e-03,\n",
            "           4.4974e-03, 1.9333e-03],\n",
            "          [3.3319e-03, 4.9666e-03, 5.5193e-03,  ..., 5.2655e-03,\n",
            "           6.3118e-03, 3.6244e-03],\n",
            "          [2.4975e-03, 3.7022e-03, 4.6186e-03,  ..., 4.3135e-03,\n",
            "           5.6570e-03, 3.1820e-03],\n",
            "          ...,\n",
            "          [2.4244e-03, 3.5474e-03, 4.4842e-03,  ..., 4.0458e-03,\n",
            "           5.4605e-03, 2.8363e-03],\n",
            "          [2.9722e-03, 4.9069e-03, 5.5456e-03,  ..., 5.2998e-03,\n",
            "           6.6481e-03, 3.4178e-03],\n",
            "          [8.5811e-04, 1.7158e-03, 2.7274e-03,  ..., 2.5826e-03,\n",
            "           3.9334e-03, 2.5789e-03]],\n",
            "\n",
            "         [[1.0272e-02, 1.3837e-02, 1.3711e-02,  ..., 1.3803e-02,\n",
            "           1.3879e-02, 1.2838e-02],\n",
            "          [1.1870e-02, 1.5559e-02, 1.4676e-02,  ..., 1.4869e-02,\n",
            "           1.4940e-02, 1.2141e-02],\n",
            "          [1.1564e-02, 1.5071e-02, 1.4168e-02,  ..., 1.4419e-02,\n",
            "           1.4593e-02, 1.2082e-02],\n",
            "          ...,\n",
            "          [1.1636e-02, 1.4819e-02, 1.3986e-02,  ..., 1.4270e-02,\n",
            "           1.4435e-02, 1.2051e-02],\n",
            "          [1.1338e-02, 1.4652e-02, 1.3917e-02,  ..., 1.4112e-02,\n",
            "           1.4407e-02, 1.2425e-02],\n",
            "          [1.1369e-02, 1.1829e-02, 1.1644e-02,  ..., 1.1701e-02,\n",
            "           1.1669e-02, 1.1316e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 4.5391e-05],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.3059e-04, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           4.2215e-04, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [1.0296e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[1.3640e-02, 1.2813e-02, 1.3775e-02,  ..., 1.4070e-02,\n",
            "           1.4454e-02, 1.3400e-02],\n",
            "          [1.4619e-02, 1.3341e-02, 1.3967e-02,  ..., 1.4083e-02,\n",
            "           1.4354e-02, 1.3330e-02],\n",
            "          [1.4636e-02, 1.3728e-02, 1.4045e-02,  ..., 1.3994e-02,\n",
            "           1.4632e-02, 1.3494e-02],\n",
            "          ...,\n",
            "          [1.4723e-02, 1.3697e-02, 1.4033e-02,  ..., 1.4086e-02,\n",
            "           1.4508e-02, 1.3399e-02],\n",
            "          [1.4512e-02, 1.3397e-02, 1.3213e-02,  ..., 1.3283e-02,\n",
            "           1.3813e-02, 1.3020e-02],\n",
            "          [1.2348e-02, 1.1778e-02, 1.1504e-02,  ..., 1.1312e-02,\n",
            "           1.1527e-02, 1.1166e-02]],\n",
            "\n",
            "         [[9.9947e-03, 8.8997e-03, 8.2790e-03,  ..., 8.3824e-03,\n",
            "           8.8585e-03, 1.0840e-02],\n",
            "          [1.5303e-02, 1.3921e-02, 1.3397e-02,  ..., 1.3436e-02,\n",
            "           1.2386e-02, 1.0785e-02],\n",
            "          [1.5711e-02, 1.3341e-02, 1.3059e-02,  ..., 1.3217e-02,\n",
            "           1.2036e-02, 1.0585e-02],\n",
            "          ...,\n",
            "          [1.5735e-02, 1.3410e-02, 1.3089e-02,  ..., 1.3206e-02,\n",
            "           1.1891e-02, 1.0522e-02],\n",
            "          [1.5656e-02, 1.4027e-02, 1.3883e-02,  ..., 1.4008e-02,\n",
            "           1.2864e-02, 1.1821e-02],\n",
            "          [1.9419e-02, 1.9365e-02, 1.9447e-02,  ..., 1.9557e-02,\n",
            "           1.8359e-02, 1.3543e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1.1132e-02, 6.2536e-03, 7.0197e-03,  ..., 6.8696e-03,\n",
            "           7.1613e-03, 6.2004e-03],\n",
            "          [9.1165e-03, 5.3900e-03, 5.7206e-03,  ..., 5.6560e-03,\n",
            "           6.6820e-03, 7.3389e-03],\n",
            "          [8.8954e-03, 5.6983e-03, 5.8118e-03,  ..., 5.6522e-03,\n",
            "           6.7736e-03, 6.8742e-03],\n",
            "          ...,\n",
            "          [8.9381e-03, 5.6448e-03, 5.9125e-03,  ..., 5.7712e-03,\n",
            "           6.9153e-03, 6.8658e-03],\n",
            "          [9.1866e-03, 6.1705e-03, 6.2490e-03,  ..., 6.2208e-03,\n",
            "           6.8754e-03, 6.5185e-03],\n",
            "          [9.0017e-03, 1.0382e-02, 9.7700e-03,  ..., 9.7932e-03,\n",
            "           1.0771e-02, 1.1173e-02]],\n",
            "\n",
            "         [[2.9519e-03, 4.0332e-03, 3.8123e-03,  ..., 3.6041e-03,\n",
            "           4.4982e-03, 1.9353e-03],\n",
            "          [3.3293e-03, 4.9692e-03, 5.5189e-03,  ..., 5.2577e-03,\n",
            "           6.2952e-03, 3.6188e-03],\n",
            "          [2.4951e-03, 3.7011e-03, 4.6148e-03,  ..., 4.3151e-03,\n",
            "           5.6466e-03, 3.1827e-03],\n",
            "          ...,\n",
            "          [2.4243e-03, 3.5472e-03, 4.4844e-03,  ..., 4.0462e-03,\n",
            "           5.4622e-03, 2.8408e-03],\n",
            "          [2.9722e-03, 4.9069e-03, 5.5455e-03,  ..., 5.3052e-03,\n",
            "           6.6458e-03, 3.4149e-03],\n",
            "          [8.5810e-04, 1.7157e-03, 2.7275e-03,  ..., 2.5863e-03,\n",
            "           3.9419e-03, 2.5831e-03]],\n",
            "\n",
            "         [[1.0271e-02, 1.3837e-02, 1.3712e-02,  ..., 1.3809e-02,\n",
            "           1.3881e-02, 1.2839e-02],\n",
            "          [1.1869e-02, 1.5555e-02, 1.4678e-02,  ..., 1.4879e-02,\n",
            "           1.4941e-02, 1.2142e-02],\n",
            "          [1.1562e-02, 1.5069e-02, 1.4169e-02,  ..., 1.4419e-02,\n",
            "           1.4592e-02, 1.2074e-02],\n",
            "          ...,\n",
            "          [1.1635e-02, 1.4819e-02, 1.3986e-02,  ..., 1.4270e-02,\n",
            "           1.4435e-02, 1.2058e-02],\n",
            "          [1.1338e-02, 1.4653e-02, 1.3917e-02,  ..., 1.4113e-02,\n",
            "           1.4406e-02, 1.2424e-02],\n",
            "          [1.1369e-02, 1.1828e-02, 1.1644e-02,  ..., 1.1697e-02,\n",
            "           1.1673e-02, 1.1316e-02]]],\n",
            "\n",
            "\n",
            "        [[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 4.6604e-05],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.3674e-04, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           4.4355e-04, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [1.0297e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[1.3642e-02, 1.2818e-02, 1.3777e-02,  ..., 1.4074e-02,\n",
            "           1.4456e-02, 1.3401e-02],\n",
            "          [1.4624e-02, 1.3348e-02, 1.3969e-02,  ..., 1.4085e-02,\n",
            "           1.4355e-02, 1.3332e-02],\n",
            "          [1.4641e-02, 1.3732e-02, 1.4050e-02,  ..., 1.3998e-02,\n",
            "           1.4632e-02, 1.3491e-02],\n",
            "          ...,\n",
            "          [1.4723e-02, 1.3697e-02, 1.4033e-02,  ..., 1.4086e-02,\n",
            "           1.4513e-02, 1.3406e-02],\n",
            "          [1.4512e-02, 1.3398e-02, 1.3213e-02,  ..., 1.3295e-02,\n",
            "           1.3820e-02, 1.3019e-02],\n",
            "          [1.2348e-02, 1.1778e-02, 1.1504e-02,  ..., 1.1315e-02,\n",
            "           1.1529e-02, 1.1166e-02]],\n",
            "\n",
            "         [[9.9959e-03, 8.8976e-03, 8.2792e-03,  ..., 8.3811e-03,\n",
            "           8.8595e-03, 1.0843e-02],\n",
            "          [1.5306e-02, 1.3921e-02, 1.3399e-02,  ..., 1.3434e-02,\n",
            "           1.2387e-02, 1.0788e-02],\n",
            "          [1.5713e-02, 1.3341e-02, 1.3062e-02,  ..., 1.3215e-02,\n",
            "           1.2039e-02, 1.0583e-02],\n",
            "          ...,\n",
            "          [1.5735e-02, 1.3410e-02, 1.3089e-02,  ..., 1.3209e-02,\n",
            "           1.1888e-02, 1.0519e-02],\n",
            "          [1.5656e-02, 1.4027e-02, 1.3883e-02,  ..., 1.4004e-02,\n",
            "           1.2872e-02, 1.1824e-02],\n",
            "          [1.9419e-02, 1.9365e-02, 1.9447e-02,  ..., 1.9555e-02,\n",
            "           1.8362e-02, 1.3543e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1.1131e-02, 6.2481e-03, 7.0156e-03,  ..., 6.8750e-03,\n",
            "           7.1654e-03, 6.2013e-03],\n",
            "          [9.1155e-03, 5.3833e-03, 5.7191e-03,  ..., 5.6561e-03,\n",
            "           6.6854e-03, 7.3448e-03],\n",
            "          [8.8957e-03, 5.6942e-03, 5.8113e-03,  ..., 5.6487e-03,\n",
            "           6.7792e-03, 6.8795e-03],\n",
            "          ...,\n",
            "          [8.9381e-03, 5.6446e-03, 5.9121e-03,  ..., 5.7743e-03,\n",
            "           6.9186e-03, 6.8651e-03],\n",
            "          [9.1867e-03, 6.1705e-03, 6.2490e-03,  ..., 6.2235e-03,\n",
            "           6.8711e-03, 6.5201e-03],\n",
            "          [9.0018e-03, 1.0382e-02, 9.7701e-03,  ..., 9.7910e-03,\n",
            "           1.0779e-02, 1.1171e-02]],\n",
            "\n",
            "         [[2.9526e-03, 4.0339e-03, 3.8115e-03,  ..., 3.6048e-03,\n",
            "           4.4992e-03, 1.9357e-03],\n",
            "          [3.3317e-03, 4.9666e-03, 5.5192e-03,  ..., 5.2582e-03,\n",
            "           6.2932e-03, 3.6186e-03],\n",
            "          [2.4975e-03, 3.7020e-03, 4.6185e-03,  ..., 4.3112e-03,\n",
            "           5.6479e-03, 3.1809e-03],\n",
            "          ...,\n",
            "          [2.4243e-03, 3.5472e-03, 4.4844e-03,  ..., 4.0456e-03,\n",
            "           5.4625e-03, 2.8332e-03],\n",
            "          [2.9722e-03, 4.9069e-03, 5.5457e-03,  ..., 5.3069e-03,\n",
            "           6.6452e-03, 3.4158e-03],\n",
            "          [8.5808e-04, 1.7157e-03, 2.7273e-03,  ..., 2.5784e-03,\n",
            "           3.9380e-03, 2.5812e-03]],\n",
            "\n",
            "         [[1.0273e-02, 1.3837e-02, 1.3712e-02,  ..., 1.3806e-02,\n",
            "           1.3872e-02, 1.2836e-02],\n",
            "          [1.1870e-02, 1.5559e-02, 1.4676e-02,  ..., 1.4876e-02,\n",
            "           1.4940e-02, 1.2143e-02],\n",
            "          [1.1564e-02, 1.5071e-02, 1.4168e-02,  ..., 1.4420e-02,\n",
            "           1.4592e-02, 1.2074e-02],\n",
            "          ...,\n",
            "          [1.1635e-02, 1.4819e-02, 1.3986e-02,  ..., 1.4261e-02,\n",
            "           1.4433e-02, 1.2054e-02],\n",
            "          [1.1338e-02, 1.4652e-02, 1.3917e-02,  ..., 1.4110e-02,\n",
            "           1.4401e-02, 1.2426e-02],\n",
            "          [1.1369e-02, 1.1829e-02, 1.1644e-02,  ..., 1.1701e-02,\n",
            "           1.1667e-02, 1.1316e-02]]],\n",
            "\n",
            "\n",
            "        [[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 4.3182e-05],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.3367e-04, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           4.2709e-04, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [1.0314e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[1.3643e-02, 1.2818e-02, 1.3778e-02,  ..., 1.4073e-02,\n",
            "           1.4460e-02, 1.3398e-02],\n",
            "          [1.4625e-02, 1.3349e-02, 1.3970e-02,  ..., 1.4091e-02,\n",
            "           1.4367e-02, 1.3334e-02],\n",
            "          [1.4641e-02, 1.3732e-02, 1.4051e-02,  ..., 1.3993e-02,\n",
            "           1.4636e-02, 1.3497e-02],\n",
            "          ...,\n",
            "          [1.4721e-02, 1.3685e-02, 1.4026e-02,  ..., 1.4094e-02,\n",
            "           1.4520e-02, 1.3403e-02],\n",
            "          [1.4511e-02, 1.3398e-02, 1.3198e-02,  ..., 1.3295e-02,\n",
            "           1.3819e-02, 1.3022e-02],\n",
            "          [1.2345e-02, 1.1771e-02, 1.1489e-02,  ..., 1.1317e-02,\n",
            "           1.1525e-02, 1.1163e-02]],\n",
            "\n",
            "         [[9.9958e-03, 8.8972e-03, 8.2785e-03,  ..., 8.3772e-03,\n",
            "           8.8580e-03, 1.0840e-02],\n",
            "          [1.5306e-02, 1.3921e-02, 1.3399e-02,  ..., 1.3428e-02,\n",
            "           1.2384e-02, 1.0786e-02],\n",
            "          [1.5713e-02, 1.3340e-02, 1.3061e-02,  ..., 1.3217e-02,\n",
            "           1.2034e-02, 1.0584e-02],\n",
            "          ...,\n",
            "          [1.5731e-02, 1.3413e-02, 1.3095e-02,  ..., 1.3203e-02,\n",
            "           1.1891e-02, 1.0519e-02],\n",
            "          [1.5654e-02, 1.4029e-02, 1.3883e-02,  ..., 1.4003e-02,\n",
            "           1.2867e-02, 1.1820e-02],\n",
            "          [1.9420e-02, 1.9363e-02, 1.9450e-02,  ..., 1.9557e-02,\n",
            "           1.8359e-02, 1.3543e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1.1130e-02, 6.2472e-03, 7.0150e-03,  ..., 6.8673e-03,\n",
            "           7.1597e-03, 6.1972e-03],\n",
            "          [9.1148e-03, 5.3823e-03, 5.7186e-03,  ..., 5.6487e-03,\n",
            "           6.6795e-03, 7.3382e-03],\n",
            "          [8.8956e-03, 5.6937e-03, 5.8121e-03,  ..., 5.6501e-03,\n",
            "           6.7705e-03, 6.8763e-03],\n",
            "          ...,\n",
            "          [8.9295e-03, 5.6388e-03, 5.9096e-03,  ..., 5.7668e-03,\n",
            "           6.9087e-03, 6.8588e-03],\n",
            "          [9.1815e-03, 6.1698e-03, 6.2444e-03,  ..., 6.2113e-03,\n",
            "           6.8679e-03, 6.5156e-03],\n",
            "          [8.9967e-03, 1.0380e-02, 9.7600e-03,  ..., 9.7833e-03,\n",
            "           1.0771e-02, 1.1172e-02]],\n",
            "\n",
            "         [[2.9526e-03, 4.0337e-03, 3.8120e-03,  ..., 3.6068e-03,\n",
            "           4.4992e-03, 1.9363e-03],\n",
            "          [3.3318e-03, 4.9661e-03, 5.5199e-03,  ..., 5.2536e-03,\n",
            "           6.2925e-03, 3.6198e-03],\n",
            "          [2.4973e-03, 3.7019e-03, 4.6193e-03,  ..., 4.3118e-03,\n",
            "           5.6395e-03, 3.1760e-03],\n",
            "          ...,\n",
            "          [2.4295e-03, 3.5545e-03, 4.4888e-03,  ..., 4.0478e-03,\n",
            "           5.4588e-03, 2.8316e-03],\n",
            "          [2.9719e-03, 4.9075e-03, 5.5464e-03,  ..., 5.2945e-03,\n",
            "           6.6387e-03, 3.4172e-03],\n",
            "          [8.5743e-04, 1.7201e-03, 2.7276e-03,  ..., 2.5843e-03,\n",
            "           3.9364e-03, 2.5803e-03]],\n",
            "\n",
            "         [[1.0273e-02, 1.3838e-02, 1.3712e-02,  ..., 1.3812e-02,\n",
            "           1.3881e-02, 1.2842e-02],\n",
            "          [1.1871e-02, 1.5559e-02, 1.4677e-02,  ..., 1.4882e-02,\n",
            "           1.4938e-02, 1.2144e-02],\n",
            "          [1.1564e-02, 1.5072e-02, 1.4170e-02,  ..., 1.4420e-02,\n",
            "           1.4595e-02, 1.2071e-02],\n",
            "          ...,\n",
            "          [1.1645e-02, 1.4835e-02, 1.4001e-02,  ..., 1.4272e-02,\n",
            "           1.4435e-02, 1.2050e-02],\n",
            "          [1.1342e-02, 1.4659e-02, 1.3930e-02,  ..., 1.4111e-02,\n",
            "           1.4409e-02, 1.2428e-02],\n",
            "          [1.1372e-02, 1.1835e-02, 1.1649e-02,  ..., 1.1703e-02,\n",
            "           1.1669e-02, 1.1315e-02]]]], device='cuda:0',\n",
            "       grad_fn=<ReluBackward0>)\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mup1\u001b[0m \u001b[0;34m= tensor([[[[-0.0053, -0.0066, -0.0055,  ..., -0.0053, -0.0071, -0.0052],\n",
            "          [-0.0053, -0.0015, -0.0045,  ..., -0.0045, -0.0013, -0.0045],\n",
            "          [-0.0050, -0.0068, -0.0055,  ..., -0.0054, -0.0077, -0.0055],\n",
            "          ...,\n",
            "          [-0.0050, -0.0072, -0.0055,  ..., -0.0055, -0.0077, -0.0055],\n",
            "          [-0.0055, -0.0008, -0.0047,  ..., -0.0048, -0.0010, -0.0048],\n",
            "          [-0.0053, -0.0071, -0.0058,  ..., -0.0059, -0.0079, -0.0055]],\n",
            "\n",
            "         [[ 0.0109,  0.0117,  0.0109,  ...,  0.0110,  0.0107,  0.0112],\n",
            "          [ 0.0106,  0.0127,  0.0102,  ...,  0.0104,  0.0128,  0.0104],\n",
            "          [ 0.0111,  0.0117,  0.0108,  ...,  0.0111,  0.0108,  0.0111],\n",
            "          ...,\n",
            "          [ 0.0113,  0.0116,  0.0111,  ...,  0.0113,  0.0108,  0.0112],\n",
            "          [ 0.0113,  0.0128,  0.0113,  ...,  0.0114,  0.0133,  0.0110],\n",
            "          [ 0.0113,  0.0113,  0.0113,  ...,  0.0115,  0.0108,  0.0114]],\n",
            "\n",
            "         [[ 0.0061,  0.0090,  0.0062,  ...,  0.0064,  0.0092,  0.0066],\n",
            "          [ 0.0089,  0.0069,  0.0088,  ...,  0.0089,  0.0070,  0.0091],\n",
            "          [ 0.0061,  0.0092,  0.0061,  ...,  0.0064,  0.0094,  0.0064],\n",
            "          ...,\n",
            "          [ 0.0062,  0.0094,  0.0062,  ...,  0.0064,  0.0094,  0.0064],\n",
            "          [ 0.0088,  0.0066,  0.0087,  ...,  0.0089,  0.0074,  0.0089],\n",
            "          [ 0.0062,  0.0094,  0.0062,  ...,  0.0061,  0.0096,  0.0064]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0029, -0.0018, -0.0029,  ..., -0.0030, -0.0025, -0.0027],\n",
            "          [-0.0027, -0.0064, -0.0027,  ..., -0.0027, -0.0059, -0.0022],\n",
            "          [-0.0028, -0.0021, -0.0027,  ..., -0.0029, -0.0029, -0.0028],\n",
            "          ...,\n",
            "          [-0.0027, -0.0020, -0.0025,  ..., -0.0027, -0.0028, -0.0026],\n",
            "          [-0.0024, -0.0071, -0.0029,  ..., -0.0029, -0.0065, -0.0029],\n",
            "          [-0.0023, -0.0023, -0.0023,  ..., -0.0024, -0.0026, -0.0026]],\n",
            "\n",
            "         [[-0.0056, -0.0066, -0.0061,  ..., -0.0059, -0.0062, -0.0064],\n",
            "          [-0.0071, -0.0055, -0.0065,  ..., -0.0066, -0.0051, -0.0060],\n",
            "          [-0.0056, -0.0064, -0.0062,  ..., -0.0060, -0.0059, -0.0065],\n",
            "          ...,\n",
            "          [-0.0057, -0.0064, -0.0062,  ..., -0.0061, -0.0060, -0.0065],\n",
            "          [-0.0075, -0.0049, -0.0070,  ..., -0.0070, -0.0049, -0.0063],\n",
            "          [-0.0062, -0.0061, -0.0063,  ..., -0.0062, -0.0057, -0.0064]],\n",
            "\n",
            "         [[-0.0127, -0.0137, -0.0128,  ..., -0.0128, -0.0135, -0.0125],\n",
            "          [-0.0129, -0.0097, -0.0128,  ..., -0.0128, -0.0102, -0.0127],\n",
            "          [-0.0122, -0.0142, -0.0125,  ..., -0.0126, -0.0142, -0.0126],\n",
            "          ...,\n",
            "          [-0.0123, -0.0143, -0.0126,  ..., -0.0126, -0.0144, -0.0126],\n",
            "          [-0.0126, -0.0108, -0.0122,  ..., -0.0121, -0.0109, -0.0121],\n",
            "          [-0.0120, -0.0137, -0.0123,  ..., -0.0122, -0.0140, -0.0126]]],\n",
            "\n",
            "\n",
            "        [[[-0.0053, -0.0066, -0.0055,  ..., -0.0053, -0.0071, -0.0052],\n",
            "          [-0.0053, -0.0015, -0.0045,  ..., -0.0045, -0.0013, -0.0045],\n",
            "          [-0.0050, -0.0068, -0.0055,  ..., -0.0054, -0.0077, -0.0055],\n",
            "          ...,\n",
            "          [-0.0050, -0.0072, -0.0055,  ..., -0.0055, -0.0077, -0.0055],\n",
            "          [-0.0055, -0.0008, -0.0047,  ..., -0.0048, -0.0010, -0.0048],\n",
            "          [-0.0053, -0.0071, -0.0058,  ..., -0.0059, -0.0079, -0.0055]],\n",
            "\n",
            "         [[ 0.0109,  0.0117,  0.0109,  ...,  0.0110,  0.0107,  0.0112],\n",
            "          [ 0.0106,  0.0127,  0.0102,  ...,  0.0104,  0.0128,  0.0104],\n",
            "          [ 0.0111,  0.0117,  0.0108,  ...,  0.0111,  0.0108,  0.0111],\n",
            "          ...,\n",
            "          [ 0.0113,  0.0116,  0.0111,  ...,  0.0113,  0.0108,  0.0112],\n",
            "          [ 0.0113,  0.0128,  0.0113,  ...,  0.0114,  0.0133,  0.0110],\n",
            "          [ 0.0113,  0.0113,  0.0113,  ...,  0.0115,  0.0108,  0.0114]],\n",
            "\n",
            "         [[ 0.0061,  0.0090,  0.0062,  ...,  0.0064,  0.0092,  0.0066],\n",
            "          [ 0.0089,  0.0069,  0.0088,  ...,  0.0089,  0.0070,  0.0091],\n",
            "          [ 0.0061,  0.0092,  0.0061,  ...,  0.0064,  0.0094,  0.0064],\n",
            "          ...,\n",
            "          [ 0.0062,  0.0094,  0.0062,  ...,  0.0064,  0.0094,  0.0064],\n",
            "          [ 0.0088,  0.0066,  0.0087,  ...,  0.0089,  0.0074,  0.0089],\n",
            "          [ 0.0062,  0.0094,  0.0062,  ...,  0.0061,  0.0096,  0.0064]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0029, -0.0018, -0.0029,  ..., -0.0030, -0.0025, -0.0027],\n",
            "          [-0.0027, -0.0064, -0.0027,  ..., -0.0027, -0.0059, -0.0022],\n",
            "          [-0.0028, -0.0021, -0.0027,  ..., -0.0029, -0.0029, -0.0028],\n",
            "          ...,\n",
            "          [-0.0027, -0.0020, -0.0025,  ..., -0.0027, -0.0028, -0.0026],\n",
            "          [-0.0024, -0.0071, -0.0029,  ..., -0.0029, -0.0065, -0.0029],\n",
            "          [-0.0023, -0.0023, -0.0023,  ..., -0.0024, -0.0026, -0.0026]],\n",
            "\n",
            "         [[-0.0056, -0.0066, -0.0061,  ..., -0.0059, -0.0062, -0.0064],\n",
            "          [-0.0071, -0.0055, -0.0065,  ..., -0.0066, -0.0051, -0.0060],\n",
            "          [-0.0056, -0.0064, -0.0062,  ..., -0.0060, -0.0059, -0.0065],\n",
            "          ...,\n",
            "          [-0.0057, -0.0064, -0.0062,  ..., -0.0061, -0.0060, -0.0065],\n",
            "          [-0.0075, -0.0049, -0.0070,  ..., -0.0070, -0.0049, -0.0063],\n",
            "          [-0.0062, -0.0061, -0.0063,  ..., -0.0062, -0.0057, -0.0064]],\n",
            "\n",
            "         [[-0.0127, -0.0137, -0.0128,  ..., -0.0128, -0.0135, -0.0125],\n",
            "          [-0.0129, -0.0097, -0.0128,  ..., -0.0128, -0.0102, -0.0127],\n",
            "          [-0.0122, -0.0142, -0.0125,  ..., -0.0126, -0.0142, -0.0126],\n",
            "          ...,\n",
            "          [-0.0123, -0.0143, -0.0126,  ..., -0.0126, -0.0144, -0.0126],\n",
            "          [-0.0126, -0.0108, -0.0122,  ..., -0.0121, -0.0109, -0.0121],\n",
            "          [-0.0120, -0.0137, -0.0123,  ..., -0.0122, -0.0140, -0.0126]]],\n",
            "\n",
            "\n",
            "        [[[-0.0053, -0.0066, -0.0055,  ..., -0.0053, -0.0071, -0.0052],\n",
            "          [-0.0053, -0.0015, -0.0045,  ..., -0.0045, -0.0013, -0.0045],\n",
            "          [-0.0050, -0.0068, -0.0055,  ..., -0.0054, -0.0077, -0.0055],\n",
            "          ...,\n",
            "          [-0.0050, -0.0072, -0.0055,  ..., -0.0055, -0.0077, -0.0055],\n",
            "          [-0.0055, -0.0008, -0.0047,  ..., -0.0048, -0.0010, -0.0048],\n",
            "          [-0.0053, -0.0071, -0.0058,  ..., -0.0059, -0.0079, -0.0055]],\n",
            "\n",
            "         [[ 0.0109,  0.0117,  0.0109,  ...,  0.0110,  0.0107,  0.0112],\n",
            "          [ 0.0106,  0.0127,  0.0102,  ...,  0.0104,  0.0128,  0.0104],\n",
            "          [ 0.0111,  0.0117,  0.0108,  ...,  0.0111,  0.0108,  0.0111],\n",
            "          ...,\n",
            "          [ 0.0113,  0.0116,  0.0111,  ...,  0.0113,  0.0108,  0.0112],\n",
            "          [ 0.0113,  0.0128,  0.0113,  ...,  0.0114,  0.0133,  0.0110],\n",
            "          [ 0.0113,  0.0113,  0.0113,  ...,  0.0115,  0.0108,  0.0114]],\n",
            "\n",
            "         [[ 0.0061,  0.0090,  0.0062,  ...,  0.0064,  0.0092,  0.0066],\n",
            "          [ 0.0089,  0.0069,  0.0088,  ...,  0.0089,  0.0070,  0.0091],\n",
            "          [ 0.0061,  0.0092,  0.0061,  ...,  0.0064,  0.0094,  0.0064],\n",
            "          ...,\n",
            "          [ 0.0062,  0.0094,  0.0062,  ...,  0.0064,  0.0094,  0.0064],\n",
            "          [ 0.0088,  0.0066,  0.0087,  ...,  0.0089,  0.0074,  0.0089],\n",
            "          [ 0.0062,  0.0094,  0.0062,  ...,  0.0061,  0.0096,  0.0064]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0029, -0.0018, -0.0029,  ..., -0.0030, -0.0025, -0.0027],\n",
            "          [-0.0027, -0.0064, -0.0027,  ..., -0.0027, -0.0059, -0.0022],\n",
            "          [-0.0028, -0.0021, -0.0027,  ..., -0.0029, -0.0029, -0.0028],\n",
            "          ...,\n",
            "          [-0.0027, -0.0020, -0.0025,  ..., -0.0027, -0.0028, -0.0026],\n",
            "          [-0.0024, -0.0071, -0.0029,  ..., -0.0029, -0.0065, -0.0029],\n",
            "          [-0.0023, -0.0023, -0.0023,  ..., -0.0024, -0.0026, -0.0026]],\n",
            "\n",
            "         [[-0.0056, -0.0066, -0.0061,  ..., -0.0059, -0.0062, -0.0064],\n",
            "          [-0.0071, -0.0055, -0.0065,  ..., -0.0066, -0.0051, -0.0060],\n",
            "          [-0.0056, -0.0064, -0.0062,  ..., -0.0060, -0.0059, -0.0065],\n",
            "          ...,\n",
            "          [-0.0057, -0.0064, -0.0062,  ..., -0.0061, -0.0060, -0.0065],\n",
            "          [-0.0075, -0.0049, -0.0070,  ..., -0.0070, -0.0049, -0.0063],\n",
            "          [-0.0062, -0.0061, -0.0063,  ..., -0.0062, -0.0057, -0.0064]],\n",
            "\n",
            "         [[-0.0127, -0.0137, -0.0128,  ..., -0.0128, -0.0135, -0.0125],\n",
            "          [-0.0129, -0.0097, -0.0128,  ..., -0.0128, -0.0102, -0.0127],\n",
            "          [-0.0122, -0.0142, -0.0125,  ..., -0.0126, -0.0142, -0.0126],\n",
            "          ...,\n",
            "          [-0.0123, -0.0143, -0.0126,  ..., -0.0126, -0.0144, -0.0126],\n",
            "          [-0.0126, -0.0108, -0.0122,  ..., -0.0121, -0.0109, -0.0121],\n",
            "          [-0.0120, -0.0137, -0.0123,  ..., -0.0122, -0.0140, -0.0126]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0053, -0.0066, -0.0055,  ..., -0.0053, -0.0071, -0.0052],\n",
            "          [-0.0053, -0.0015, -0.0045,  ..., -0.0045, -0.0013, -0.0045],\n",
            "          [-0.0050, -0.0068, -0.0055,  ..., -0.0054, -0.0077, -0.0055],\n",
            "          ...,\n",
            "          [-0.0050, -0.0072, -0.0055,  ..., -0.0055, -0.0077, -0.0055],\n",
            "          [-0.0055, -0.0008, -0.0047,  ..., -0.0048, -0.0010, -0.0048],\n",
            "          [-0.0053, -0.0071, -0.0058,  ..., -0.0059, -0.0079, -0.0055]],\n",
            "\n",
            "         [[ 0.0109,  0.0117,  0.0109,  ...,  0.0110,  0.0107,  0.0112],\n",
            "          [ 0.0106,  0.0127,  0.0102,  ...,  0.0104,  0.0128,  0.0104],\n",
            "          [ 0.0111,  0.0117,  0.0108,  ...,  0.0111,  0.0108,  0.0111],\n",
            "          ...,\n",
            "          [ 0.0113,  0.0116,  0.0111,  ...,  0.0113,  0.0108,  0.0112],\n",
            "          [ 0.0113,  0.0128,  0.0113,  ...,  0.0114,  0.0133,  0.0110],\n",
            "          [ 0.0113,  0.0113,  0.0113,  ...,  0.0115,  0.0108,  0.0114]],\n",
            "\n",
            "         [[ 0.0061,  0.0090,  0.0062,  ...,  0.0064,  0.0092,  0.0066],\n",
            "          [ 0.0089,  0.0069,  0.0088,  ...,  0.0089,  0.0070,  0.0091],\n",
            "          [ 0.0061,  0.0092,  0.0061,  ...,  0.0064,  0.0094,  0.0064],\n",
            "          ...,\n",
            "          [ 0.0062,  0.0094,  0.0062,  ...,  0.0064,  0.0094,  0.0064],\n",
            "          [ 0.0088,  0.0066,  0.0087,  ...,  0.0089,  0.0074,  0.0089],\n",
            "          [ 0.0062,  0.0094,  0.0062,  ...,  0.0061,  0.0096,  0.0064]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0029, -0.0018, -0.0029,  ..., -0.0030, -0.0025, -0.0027],\n",
            "          [-0.0027, -0.0064, -0.0027,  ..., -0.0027, -0.0059, -0.0022],\n",
            "          [-0.0028, -0.0021, -0.0027,  ..., -0.0029, -0.0029, -0.0028],\n",
            "          ...,\n",
            "          [-0.0027, -0.0020, -0.0025,  ..., -0.0027, -0.0028, -0.0026],\n",
            "          [-0.0024, -0.0071, -0.0029,  ..., -0.0029, -0.0065, -0.0029],\n",
            "          [-0.0023, -0.0023, -0.0023,  ..., -0.0024, -0.0026, -0.0026]],\n",
            "\n",
            "         [[-0.0056, -0.0066, -0.0061,  ..., -0.0059, -0.0062, -0.0064],\n",
            "          [-0.0071, -0.0055, -0.0065,  ..., -0.0066, -0.0051, -0.0060],\n",
            "          [-0.0056, -0.0064, -0.0062,  ..., -0.0060, -0.0059, -0.0065],\n",
            "          ...,\n",
            "          [-0.0057, -0.0064, -0.0062,  ..., -0.0061, -0.0060, -0.0065],\n",
            "          [-0.0075, -0.0049, -0.0070,  ..., -0.0070, -0.0049, -0.0063],\n",
            "          [-0.0062, -0.0061, -0.0063,  ..., -0.0062, -0.0057, -0.0064]],\n",
            "\n",
            "         [[-0.0127, -0.0137, -0.0128,  ..., -0.0128, -0.0135, -0.0125],\n",
            "          [-0.0129, -0.0097, -0.0128,  ..., -0.0128, -0.0102, -0.0127],\n",
            "          [-0.0122, -0.0142, -0.0125,  ..., -0.0126, -0.0142, -0.0126],\n",
            "          ...,\n",
            "          [-0.0123, -0.0143, -0.0126,  ..., -0.0126, -0.0144, -0.0126],\n",
            "          [-0.0126, -0.0108, -0.0122,  ..., -0.0121, -0.0109, -0.0121],\n",
            "          [-0.0120, -0.0137, -0.0123,  ..., -0.0122, -0.0140, -0.0126]]],\n",
            "\n",
            "\n",
            "        [[[-0.0053, -0.0066, -0.0055,  ..., -0.0053, -0.0071, -0.0052],\n",
            "          [-0.0053, -0.0015, -0.0045,  ..., -0.0045, -0.0013, -0.0045],\n",
            "          [-0.0050, -0.0068, -0.0055,  ..., -0.0054, -0.0077, -0.0055],\n",
            "          ...,\n",
            "          [-0.0050, -0.0072, -0.0055,  ..., -0.0055, -0.0077, -0.0055],\n",
            "          [-0.0055, -0.0008, -0.0047,  ..., -0.0048, -0.0010, -0.0048],\n",
            "          [-0.0053, -0.0071, -0.0058,  ..., -0.0059, -0.0079, -0.0055]],\n",
            "\n",
            "         [[ 0.0109,  0.0117,  0.0109,  ...,  0.0110,  0.0107,  0.0112],\n",
            "          [ 0.0106,  0.0127,  0.0102,  ...,  0.0104,  0.0128,  0.0104],\n",
            "          [ 0.0111,  0.0117,  0.0108,  ...,  0.0111,  0.0108,  0.0111],\n",
            "          ...,\n",
            "          [ 0.0113,  0.0116,  0.0111,  ...,  0.0113,  0.0108,  0.0112],\n",
            "          [ 0.0113,  0.0128,  0.0113,  ...,  0.0114,  0.0133,  0.0110],\n",
            "          [ 0.0113,  0.0113,  0.0113,  ...,  0.0115,  0.0108,  0.0114]],\n",
            "\n",
            "         [[ 0.0061,  0.0090,  0.0062,  ...,  0.0064,  0.0092,  0.0066],\n",
            "          [ 0.0089,  0.0069,  0.0088,  ...,  0.0089,  0.0070,  0.0091],\n",
            "          [ 0.0061,  0.0092,  0.0061,  ...,  0.0064,  0.0094,  0.0064],\n",
            "          ...,\n",
            "          [ 0.0062,  0.0094,  0.0062,  ...,  0.0064,  0.0094,  0.0064],\n",
            "          [ 0.0088,  0.0066,  0.0087,  ...,  0.0089,  0.0074,  0.0089],\n",
            "          [ 0.0062,  0.0094,  0.0062,  ...,  0.0061,  0.0096,  0.0064]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0029, -0.0018, -0.0029,  ..., -0.0030, -0.0025, -0.0027],\n",
            "          [-0.0027, -0.0064, -0.0027,  ..., -0.0027, -0.0059, -0.0022],\n",
            "          [-0.0028, -0.0021, -0.0027,  ..., -0.0029, -0.0029, -0.0028],\n",
            "          ...,\n",
            "          [-0.0027, -0.0020, -0.0025,  ..., -0.0027, -0.0028, -0.0026],\n",
            "          [-0.0024, -0.0071, -0.0029,  ..., -0.0029, -0.0065, -0.0029],\n",
            "          [-0.0023, -0.0023, -0.0023,  ..., -0.0024, -0.0026, -0.0026]],\n",
            "\n",
            "         [[-0.0056, -0.0066, -0.0061,  ..., -0.0059, -0.0062, -0.0064],\n",
            "          [-0.0071, -0.0055, -0.0065,  ..., -0.0066, -0.0051, -0.0060],\n",
            "          [-0.0056, -0.0064, -0.0062,  ..., -0.0060, -0.0059, -0.0065],\n",
            "          ...,\n",
            "          [-0.0057, -0.0064, -0.0062,  ..., -0.0061, -0.0060, -0.0065],\n",
            "          [-0.0075, -0.0049, -0.0070,  ..., -0.0070, -0.0049, -0.0063],\n",
            "          [-0.0062, -0.0061, -0.0063,  ..., -0.0062, -0.0057, -0.0064]],\n",
            "\n",
            "         [[-0.0127, -0.0137, -0.0128,  ..., -0.0128, -0.0135, -0.0125],\n",
            "          [-0.0129, -0.0097, -0.0128,  ..., -0.0128, -0.0102, -0.0127],\n",
            "          [-0.0122, -0.0142, -0.0125,  ..., -0.0126, -0.0142, -0.0126],\n",
            "          ...,\n",
            "          [-0.0123, -0.0143, -0.0126,  ..., -0.0126, -0.0144, -0.0126],\n",
            "          [-0.0126, -0.0108, -0.0122,  ..., -0.0121, -0.0109, -0.0121],\n",
            "          [-0.0120, -0.0137, -0.0123,  ..., -0.0122, -0.0140, -0.0126]]],\n",
            "\n",
            "\n",
            "        [[[-0.0053, -0.0066, -0.0055,  ..., -0.0053, -0.0071, -0.0052],\n",
            "          [-0.0053, -0.0015, -0.0045,  ..., -0.0045, -0.0013, -0.0045],\n",
            "          [-0.0050, -0.0068, -0.0055,  ..., -0.0054, -0.0077, -0.0055],\n",
            "          ...,\n",
            "          [-0.0050, -0.0072, -0.0055,  ..., -0.0055, -0.0077, -0.0055],\n",
            "          [-0.0055, -0.0008, -0.0047,  ..., -0.0048, -0.0010, -0.0048],\n",
            "          [-0.0053, -0.0071, -0.0058,  ..., -0.0059, -0.0079, -0.0055]],\n",
            "\n",
            "         [[ 0.0109,  0.0117,  0.0109,  ...,  0.0110,  0.0107,  0.0112],\n",
            "          [ 0.0106,  0.0127,  0.0102,  ...,  0.0104,  0.0128,  0.0104],\n",
            "          [ 0.0111,  0.0117,  0.0108,  ...,  0.0111,  0.0108,  0.0111],\n",
            "          ...,\n",
            "          [ 0.0113,  0.0116,  0.0111,  ...,  0.0113,  0.0108,  0.0112],\n",
            "          [ 0.0113,  0.0128,  0.0113,  ...,  0.0114,  0.0133,  0.0110],\n",
            "          [ 0.0113,  0.0113,  0.0113,  ...,  0.0115,  0.0108,  0.0114]],\n",
            "\n",
            "         [[ 0.0061,  0.0090,  0.0062,  ...,  0.0064,  0.0092,  0.0066],\n",
            "          [ 0.0089,  0.0069,  0.0088,  ...,  0.0089,  0.0070,  0.0091],\n",
            "          [ 0.0061,  0.0092,  0.0061,  ...,  0.0064,  0.0094,  0.0064],\n",
            "          ...,\n",
            "          [ 0.0062,  0.0094,  0.0062,  ...,  0.0064,  0.0094,  0.0064],\n",
            "          [ 0.0088,  0.0066,  0.0087,  ...,  0.0089,  0.0074,  0.0089],\n",
            "          [ 0.0062,  0.0094,  0.0062,  ...,  0.0061,  0.0096,  0.0064]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0029, -0.0018, -0.0029,  ..., -0.0030, -0.0025, -0.0027],\n",
            "          [-0.0027, -0.0064, -0.0027,  ..., -0.0027, -0.0059, -0.0022],\n",
            "          [-0.0028, -0.0021, -0.0027,  ..., -0.0029, -0.0029, -0.0028],\n",
            "          ...,\n",
            "          [-0.0027, -0.0020, -0.0025,  ..., -0.0027, -0.0028, -0.0026],\n",
            "          [-0.0024, -0.0071, -0.0029,  ..., -0.0029, -0.0065, -0.0029],\n",
            "          [-0.0023, -0.0023, -0.0023,  ..., -0.0024, -0.0026, -0.0026]],\n",
            "\n",
            "         [[-0.0056, -0.0066, -0.0061,  ..., -0.0059, -0.0062, -0.0064],\n",
            "          [-0.0071, -0.0055, -0.0065,  ..., -0.0066, -0.0051, -0.0060],\n",
            "          [-0.0056, -0.0064, -0.0062,  ..., -0.0060, -0.0059, -0.0065],\n",
            "          ...,\n",
            "          [-0.0057, -0.0064, -0.0062,  ..., -0.0061, -0.0060, -0.0065],\n",
            "          [-0.0075, -0.0049, -0.0070,  ..., -0.0070, -0.0049, -0.0063],\n",
            "          [-0.0062, -0.0061, -0.0063,  ..., -0.0062, -0.0057, -0.0064]],\n",
            "\n",
            "         [[-0.0127, -0.0137, -0.0128,  ..., -0.0128, -0.0135, -0.0125],\n",
            "          [-0.0129, -0.0097, -0.0128,  ..., -0.0128, -0.0102, -0.0127],\n",
            "          [-0.0122, -0.0142, -0.0125,  ..., -0.0126, -0.0142, -0.0126],\n",
            "          ...,\n",
            "          [-0.0123, -0.0143, -0.0126,  ..., -0.0126, -0.0144, -0.0126],\n",
            "          [-0.0126, -0.0108, -0.0122,  ..., -0.0121, -0.0109, -0.0121],\n",
            "          [-0.0120, -0.0137, -0.0123,  ..., -0.0122, -0.0140, -0.0126]]]],\n",
            "       device='cuda:0', grad_fn=<CudnnConvolutionTransposeBackward>)\u001b[0m\n",
            "\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     75\u001b[0m     \u001b[0mup2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 63 and 64 in dimension 2 at /pytorch/aten/src/THC/generic/THCTensorMath.cu:71\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QY4owfQwm-Ni"
      },
      "source": [
        "___\n",
        "\n",
        "### Part 1b\n",
        "Implement a cost function\n",
        "\n",
        "You should still use cross-entropy as your cost function, but you may need to think hard about how exactly to set this up – your network should output cancer/not-cancer probabilities for each pixel, which can be viewed as a two-class classification problem.\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Adapt CrossEntropyLoss for 2 class pixel classification\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XPgrP88aOtfy",
        "colab": {}
      },
      "source": [
        "# You'll probably want a function or something to test input / output sizes of the ConvTranspose2d layer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jq22IyKanxo_",
        "colab": {}
      },
      "source": [
        "# Since you will be using the output of one network in two places(convolution and maxpooling),\n",
        "# you can't use nn.Sequential.\n",
        "# Instead you will write up the network like normal variable assignment as the example shown below:\n",
        "# You are welcome (and encouraged) to use the built-in batch normalization and dropout layer.\n",
        "\n",
        "# TODO: You need to change this to fit the UNet structure!!!\n",
        "class CancerDetection(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CancerDetection, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3,64,kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.conv3 = nn.Conv2d(64,128,kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.relu4 = nn.ReLU()\n",
        " \n",
        "  def forward(self, input):\n",
        "    conv1_out = self.conv1(input)\n",
        "    relu2_out = self.relu2(conv1_out)\n",
        "    conv3_out = self.conv3(relu2_out)\n",
        "    relu4_out = self.relu4(conv3_out) \n",
        "    return relu4_out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NAjagHCdGNAh",
        "colab": {}
      },
      "source": [
        "# Create your datasets and neural network as you have before\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RkieTbwlYWPS",
        "colab": {}
      },
      "source": [
        "# This is what was talked about in the video for memory management\n",
        "\n",
        "def scope():\n",
        "  try:\n",
        "    #your code for calling dataset and dataloader\n",
        "    gc.collect()\n",
        "    print(torch.cuda.memory_allocated(0) / 1e9)\n",
        "    \n",
        "    #for epochs:\n",
        "    # Call your model, figure out loss and accuracy\n",
        "    \n",
        "  except:\n",
        "    __ITB__()\n",
        "    \n",
        "scope()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CZ062Jv1jIIu"
      },
      "source": [
        "\n",
        "___\n",
        "\n",
        "### Part 2\n",
        "\n",
        "Plot performance over time\n",
        "\n",
        "Please generate a plot that shows loss on the training set as a function of training time. Make sure your axes are labeled!\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Plot training loss as function of training time (not Epochs)\n",
        "\n",
        "**DONE:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mTg1jyIsYVZN",
        "colab": {}
      },
      "source": [
        "# Your plotting code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S4s92S2_jQOG"
      },
      "source": [
        "___\n",
        "\n",
        "### Part 3\n",
        "\n",
        "Generate a prediction on the pos_test_000072.png image\n",
        "\n",
        "Calculate the output of your trained network on the pos_test_000072.png image,\n",
        "then make a hard decision (cancerous/not-cancerous) for each pixel.\n",
        "The resulting image should be black-and-white, where white pixels represent things\n",
        "you think are probably cancerous.\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "**NOTE:**\n",
        "\n",
        "Guessing that the pixel is not cancerous every single time will give you an accuracy of ~ 85%.\n",
        "Your trained network should be able to do better than that (but you will not be graded on accuracy).\n",
        "This is the result I got after 1 hour or training.\n",
        "\n",
        "![](http://liftothers.org/dokuwiki/lib/exe/fetch.php?w=400&tok=d23e0b&media=cs501r_f2016:training_accuracy.png)\n",
        "![](http://liftothers.org/dokuwiki/lib/exe/fetch.php?w=400&tok=bb8e3c&media=cs501r_f2016:training_loss.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XXfG3wClh8an",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "# Code for testing prediction on an image\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}