{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DL_Lab4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericburdett/cs474_labs_f2019/blob/master/DL_Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LjLT357VGND",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericburdett/cs474_labs_f2019/blob/master/DL_Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# Lab 4: Cancer Detection\n",
        "\n",
        "## Objective\n",
        "* To build a dense prediction model\n",
        "* To begin reading current papers in DNN research\n",
        "\n",
        "## Deliverable\n",
        "For this lab, you will turn in a notebook that describes your efforts at creating\n",
        "a pytorch radiologist. Your final deliverable is a notebook that has (1) deep network,\n",
        "(2) cost function, (3) method of calculating accuracy,\n",
        "(4) an image that shows the dense prediction produced by your network on the pos_test_000072.png image.\n",
        "This is an image in the test set that your network will not have seen before.\n",
        "This image, and the ground truth labeling, is shown below.\n",
        "(And is contained in the downloadable dataset below).\n",
        "\n",
        "![](http://liftothers.org/dokuwiki/lib/exe/fetch.php?w=200&tok=a8ac31&media=cs501r_f2016:pos_test_000072_output.png)\n",
        "<img src=\"http://liftothers.org/dokuwiki/lib/exe/fetch.php?media=cs501r_f2016:pos_test_000072.png\" width=\"200\">\n",
        "\n",
        "\n",
        "## Grading standards\n",
        "Your notebook will be graded on the following:\n",
        "* 40% Proper design, creation and debugging of a dense prediction network\n",
        "* 40% Proper implementation of a loss function and train/test set accuracy measure\n",
        "* 10% Tidy visualizations of loss of your dense predictor during training\n",
        "* 10% Test image output\n",
        "\n",
        "\n",
        "## Data set\n",
        "The data is given as a set of 1024×1024 PNG images. Each input image (in \n",
        "the ```inputs``` directory) is an RGB image of a section of tissue,\n",
        "and there a file with the same name (in the ```outputs``` directory) \n",
        "that has a dense labeling of whether or not a section of tissue is cancerous\n",
        "(white pixels mean “cancerous”, while black pixels mean “not cancerous”).\n",
        "\n",
        "The data has been pre-split for you into test and training splits.\n",
        "Filenames also reflect whether or not the image has any cancer at all \n",
        "(files starting with ```pos_``` have some cancerous pixels, while files \n",
        "starting with ```neg_``` have no cancer anywhere).\n",
        "All of the data is hand-labeled, so the dataset is not very large.\n",
        "That means that overfitting is a real possibility.\n",
        "\n",
        "## Description\n",
        "For a video including some tips and tricks that can help with this lab: [https://youtu.be/Ms19kgK_D8w](https://youtu.be/Ms19kgK_D8w)\n",
        "For this lab, you will implement a virtual radiologist.\n",
        "You are given images of possibly cancerous tissue samples, \n",
        "and you must build a detector that identifies where in the tissue cancer may reside.\n",
        "\n",
        "---\n",
        "\n",
        "### Part 0\n",
        "Watch and follow video tutorial:\n",
        "\n",
        "https://youtu.be/Ms19kgK_D8w\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "**DONE:**\n",
        "* Watch tutorial\n",
        "\n",
        "### Part 1\n",
        "Implement a dense predictor\n",
        "\n",
        "In previous labs and lectures, we have talked about DNNs that classify an \n",
        "entire image as a single class. Here, however, we are interested in a more nuanced classification: \n",
        "given an input image, we would like to identify each pixel that is possibly cancerous. \n",
        "That means that instead of a single output, your network should output an “image”, \n",
        "where each output pixel of your network represents the probability that a pixel is cancerous.\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Create a Network that classifies each pixel as a 1 or 0 for cancerous / not cancerous\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "___\n",
        "\n",
        "### Part 1a\n",
        "Implement your network topology\n",
        "\n",
        "\n",
        "Use the “Deep Convolution U-Net” from this paper: [(U-Net: Convolutional Networks for Biomedical Image Segmentation)](https://arxiv.org/pdf/1505.04597.pdf) \n",
        "\n",
        "![(Figure 1)](https://lh3.googleusercontent.com/qnHiB3B2KRxC3NjiSDtY08_DgDGTDsHcO6PP53oNRuct-p2QXCR-gyLkDveO850F2tTAhIOPC5Ha06NP9xq1JPsVAHlQ5UXA5V-9zkUrJHGhP_MNHFoRGnjBz1vn1p8P2rMWhlAb6HQ=w2400)\n",
        "\n",
        "You should use existing pytorch functions (not your own Conv2D module), such as ```nn.Conv2d```;\n",
        "you will also need the pytorch function ```torch.cat``` and ```nn.ConvTranspose2d```\n",
        "\n",
        "```torch.cat``` allows you to concatenate tensors.\n",
        "```nn.ConvTranspose2d``` is the opposite of ```nn.Conv2d```.\n",
        "It is used to bring an image from low res to higher res.\n",
        "[This blog](https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0) should help you understand this function in detail.\n",
        "\n",
        "Note that the simplest network you could implement (with all the desired properties)\n",
        "is just a single convolution layer with two filters and no relu! \n",
        "Why is that? (of course it wouldn't work very well!)\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Understand the U-Net architecture\n",
        "* Understand concatenation of inputs from multiple prior layers\n",
        "* Understand ConvTranspose\n",
        "* Answer Question / Reflect on simplest network with the desired properties\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "\n",
        "___\n",
        "The intention of this lab is to learn how to make deep neural nets and implement loss function.\n",
        "Therefore we'll help you with the implementation of Dataset.\n",
        "This code will download the dataset for you so that you are ready to use it and focus on network\n",
        "implementation, losses and accuracies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wQOefmcZVgTl",
        "pycharm": {
          "is_executing": false
        },
        "outputId": "e45424a8-51c4-4ec0-e7d0-025be2695bf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!pip3 install torch\n",
        "!pip3 install torchvision\n",
        "!pip3 install tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils, datasets\n",
        "from tqdm import tqdm\n",
        "from torch.nn.parameter import Parameter\n",
        "import pdb\n",
        "import torchvision\n",
        "import os\n",
        "import gzip\n",
        "import tarfile\n",
        "import gc\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n",
        "\n",
        "assert torch.cuda.is_available(), \"You need to request a GPU from Runtime > Change Runtime\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.16.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Il_53HLSWPTY",
        "colab": {}
      },
      "source": [
        "class CancerDataset(Dataset):\n",
        "  def __init__(self, root, download=True, size=512, train=True):\n",
        "    if download and not os.path.exists(os.path.join(root, 'cancer_data')):\n",
        "      datasets.utils.download_url('http://liftothers.org/cancer_data.tar.gz', root, 'cancer_data.tar.gz', None)\n",
        "      self.extract_gzip(os.path.join(root, 'cancer_data.tar.gz'))\n",
        "      self.extract_tar(os.path.join(root, 'cancer_data.tar'))\n",
        "    \n",
        "    postfix = 'train' if train else 'test'\n",
        "    root = os.path.join(root, 'cancer_data', 'cancer_data')\n",
        "    self.dataset_folder = torchvision.datasets.ImageFolder(os.path.join(root, 'inputs_' + postfix) ,transform = transforms.Compose([transforms.Resize(size),transforms.ToTensor()]))\n",
        "    self.label_folder = torchvision.datasets.ImageFolder(os.path.join(root, 'outputs_' + postfix) ,transform = transforms.Compose([transforms.Resize(size),transforms.ToTensor()]))\n",
        "\n",
        "  @staticmethod\n",
        "  def extract_gzip(gzip_path, remove_finished=False):\n",
        "    print('Extracting {}'.format(gzip_path))\n",
        "    with open(gzip_path.replace('.gz', ''), 'wb') as out_f, gzip.GzipFile(gzip_path) as zip_f:\n",
        "      out_f.write(zip_f.read())\n",
        "    if remove_finished:\n",
        "      os.unlink(gzip_path)\n",
        "  \n",
        "  @staticmethod\n",
        "  def extract_tar(tar_path):\n",
        "    print('Untarring {}'.format(tar_path))\n",
        "    z = tarfile.TarFile(tar_path)\n",
        "    z.extractall(tar_path.replace('.tar', ''))\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    img = self.dataset_folder[index]\n",
        "    label = self.label_folder[index]\n",
        "    return img[0],label[0][0]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.dataset_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSovbvhAMfeB",
        "colab_type": "code",
        "outputId": "58e98f2c-f3e7-4ef2-dccf-504eae95e3ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "data = CancerDataset('/tmp/cancer_data')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://liftothers.org/cancer_data.tar.gz to /tmp/cancer_data/cancer_data.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 2749825024/2750494655 [01:35<00:00, 28383305.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/cancer_data/cancer_data.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r2750496768it [01:50, 28383305.83it/s]                                "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Untarring /tmp/cancer_data/cancer_data.tar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WVIbOlwzM_B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "outputId": "0c260b69-eb74-409d-9c9f-1f5f97eceacc"
      },
      "source": [
        ""
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-6afff3f1686d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mo1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mo2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mo2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    336\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    337\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 338\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: conv2d(): argument 'input' (position 1) must be Tensor, not tuple"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDv3nG8rX5M-",
        "colab_type": "code",
        "outputId": "550bb9ae-29fd-4b83-d681-9afe6ddb16db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(torch.cuda.memory_allocated(0)/ 1e9) # How much memory is the GPU using in GB\n",
        "\n",
        "gc.collect() # Force the garbage collector to release memory\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6iCBoiCespm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  pass\n",
        "except: # Without specifying the exception, this will catch all exceptions\n",
        "  __ITB__() # prints a stack trace\n",
        "  \n",
        "# Place code in a function, rather than keeping global scope. This will prevent memory problems"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2H1SLoon8c1",
        "colab_type": "code",
        "outputId": "b518636f-41ef-4f25-ef81-1b35a9ae2f0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Sep 29 00:59:57 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.40       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0    27W /  70W |  15051MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCEc1WJgiZyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Concatenate multiple tensors\n",
        "# torch.cat()\n",
        "\n",
        "# Use Convolution Transpose\n",
        "# There should be a module in nn that does this\n",
        "\n",
        "# In loaders...\n",
        "# modify \"num_workers\" parameter\n",
        "# somewhere between 2 - 6\n",
        "# mostly applicable to the training loaders, but you could do it on validation loaders as well\n",
        "\n",
        "# See GPU Utilization\n",
        "# GPUTIL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUaDr7Vya9J2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(ConvBlock, self).__init__()\n",
        "    \n",
        "    self.net = nn.Sequential(\n",
        "      nn.Conv2d(in_channels, out_channels, (3, 3), padding=(1, 1)),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(out_channels, out_channels, (3, 3), padding=(1, 1)),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(out_channels, out_channels, (3, 3), padding=(1, 1)),\n",
        "      nn.ReLU()\n",
        "    )\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "  \n",
        "class ConvNetwork(nn.Module):\n",
        "  def __init__(self, dataset):\n",
        "    super(ConvNetwork, self).__init__()\n",
        "    x, y = dataset[0]\n",
        "    c, h, w = x.size()\n",
        "    \n",
        "    # Setting up objects\n",
        "    \n",
        "    # Downwards\n",
        "    self.max_pool = nn.MaxPool2d(2)\n",
        "    \n",
        "    self.conv1 = ConvBlock(c, 64) # For a 3x3 kernel, the padding is always (1, 1)\n",
        "    self.conv2 = ConvBlock(64, 128)\n",
        "    self.conv3 = ConvBlock(128, 256)\n",
        "    self.conv4 = ConvBlock(256, 512)\n",
        "    self.conv5 = ConvBlock(512, 1024)\n",
        "    \n",
        "    # Questions to Answer\n",
        "    # - What am I expecting after an Up-Convolution?\n",
        "    # - What am I concatenating together and how does the cat function work\n",
        "    \n",
        "    # Upwards\n",
        "    self.upconv1 = nn.ConvTranspose2d(1024, 512, 2, stride=2, padding=0)\n",
        "    self.conv6 = ConvBlock(1024, 512)\n",
        "    \n",
        "    self.upconv2 = nn.ConvTranspose2d(512, 256, 2, stride=2, padding=0)\n",
        "    self.conv7 = ConvBlock(512, 256)\n",
        "    \n",
        "    self.upconv3 = nn.ConvTranspose2d(256, 128, 2, stride=2, padding=0)\n",
        "    self.conv8 = ConvBlock(256, 128)\n",
        "    \n",
        "    self.upconv4 = nn.ConvTranspose2d(128, 64, 2, stride=2, padding=0)\n",
        "    self.conv9 = ConvBlock(128, 64)\n",
        "    \n",
        "    # 1x1 conv with 2 output channels\n",
        "    self.outconv = nn.Conv2d(64, 2, (1, 1), padding=(0, 0))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    n, c, h, w = x.size()\n",
        "    \n",
        "    # Downwards\n",
        "    o1 = self.conv1(x)\n",
        "    mp1 = self.max_pool(o1)\n",
        "    o2 = self.conv2(mp1)\n",
        "    mp2 = self.max_pool(o2)\n",
        "    o3 = self.conv3(mp2)\n",
        "    mp3 = self.max_pool(o3)\n",
        "    o4 = self.conv4(mp3)\n",
        "    mp4 = self.max_pool(o4)\n",
        "    o5 = self.conv5(mp4)\n",
        "    \n",
        "    # Upwards\n",
        "    up1 = self.upconv1(o5)\n",
        "    o6 = self.conv6(torch.cat((o4, up1), 1))\n",
        "\n",
        "    up2 = self.upconv2(o6)\n",
        "    o7 = self.conv7(torch.cat((o3, up2), 1))\n",
        "    \n",
        "    up3 = self.upconv3(o7)\n",
        "    o8 = self.conv8(torch.cat((o2, up3), 1))\n",
        "    \n",
        "    up4 = self.upconv4(o8)\n",
        "    o9 = self.conv9(torch.cat((o1, up4), 1))\n",
        "    \n",
        "    # Final 1x1 convolution\n",
        "    o10 = self.outconv(o9)\n",
        "    \n",
        "    return o10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sv8g1dh0bll",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a5a3a29d-7e4a-44cc-fcb9-ba0f90c5aad6"
      },
      "source": [
        "def compute_accuracy(y_hat, y_truth):\n",
        "  y_hat = y_hat[0]\n",
        "  y_truth = y_truth[0]\n",
        "  y_hat[0] = y_hat[0] > .5\n",
        "  y_hat[1] = y_hat[1] >= .5\n",
        "  intersection = torch.sum(y_hat[0] * y_truth)\n",
        "  union = torch.sum(y_hat[0] + y_truth >= 1)\n",
        "  return intersection / union\n",
        "\n",
        "def main():\n",
        "  root = '/tmp/cancer_data'\n",
        "  num_epochs = 1\n",
        "  \n",
        "  train_dataset = CancerDataset(root, train=True)\n",
        "  val_dataset = CancerDataset(root, train=False)\n",
        "  \n",
        "  model = ConvNetwork(train_dataset)\n",
        "  model = model.cuda()\n",
        "  \n",
        "  objective = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "  \n",
        "  train_loader = DataLoader(train_dataset,\n",
        "                           batch_size=4,\n",
        "                           num_workers=2,\n",
        "                           pin_memory=True,\n",
        "                           shuffle=True)\n",
        "  \n",
        "  val_loader = DataLoader(train_dataset,\n",
        "                         batch_size=4,\n",
        "                         num_workers=2,\n",
        "                         pin_memory=True,\n",
        "                         shuffle=True)\n",
        "  \n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  train_acc = []\n",
        "  val_acc = []\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "    loop = tqdm(total=len(train_loader), position=0, leave=False)\n",
        "    \n",
        "    for batch, (x, y_truth) in enumerate(train_loader):\n",
        "      gc.collect()\n",
        "      x, y_truth = x.cuda(async=True), y_truth.cuda(async=True)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      y_hat = model(x)\n",
        "      \n",
        "      #pdb.set_trace()\n",
        "      accuracy = compute_accuracy(y_hat, y_truth)\n",
        "      loss = objective(y_hat, y_truth.long())\n",
        "      \n",
        "      loss.backward()\n",
        "      \n",
        "      train_losses.append(loss.item())\n",
        "      train_acc.append(accuracy.item())\n",
        "      \n",
        "      loop.set_description('epoch:{}, loss:{:.4f}, accuracy:{:.3f}'.format(epoch, loss.item(), accuracy.item()))\n",
        "      loop.update(1)\n",
        "      \n",
        "      optimizer.step()\n",
        "\n",
        "      \n",
        "      if batch % 10 == 0:\n",
        "        val_single_acc = []\n",
        "        val_single_loss = []\n",
        "        \n",
        "        for val_x, val_y_truth in val_loader:\n",
        "          gc.collect()\n",
        "          val_x = val_x.cuda()\n",
        "          val_y_truth = val_y_truth.cuda()\n",
        "          \n",
        "          val_y_hat = model(val_x)\n",
        "          val_single_acc.append(compute_accuracy(val_y_hat, val_y_truth).item())\n",
        "          val_single_loss.append(objective(val_y_hat, val_y_truth.long()).item())\n",
        "          \n",
        "        val_losses.append(np.mean(val_single_loss))\n",
        "        val_acc.append(np.mean(val_single_acc))\n",
        "\n",
        "#          for x, y_truth in enumerate(val_loader):\n",
        "#             val_loss = objective(model(x.cuda()))\n",
        "#             val_y_hat = \n",
        "    \n",
        "#         val = np.mean([objective(model(x.cuda()), y.cuda().long()).item()\n",
        "#                       for x, y in val_loader])\n",
        "\n",
        "\n",
        "#         val_losses.append((len(train_losses), val))\n",
        "#         val_acc.append((len(train_losses), val_y_hats))\n",
        "        \n",
        "    loop.close()\n",
        "    \n",
        "    return train_losses, val_losses, train_acc, val_acc\n",
        "\n",
        "try:\n",
        "  train_losses, val_losses, train_acc, val_acc = main()\n",
        "  gc.collect()\n",
        "except:\n",
        "  gc.collect()\n",
        "  __ITB__()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0, loss:0.7030, accuracy:nan:   0%|          | 1/336 [00:01<10:50,  1.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
            "\u001b[0;32m<ipython-input-5-30c051b2d8b8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n",
            "\u001b[1;32m     72\u001b[0m           \u001b[0mval_y_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_y_truth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m---> 74\u001b[0;31m           \u001b[0mval_y_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m        \u001b[0;36mval_y_hat\u001b[0m \u001b[0;34m= tensor([[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0477, -0.0488, -0.0486,  ..., -0.0490, -0.0486, -0.0472],\n",
            "          [-0.0480, -0.0502, -0.0503,  ..., -0.0496, -0.0491, -0.0477],\n",
            "          [-0.0481, -0.0499, -0.0496,  ..., -0.0489, -0.0488, -0.0481],\n",
            "          ...,\n",
            "          [-0.0475, -0.0492, -0.0485,  ..., -0.0480, -0.0480, -0.0474],\n",
            "          [-0.0471, -0.0487, -0.0482,  ..., -0.0480, -0.0481, -0.0485],\n",
            "          [-0.0467, -0.0488, -0.0482,  ..., -0.0478, -0.0481, -0.0484]],\n",
            "\n",
            "         [[-0.0255, -0.0235, -0.0237,  ..., -0.0243, -0.0258, -0.0241],\n",
            "          [-0.0265, -0.0242, -0.0253,  ..., -0.0264, -0.0281, -0.0253],\n",
            "          [-0.0267, -0.0246, -0.0260,  ..., -0.0267, -0.0282, -0.0259],\n",
            "          ...,\n",
            "          [-0.0275, -0.0252, -0.0269,  ..., -0.0275, -0.0284, -0.0260],\n",
            "          [-0.0280, -0.0263, -0.0278,  ..., -0.0284, -0.0284, -0.0259],\n",
            "          [-0.0297, -0.0274, -0.0295,  ..., -0.0297, -0.0297, -0.0289]]],\n",
            "\n",
            "\n",
            "        [[[-0.0476, -0.0486, -0.0485,  ..., -0.0484, -0.0483, -0.0471],\n",
            "          [-0.0479, -0.0500, -0.0500,  ..., -0.0489, -0.0485, -0.0479],\n",
            "          [-0.0480, -0.0498, -0.0495,  ..., -0.0486, -0.0484, -0.0483],\n",
            "          ...,\n",
            "          [-0.0476, -0.0493, -0.0485,  ..., -0.0483, -0.0482, -0.0478],\n",
            "          [-0.0471, -0.0487, -0.0483,  ..., -0.0478, -0.0478, -0.0486],\n",
            "          [-0.0467, -0.0487, -0.0481,  ..., -0.0477, -0.0480, -0.0483]],\n",
            "\n",
            "         [[-0.0255, -0.0236, -0.0237,  ..., -0.0241, -0.0251, -0.0239],\n",
            "          [-0.0265, -0.0243, -0.0254,  ..., -0.0264, -0.0272, -0.0247],\n",
            "          [-0.0268, -0.0247, -0.0262,  ..., -0.0269, -0.0277, -0.0254],\n",
            "          ...,\n",
            "          [-0.0276, -0.0253, -0.0271,  ..., -0.0271, -0.0280, -0.0257],\n",
            "          [-0.0280, -0.0266, -0.0280,  ..., -0.0278, -0.0279, -0.0255],\n",
            "          [-0.0298, -0.0275, -0.0295,  ..., -0.0297, -0.0297, -0.0288]]],\n",
            "\n",
            "\n",
            "        [[[-0.0476, -0.0487, -0.0485,  ..., -0.0489, -0.0486, -0.0472],\n",
            "          [-0.0479, -0.0500, -0.0501,  ..., -0.0495, -0.0491, -0.0477],\n",
            "          [-0.0479, -0.0498, -0.0494,  ..., -0.0488, -0.0487, -0.0481],\n",
            "          ...,\n",
            "          [-0.0473, -0.0491, -0.0486,  ..., -0.0481, -0.0481, -0.0476],\n",
            "          [-0.0469, -0.0488, -0.0481,  ..., -0.0481, -0.0481, -0.0486],\n",
            "          [-0.0467, -0.0487, -0.0483,  ..., -0.0479, -0.0482, -0.0485]],\n",
            "\n",
            "         [[-0.0255, -0.0236, -0.0238,  ..., -0.0243, -0.0257, -0.0241],\n",
            "          [-0.0266, -0.0242, -0.0254,  ..., -0.0264, -0.0280, -0.0252],\n",
            "          [-0.0268, -0.0245, -0.0261,  ..., -0.0267, -0.0282, -0.0258],\n",
            "          ...,\n",
            "          [-0.0274, -0.0252, -0.0271,  ..., -0.0276, -0.0284, -0.0260],\n",
            "          [-0.0278, -0.0261, -0.0276,  ..., -0.0284, -0.0284, -0.0259],\n",
            "          [-0.0296, -0.0273, -0.0294,  ..., -0.0297, -0.0297, -0.0289]]]],\n",
            "       device='cuda:0', grad_fn=<CopySlices>)\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mmodel\u001b[0m \u001b[0;34m= ConvNetwork(\n",
            "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv1): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (conv2): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (conv3): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (conv4): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (conv5): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (upconv1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (conv6): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (upconv2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (conv7): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (conv8): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (upconv4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (conv9): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (outconv): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mval_x\u001b[0m \u001b[0;34m= tensor([[[[0.9333, 0.8667, 0.8824,  ..., 0.9529, 0.9608, 0.9608],\n",
            "          [0.8941, 0.8902, 0.9098,  ..., 0.9569, 0.9176, 0.9098],\n",
            "          [0.8706, 0.8353, 0.7490,  ..., 0.9137, 0.9255, 0.8863],\n",
            "          ...,\n",
            "          [0.9020, 0.9255, 0.9216,  ..., 0.8549, 0.6863, 0.5294],\n",
            "          [0.5961, 0.8824, 0.9412,  ..., 0.8392, 0.8471, 0.8941],\n",
            "          [0.2588, 0.5843, 0.9059,  ..., 0.5961, 0.6039, 0.7765]],\n",
            "\n",
            "         [[0.9137, 0.8588, 0.8510,  ..., 0.9529, 0.9686, 0.9490],\n",
            "          [0.8745, 0.8863, 0.8784,  ..., 0.9490, 0.8980, 0.9098],\n",
            "          [0.8353, 0.8196, 0.7294,  ..., 0.9098, 0.9216, 0.8706],\n",
            "          ...,\n",
            "          [0.8902, 0.9059, 0.9020,  ..., 0.8314, 0.6863, 0.5490],\n",
            "          [0.6471, 0.8902, 0.9059,  ..., 0.8118, 0.8431, 0.8745],\n",
            "          [0.3176, 0.6392, 0.8902,  ..., 0.6039, 0.6157, 0.7725]],\n",
            "\n",
            "         [[0.8784, 0.8275, 0.8118,  ..., 0.9490, 0.9569, 0.9451],\n",
            "          [0.8471, 0.8510, 0.8588,  ..., 0.9373, 0.8941, 0.9020],\n",
            "          [0.8118, 0.7804, 0.7529,  ..., 0.8980, 0.9098, 0.8510],\n",
            "          ...,\n",
            "          [0.8902, 0.8902, 0.8706,  ..., 0.8196, 0.7294, 0.6353],\n",
            "          [0.7451, 0.8980, 0.8980,  ..., 0.8275, 0.8549, 0.8745],\n",
            "          [0.4941, 0.7216, 0.8824,  ..., 0.6745, 0.6941, 0.7882]]],\n",
            "\n",
            "\n",
            "        [[[0.8980, 0.8902, 0.8824,  ..., 0.9490, 0.9490, 0.9490],\n",
            "          [0.8941, 0.8824, 0.8863,  ..., 0.9529, 0.9529, 0.9529],\n",
            "          [0.8824, 0.8745, 0.8863,  ..., 0.9451, 0.9451, 0.9451],\n",
            "          ...,\n",
            "          [0.8627, 0.8863, 0.8667,  ..., 0.9490, 0.9490, 0.9412],\n",
            "          [0.8824, 0.8863, 0.8549,  ..., 0.9529, 0.9529, 0.9490],\n",
            "          [0.8824, 0.8863, 0.8980,  ..., 0.9490, 0.9490, 0.9490]],\n",
            "\n",
            "         [[0.8784, 0.8627, 0.8745,  ..., 0.9373, 0.9412, 0.9490],\n",
            "          [0.8784, 0.8667, 0.8667,  ..., 0.9412, 0.9451, 0.9490],\n",
            "          [0.8667, 0.8667, 0.8667,  ..., 0.9412, 0.9451, 0.9490],\n",
            "          ...,\n",
            "          [0.8588, 0.8745, 0.8588,  ..., 0.9451, 0.9451, 0.9451],\n",
            "          [0.8667, 0.8902, 0.8549,  ..., 0.9451, 0.9451, 0.9451],\n",
            "          [0.8510, 0.8784, 0.8784,  ..., 0.9451, 0.9451, 0.9451]],\n",
            "\n",
            "         [[0.8824, 0.8667, 0.8667,  ..., 0.9412, 0.9412, 0.9451],\n",
            "          [0.8667, 0.8549, 0.8627,  ..., 0.9412, 0.9373, 0.9451],\n",
            "          [0.8667, 0.8549, 0.8588,  ..., 0.9451, 0.9490, 0.9412],\n",
            "          ...,\n",
            "          [0.8431, 0.8510, 0.8471,  ..., 0.9490, 0.9490, 0.9451],\n",
            "          [0.8549, 0.8667, 0.8235,  ..., 0.9412, 0.9412, 0.9412],\n",
            "          [0.8510, 0.8627, 0.8706,  ..., 0.9333, 0.9333, 0.9529]]],\n",
            "\n",
            "\n",
            "        [[[0.9569, 0.9569, 0.9569,  ..., 0.9216, 0.8745, 0.8000],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.8980, 0.8667, 0.8706],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.8588, 0.8627, 0.9137],\n",
            "          ...,\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.8706, 0.8706, 0.8980],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.9137, 0.8863, 0.9216],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.8549, 0.8314, 0.8745]],\n",
            "\n",
            "         [[0.9647, 0.9647, 0.9647,  ..., 0.9098, 0.8549, 0.7882],\n",
            "          [0.9647, 0.9647, 0.9647,  ..., 0.8902, 0.8588, 0.8549],\n",
            "          [0.9647, 0.9647, 0.9647,  ..., 0.8471, 0.8471, 0.8941],\n",
            "          ...,\n",
            "          [0.9647, 0.9647, 0.9647,  ..., 0.8588, 0.8510, 0.8863],\n",
            "          [0.9647, 0.9647, 0.9647,  ..., 0.8902, 0.8588, 0.8941],\n",
            "          [0.9647, 0.9647, 0.9647,  ..., 0.8275, 0.8196, 0.8431]],\n",
            "\n",
            "         [[0.9569, 0.9569, 0.9569,  ..., 0.8941, 0.8392, 0.7490],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.9098, 0.8157, 0.7961],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.8314, 0.7922, 0.8235],\n",
            "          ...,\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.8118, 0.7804, 0.7961],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.8157, 0.8039, 0.8196],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.7843, 0.7529, 0.7725]]],\n",
            "\n",
            "\n",
            "        [[[0.7843, 0.7843, 0.7804,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.7882, 0.7882, 0.7843,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.8039, 0.8000, 0.7961,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          ...,\n",
            "          [0.8824, 0.8902, 0.9059,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.8784, 0.8784, 0.8941,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.8706, 0.8667, 0.8941,  ..., 0.9647, 0.9647, 0.9647]],\n",
            "\n",
            "         [[0.5608, 0.5686, 0.5686,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.5804, 0.5725, 0.5725,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.5961, 0.5922, 0.5922,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          ...,\n",
            "          [0.8235, 0.8314, 0.8431,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.8196, 0.8235, 0.8392,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.8078, 0.8118, 0.8275,  ..., 0.9647, 0.9647, 0.9647]],\n",
            "\n",
            "         [[0.6275, 0.6353, 0.6392,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.6431, 0.6471, 0.6510,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.6471, 0.6510, 0.6549,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          ...,\n",
            "          [0.8510, 0.8549, 0.8667,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.8471, 0.8510, 0.8588,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.8353, 0.8392, 0.8471,  ..., 0.9647, 0.9647, 0.9647]]]],\n",
            "       device='cuda:0')\u001b[0m\n",
            "\u001b[1;32m     75\u001b[0m           \u001b[0mval_single_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_y_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     76\u001b[0m           \u001b[0mval_single_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_y_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y_truth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self=ConvNetwork(\n",
            "  (max_pool): MaxPool2d(kernel_size...onv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "), *input=(tensor([[[[0.9333, 0.8667, 0.8824,  ..., 0.9529,...9647, 0.9647, 0.9647]]]],\n",
            "       device='cuda:0'),), **kwargs={})\u001b[0m\n",
            "\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m        \u001b[0;36mresult\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mself.forward\u001b[0m \u001b[0;34m= <bound method ConvNetwork.forward of ConvNetwork(\n",
            "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv1): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (conv2): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (conv3): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (conv4): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (conv5): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (upconv1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (conv6): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (upconv2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (conv7): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (conv8): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (upconv4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (conv9): ConvBlock(\n",
            "    (net): Sequential(\n",
            "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (outconv): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            ")>\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36minput\u001b[0m \u001b[0;34m= (tensor([[[[0.9333, 0.8667, 0.8824,  ..., 0.9529, 0.9608, 0.9608],\n",
            "          [0.8941, 0.8902, 0.9098,  ..., 0.9569, 0.9176, 0.9098],\n",
            "          [0.8706, 0.8353, 0.7490,  ..., 0.9137, 0.9255, 0.8863],\n",
            "          ...,\n",
            "          [0.9020, 0.9255, 0.9216,  ..., 0.8549, 0.6863, 0.5294],\n",
            "          [0.5961, 0.8824, 0.9412,  ..., 0.8392, 0.8471, 0.8941],\n",
            "          [0.2588, 0.5843, 0.9059,  ..., 0.5961, 0.6039, 0.7765]],\n",
            "\n",
            "         [[0.9137, 0.8588, 0.8510,  ..., 0.9529, 0.9686, 0.9490],\n",
            "          [0.8745, 0.8863, 0.8784,  ..., 0.9490, 0.8980, 0.9098],\n",
            "          [0.8353, 0.8196, 0.7294,  ..., 0.9098, 0.9216, 0.8706],\n",
            "          ...,\n",
            "          [0.8902, 0.9059, 0.9020,  ..., 0.8314, 0.6863, 0.5490],\n",
            "          [0.6471, 0.8902, 0.9059,  ..., 0.8118, 0.8431, 0.8745],\n",
            "          [0.3176, 0.6392, 0.8902,  ..., 0.6039, 0.6157, 0.7725]],\n",
            "\n",
            "         [[0.8784, 0.8275, 0.8118,  ..., 0.9490, 0.9569, 0.9451],\n",
            "          [0.8471, 0.8510, 0.8588,  ..., 0.9373, 0.8941, 0.9020],\n",
            "          [0.8118, 0.7804, 0.7529,  ..., 0.8980, 0.9098, 0.8510],\n",
            "          ...,\n",
            "          [0.8902, 0.8902, 0.8706,  ..., 0.8196, 0.7294, 0.6353],\n",
            "          [0.7451, 0.8980, 0.8980,  ..., 0.8275, 0.8549, 0.8745],\n",
            "          [0.4941, 0.7216, 0.8824,  ..., 0.6745, 0.6941, 0.7882]]],\n",
            "\n",
            "\n",
            "        [[[0.8980, 0.8902, 0.8824,  ..., 0.9490, 0.9490, 0.9490],\n",
            "          [0.8941, 0.8824, 0.8863,  ..., 0.9529, 0.9529, 0.9529],\n",
            "          [0.8824, 0.8745, 0.8863,  ..., 0.9451, 0.9451, 0.9451],\n",
            "          ...,\n",
            "          [0.8627, 0.8863, 0.8667,  ..., 0.9490, 0.9490, 0.9412],\n",
            "          [0.8824, 0.8863, 0.8549,  ..., 0.9529, 0.9529, 0.9490],\n",
            "          [0.8824, 0.8863, 0.8980,  ..., 0.9490, 0.9490, 0.9490]],\n",
            "\n",
            "         [[0.8784, 0.8627, 0.8745,  ..., 0.9373, 0.9412, 0.9490],\n",
            "          [0.8784, 0.8667, 0.8667,  ..., 0.9412, 0.9451, 0.9490],\n",
            "          [0.8667, 0.8667, 0.8667,  ..., 0.9412, 0.9451, 0.9490],\n",
            "          ...,\n",
            "          [0.8588, 0.8745, 0.8588,  ..., 0.9451, 0.9451, 0.9451],\n",
            "          [0.8667, 0.8902, 0.8549,  ..., 0.9451, 0.9451, 0.9451],\n",
            "          [0.8510, 0.8784, 0.8784,  ..., 0.9451, 0.9451, 0.9451]],\n",
            "\n",
            "         [[0.8824, 0.8667, 0.8667,  ..., 0.9412, 0.9412, 0.9451],\n",
            "          [0.8667, 0.8549, 0.8627,  ..., 0.9412, 0.9373, 0.9451],\n",
            "          [0.8667, 0.8549, 0.8588,  ..., 0.9451, 0.9490, 0.9412],\n",
            "          ...,\n",
            "          [0.8431, 0.8510, 0.8471,  ..., 0.9490, 0.9490, 0.9451],\n",
            "          [0.8549, 0.8667, 0.8235,  ..., 0.9412, 0.9412, 0.9412],\n",
            "          [0.8510, 0.8627, 0.8706,  ..., 0.9333, 0.9333, 0.9529]]],\n",
            "\n",
            "\n",
            "        [[[0.9569, 0.9569, 0.9569,  ..., 0.9216, 0.8745, 0.8000],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.8980, 0.8667, 0.8706],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.8588, 0.8627, 0.9137],\n",
            "          ...,\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.8706, 0.8706, 0.8980],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.9137, 0.8863, 0.9216],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.8549, 0.8314, 0.8745]],\n",
            "\n",
            "         [[0.9647, 0.9647, 0.9647,  ..., 0.9098, 0.8549, 0.7882],\n",
            "          [0.9647, 0.9647, 0.9647,  ..., 0.8902, 0.8588, 0.8549],\n",
            "          [0.9647, 0.9647, 0.9647,  ..., 0.8471, 0.8471, 0.8941],\n",
            "          ...,\n",
            "          [0.9647, 0.9647, 0.9647,  ..., 0.8588, 0.8510, 0.8863],\n",
            "          [0.9647, 0.9647, 0.9647,  ..., 0.8902, 0.8588, 0.8941],\n",
            "          [0.9647, 0.9647, 0.9647,  ..., 0.8275, 0.8196, 0.8431]],\n",
            "\n",
            "         [[0.9569, 0.9569, 0.9569,  ..., 0.8941, 0.8392, 0.7490],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.9098, 0.8157, 0.7961],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.8314, 0.7922, 0.8235],\n",
            "          ...,\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.8118, 0.7804, 0.7961],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.8157, 0.8039, 0.8196],\n",
            "          [0.9569, 0.9569, 0.9569,  ..., 0.7843, 0.7529, 0.7725]]],\n",
            "\n",
            "\n",
            "        [[[0.7843, 0.7843, 0.7804,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.7882, 0.7882, 0.7843,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.8039, 0.8000, 0.7961,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          ...,\n",
            "          [0.8824, 0.8902, 0.9059,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.8784, 0.8784, 0.8941,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.8706, 0.8667, 0.8941,  ..., 0.9647, 0.9647, 0.9647]],\n",
            "\n",
            "         [[0.5608, 0.5686, 0.5686,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.5804, 0.5725, 0.5725,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.5961, 0.5922, 0.5922,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          ...,\n",
            "          [0.8235, 0.8314, 0.8431,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.8196, 0.8235, 0.8392,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.8078, 0.8118, 0.8275,  ..., 0.9647, 0.9647, 0.9647]],\n",
            "\n",
            "         [[0.6275, 0.6353, 0.6392,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.6431, 0.6471, 0.6510,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.6471, 0.6510, 0.6549,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          ...,\n",
            "          [0.8510, 0.8549, 0.8667,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.8471, 0.8510, 0.8588,  ..., 0.9647, 0.9647, 0.9647],\n",
            "          [0.8353, 0.8392, 0.8471,  ..., 0.9647, 0.9647, 0.9647]]]],\n",
            "       device='cuda:0'),)\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mkwargs\u001b[0m \u001b[0;34m= {}\u001b[0m\n",
            "\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m<ipython-input-3-93874c01be9c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self=ConvNetwork(\n",
            "  (max_pool): MaxPool2d(kernel_size...onv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "), x=tensor([[[[0.9333, 0.8667, 0.8824,  ..., 0.9529,...9647, 0.9647, 0.9647]]]],\n",
            "       device='cuda:0'))\u001b[0m\n",
            "\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     78\u001b[0m     \u001b[0mup4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mo9\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv9\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mup4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m        \u001b[0;36mo9\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mself.conv9\u001b[0m \u001b[0;34m= ConvBlock(\n",
            "  (net): Sequential(\n",
            "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU()\n",
            "  )\n",
            ")\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mtorch.cat\u001b[0m \u001b[0;34m= <built-in method cat of type object at 0x7f2cdbdb2b80>\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mo1\u001b[0m \u001b[0;34m= tensor([[[[3.6314e-02, 4.6961e-02, 2.2998e-02,  ..., 3.0595e-02,\n",
            "           4.9634e-02, 5.2887e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.5765e-02, 7.3319e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           4.6982e-03, 8.1248e-02],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           2.7676e-02, 7.5103e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           2.4341e-02, 8.0332e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           8.4215e-03, 5.6973e-02]],\n",
            "\n",
            "         [[2.4007e-02, 3.3356e-02, 4.6384e-02,  ..., 4.5770e-02,\n",
            "           4.3790e-02, 3.0564e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 9.4230e-03],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 7.0309e-03],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 1.0826e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 9.2374e-03],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[6.9207e-03, 2.5831e-02, 2.8034e-02,  ..., 3.6641e-02,\n",
            "           3.4410e-02, 1.2301e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           4.9496e-03, 0.0000e+00],\n",
            "          [1.8732e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [3.1745e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0000e+00, 8.1682e-02, 1.1318e-01,  ..., 1.0588e-01,\n",
            "           9.2324e-02, 9.3525e-02],\n",
            "          [0.0000e+00, 8.5598e-02, 1.4967e-01,  ..., 1.3932e-01,\n",
            "           1.0614e-01, 1.4524e-01],\n",
            "          [0.0000e+00, 9.2270e-02, 1.7230e-01,  ..., 1.6242e-01,\n",
            "           1.1996e-01, 1.6512e-01],\n",
            "          ...,\n",
            "          [6.5392e-03, 1.1803e-01, 1.6224e-01,  ..., 1.3590e-01,\n",
            "           1.1077e-01, 1.2740e-01],\n",
            "          [2.9185e-02, 1.2615e-01, 1.6312e-01,  ..., 1.5489e-01,\n",
            "           1.2831e-01, 1.4031e-01],\n",
            "          [5.3460e-02, 1.1641e-01, 1.3957e-01,  ..., 1.1365e-01,\n",
            "           8.0848e-02, 7.6834e-02]],\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 2.2134e-02,  ..., 1.4475e-02,\n",
            "           2.8019e-02, 6.1270e-03],\n",
            "          [0.0000e+00, 6.4858e-04, 0.0000e+00,  ..., 1.4999e-02,\n",
            "           2.2941e-02, 1.5787e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.9520e-04,\n",
            "           8.8359e-03, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2534e-02,\n",
            "           2.0030e-02, 2.1045e-03],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 1.0264e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[3.9927e-02, 7.6265e-02, 6.6369e-02,  ..., 7.4152e-02,\n",
            "           6.0834e-02, 4.5759e-02],\n",
            "          [1.7032e-02, 1.8082e-02, 0.0000e+00,  ..., 1.3579e-02,\n",
            "           1.2090e-02, 0.0000e+00],\n",
            "          [5.8836e-02, 4.4012e-02, 6.5265e-03,  ..., 3.7702e-02,\n",
            "           3.3293e-02, 7.3400e-03],\n",
            "          ...,\n",
            "          [5.5003e-02, 3.5287e-02, 2.8939e-02,  ..., 4.5897e-02,\n",
            "           3.8253e-02, 4.3329e-03],\n",
            "          [5.9893e-02, 2.4013e-02, 6.9462e-03,  ..., 3.1123e-02,\n",
            "           1.4382e-02, 1.0302e-04],\n",
            "          [4.8097e-02, 1.4859e-02, 2.3119e-02,  ..., 1.3433e-02,\n",
            "           2.3339e-02, 1.5284e-02]]],\n",
            "\n",
            "\n",
            "        [[[3.5632e-02, 5.0799e-02, 2.6608e-02,  ..., 3.3591e-02,\n",
            "           5.1498e-02, 5.0771e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.2580e-02, 7.3006e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.3923e-03, 8.0900e-02],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.6456e-02, 7.7544e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           2.6919e-02, 8.0658e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 5.4294e-02]],\n",
            "\n",
            "         [[2.4739e-02, 3.0401e-02, 4.4067e-02,  ..., 4.5568e-02,\n",
            "           4.0318e-02, 2.9079e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 9.8769e-03],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 2.5114e-03],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 1.7888e-02,  ..., 9.7413e-03,\n",
            "           0.0000e+00, 1.7073e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[3.2047e-03, 3.0711e-02, 3.5575e-02,  ..., 4.2084e-02,\n",
            "           3.7847e-02, 1.2127e-02],\n",
            "          [0.0000e+00, 2.7815e-03, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           6.2934e-03, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [2.5590e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [2.8653e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0000e+00, 8.2626e-02, 1.1351e-01,  ..., 1.0859e-01,\n",
            "           9.2520e-02, 9.3437e-02],\n",
            "          [0.0000e+00, 8.7831e-02, 1.5196e-01,  ..., 1.4445e-01,\n",
            "           1.1106e-01, 1.4734e-01],\n",
            "          [0.0000e+00, 9.6065e-02, 1.8133e-01,  ..., 1.6875e-01,\n",
            "           1.2071e-01, 1.6948e-01],\n",
            "          ...,\n",
            "          [2.7429e-04, 1.1338e-01, 1.8006e-01,  ..., 1.7869e-01,\n",
            "           1.3005e-01, 1.6090e-01],\n",
            "          [1.5407e-02, 1.2483e-01, 1.8003e-01,  ..., 1.8349e-01,\n",
            "           1.3437e-01, 1.6721e-01],\n",
            "          [5.4150e-02, 1.1181e-01, 1.4740e-01,  ..., 1.3774e-01,\n",
            "           9.1573e-02, 8.3392e-02]],\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 1.8022e-02,  ..., 2.0907e-02,\n",
            "           3.0792e-02, 6.5747e-03],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.7922e-02,\n",
            "           3.2420e-02, 2.1057e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.9006e-03,\n",
            "           1.6405e-02, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.8423e-03,\n",
            "           1.0636e-02, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.6130e-02,\n",
            "           3.9710e-03, 1.0007e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[4.2069e-02, 7.9990e-02, 7.4290e-02,  ..., 7.7328e-02,\n",
            "           6.3132e-02, 4.6388e-02],\n",
            "          [1.6455e-02, 2.4204e-02, 0.0000e+00,  ..., 1.8227e-02,\n",
            "           1.5480e-02, 0.0000e+00],\n",
            "          [5.7146e-02, 6.3833e-02, 2.5841e-02,  ..., 4.2442e-02,\n",
            "           3.6871e-02, 7.1338e-03],\n",
            "          ...,\n",
            "          [5.8462e-02, 4.8933e-02, 3.2322e-02,  ..., 4.8145e-02,\n",
            "           5.5661e-02, 2.2400e-03],\n",
            "          [4.4160e-02, 2.6363e-02, 8.8131e-03,  ..., 3.0430e-02,\n",
            "           2.7009e-02, 6.4331e-03],\n",
            "          [5.2599e-02, 2.5899e-02, 2.2496e-02,  ..., 2.2705e-02,\n",
            "           4.9214e-02, 1.2809e-02]]],\n",
            "\n",
            "\n",
            "        [[[3.5908e-02, 5.3080e-02, 2.6352e-02,  ..., 3.0902e-02,\n",
            "           5.0619e-02, 4.9529e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.8146e-02, 7.1045e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.1019e-02, 7.7996e-02],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.8779e-02, 7.5050e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           2.7602e-02, 7.9135e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           4.0371e-03, 5.5756e-02]],\n",
            "\n",
            "         [[2.4976e-02, 3.0767e-02, 4.7317e-02,  ..., 4.4728e-02,\n",
            "           3.5990e-02, 2.6535e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 1.2699e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 5.1942e-03],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 2.8498e-03],\n",
            "          [0.0000e+00, 0.0000e+00, 1.7984e-02,  ..., 1.2817e-02,\n",
            "           0.0000e+00, 1.7283e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[3.0663e-03, 3.6202e-02, 4.1627e-02,  ..., 3.6524e-02,\n",
            "           3.4277e-02, 8.8368e-03],\n",
            "          [0.0000e+00, 4.2887e-03, 2.8998e-04,  ..., 0.0000e+00,\n",
            "           6.6367e-03, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [6.3648e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [2.7359e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0000e+00, 8.8460e-02, 1.2252e-01,  ..., 1.0305e-01,\n",
            "           8.6217e-02, 8.4078e-02],\n",
            "          [0.0000e+00, 9.4936e-02, 1.6333e-01,  ..., 1.3446e-01,\n",
            "           1.1501e-01, 1.3809e-01],\n",
            "          [0.0000e+00, 1.0284e-01, 1.9489e-01,  ..., 1.6024e-01,\n",
            "           1.2139e-01, 1.5871e-01],\n",
            "          ...,\n",
            "          [0.0000e+00, 1.2252e-01, 1.9414e-01,  ..., 1.7326e-01,\n",
            "           1.2809e-01, 1.5277e-01],\n",
            "          [1.5525e-02, 1.3403e-01, 1.9704e-01,  ..., 1.7624e-01,\n",
            "           1.3077e-01, 1.5597e-01],\n",
            "          [5.6989e-02, 1.1982e-01, 1.5852e-01,  ..., 1.2955e-01,\n",
            "           8.7080e-02, 7.6870e-02]],\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 2.3863e-02,  ..., 1.3958e-02,\n",
            "           2.2319e-02, 0.0000e+00],\n",
            "          [0.0000e+00, 3.2969e-03, 0.0000e+00,  ..., 1.1015e-02,\n",
            "           2.1987e-02, 1.6561e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           5.6566e-03, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.7858e-03,\n",
            "           2.7872e-03, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.1811e-02,\n",
            "           0.0000e+00, 1.1960e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[4.4239e-02, 8.4851e-02, 7.8525e-02,  ..., 7.0236e-02,\n",
            "           5.8815e-02, 4.3588e-02],\n",
            "          [1.6656e-02, 2.6548e-02, 5.7196e-04,  ..., 1.4356e-02,\n",
            "           9.0589e-03, 0.0000e+00],\n",
            "          [6.2715e-02, 7.3824e-02, 3.1653e-02,  ..., 3.3312e-02,\n",
            "           2.6556e-02, 7.0033e-03],\n",
            "          ...,\n",
            "          [6.5317e-02, 6.1092e-02, 3.9713e-02,  ..., 3.5178e-02,\n",
            "           4.2833e-02, 3.6737e-04],\n",
            "          [5.1504e-02, 3.5237e-02, 1.5400e-02,  ..., 1.5689e-02,\n",
            "           2.1376e-02, 6.3370e-04],\n",
            "          [5.7595e-02, 3.1601e-02, 2.5260e-02,  ..., 1.1330e-02,\n",
            "           3.9006e-02, 7.6996e-03]]],\n",
            "\n",
            "\n",
            "        [[[3.3789e-02, 4.5225e-02, 2.4406e-02,  ..., 3.3460e-02,\n",
            "           5.2006e-02, 5.1164e-02],\n",
            "          [5.9281e-04, 6.2387e-03, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.1980e-02, 7.3827e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.0177e-03, 8.1914e-02],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.6004e-02, 7.8857e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           2.6715e-02, 8.1499e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 5.4553e-02]],\n",
            "\n",
            "         [[2.2035e-02, 2.4327e-02, 3.3589e-02,  ..., 4.5852e-02,\n",
            "           4.0798e-02, 2.9285e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 9.6048e-03],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 1.7870e-03],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 1.5851e-02,  ..., 1.0008e-02,\n",
            "           0.0000e+00, 1.7003e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[6.2219e-03, 1.5120e-02, 2.0285e-02,  ..., 4.3504e-02,\n",
            "           3.9021e-02, 1.2646e-02],\n",
            "          [0.0000e+00, 3.4379e-03, 2.0862e-03,  ..., 0.0000e+00,\n",
            "           6.7630e-03, 0.0000e+00],\n",
            "          [1.6434e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [3.1889e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [2.4604e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0000e+00, 6.3611e-02, 8.6769e-02,  ..., 1.1010e-01,\n",
            "           9.3732e-02, 9.4638e-02],\n",
            "          [0.0000e+00, 7.0254e-02, 1.2646e-01,  ..., 1.4656e-01,\n",
            "           1.1251e-01, 1.4989e-01],\n",
            "          [0.0000e+00, 8.5735e-02, 1.5811e-01,  ..., 1.7108e-01,\n",
            "           1.2187e-01, 1.7238e-01],\n",
            "          ...,\n",
            "          [0.0000e+00, 1.1418e-01, 1.8150e-01,  ..., 1.8082e-01,\n",
            "           1.3159e-01, 1.6346e-01],\n",
            "          [1.4744e-02, 1.2522e-01, 1.8317e-01,  ..., 1.8605e-01,\n",
            "           1.3583e-01, 1.7001e-01],\n",
            "          [5.4050e-02, 1.1485e-01, 1.4979e-01,  ..., 1.3952e-01,\n",
            "           9.2685e-02, 8.4917e-02]],\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 3.5965e-03,  ..., 2.2351e-02,\n",
            "           3.2323e-02, 7.8376e-03],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.9154e-02,\n",
            "           3.4402e-02, 2.2130e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0272e-02,\n",
            "           1.8671e-02, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.9279e-03,\n",
            "           1.1655e-02, 5.9716e-04],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.6519e-02,\n",
            "           4.0390e-03, 1.0535e-02],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[4.2420e-02, 7.2491e-02, 6.9158e-02,  ..., 7.8436e-02,\n",
            "           6.3930e-02, 4.6924e-02],\n",
            "          [1.8516e-02, 1.9292e-02, 7.9500e-04,  ..., 1.8369e-02,\n",
            "           1.5675e-02, 0.0000e+00],\n",
            "          [4.3877e-02, 3.8488e-02, 1.1129e-02,  ..., 4.3778e-02,\n",
            "           3.8004e-02, 7.3518e-03],\n",
            "          ...,\n",
            "          [5.6556e-02, 4.9378e-02, 3.2586e-02,  ..., 4.9582e-02,\n",
            "           5.7076e-02, 2.0635e-03],\n",
            "          [4.3549e-02, 2.4799e-02, 8.8628e-03,  ..., 3.1545e-02,\n",
            "           2.7366e-02, 6.8380e-03],\n",
            "          [5.1282e-02, 2.4523e-02, 2.0538e-02,  ..., 2.3151e-02,\n",
            "           4.9941e-02, 1.3228e-02]]]], device='cuda:0',\n",
            "       grad_fn=<ReluBackward0>)\u001b[0m\u001b[0;34m\n",
            "        \u001b[0m\u001b[0;36mup4\u001b[0m \u001b[0;34m= tensor([[[[ 3.9359e-02,  4.8733e-02,  3.8757e-02,  ...,  4.9545e-02,\n",
            "            3.9489e-02,  5.0595e-02],\n",
            "          [ 4.5088e-02,  4.7706e-02,  4.5520e-02,  ...,  4.7561e-02,\n",
            "            4.5988e-02,  4.8734e-02],\n",
            "          [ 3.9833e-02,  4.8069e-02,  3.9937e-02,  ...,  4.9560e-02,\n",
            "            4.1656e-02,  5.0267e-02],\n",
            "          ...,\n",
            "          [ 4.5682e-02,  4.6074e-02,  4.4268e-02,  ...,  4.6240e-02,\n",
            "            4.5724e-02,  4.7119e-02],\n",
            "          [ 3.9517e-02,  4.8464e-02,  4.1266e-02,  ...,  4.9347e-02,\n",
            "            4.2306e-02,  4.9513e-02],\n",
            "          [ 4.5869e-02,  4.5388e-02,  4.3930e-02,  ...,  4.6027e-02,\n",
            "            4.4589e-02,  4.6895e-02]],\n",
            "\n",
            "         [[ 4.1639e-02,  2.7481e-02,  4.0959e-02,  ...,  2.8715e-02,\n",
            "            3.9719e-02,  2.8360e-02],\n",
            "          [ 3.1734e-02,  3.9348e-02,  3.2886e-02,  ...,  3.9767e-02,\n",
            "            3.1882e-02,  3.7949e-02],\n",
            "          [ 4.3044e-02,  2.7687e-02,  4.2026e-02,  ...,  2.7887e-02,\n",
            "            3.9963e-02,  2.8235e-02],\n",
            "          ...,\n",
            "          [ 3.1097e-02,  4.0084e-02,  3.1728e-02,  ...,  4.0523e-02,\n",
            "            3.0986e-02,  3.8328e-02],\n",
            "          [ 4.2484e-02,  2.7896e-02,  4.1668e-02,  ...,  2.7784e-02,\n",
            "            4.0013e-02,  2.8268e-02],\n",
            "          [ 3.0273e-02,  3.9359e-02,  3.1115e-02,  ...,  3.9694e-02,\n",
            "            3.1186e-02,  3.8410e-02]],\n",
            "\n",
            "         [[-7.7449e-03, -1.5990e-02, -7.6587e-03,  ..., -1.7304e-02,\n",
            "           -7.4203e-03, -1.5916e-02],\n",
            "          [-1.2132e-02, -1.0989e-02, -1.2638e-02,  ..., -1.2209e-02,\n",
            "           -1.4410e-02, -1.2592e-02],\n",
            "          [-8.1533e-03, -1.5984e-02, -7.1169e-03,  ..., -1.6747e-02,\n",
            "           -6.5006e-03, -1.5231e-02],\n",
            "          ...,\n",
            "          [-1.0087e-02, -1.1470e-02, -1.1293e-02,  ..., -1.2342e-02,\n",
            "           -1.4940e-02, -1.2398e-02],\n",
            "          [-9.2544e-03, -1.5915e-02, -6.7983e-03,  ..., -1.6239e-02,\n",
            "           -6.3648e-03, -1.5129e-02],\n",
            "          [-1.0630e-02, -1.3188e-02, -1.1643e-02,  ..., -1.3457e-02,\n",
            "           -1.5135e-02, -1.2956e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.8017e-04, -4.5999e-03, -7.6563e-04,  ..., -3.6892e-03,\n",
            "           -3.1158e-05, -3.4455e-03],\n",
            "          [-1.1862e-02,  4.4185e-03, -1.2177e-02,  ...,  4.2232e-03,\n",
            "           -1.1320e-02,  3.2572e-03],\n",
            "          [-1.1582e-03, -5.2345e-03, -2.1462e-03,  ..., -5.0232e-03,\n",
            "           -1.0791e-03, -3.7994e-03],\n",
            "          ...,\n",
            "          [-1.1967e-02,  4.6762e-03, -1.3243e-02,  ...,  5.4712e-03,\n",
            "           -1.1717e-02,  5.0240e-03],\n",
            "          [-6.2826e-04, -5.8880e-03, -1.6080e-03,  ..., -7.2992e-03,\n",
            "           -9.9228e-04, -6.1584e-03],\n",
            "          [-1.1143e-02,  4.2525e-03, -1.1674e-02,  ...,  5.3102e-03,\n",
            "           -1.1010e-02,  5.6089e-03]],\n",
            "\n",
            "         [[-2.5738e-04,  7.1859e-03, -1.9846e-04,  ...,  7.4468e-03,\n",
            "           -2.3018e-03,  6.7864e-03],\n",
            "          [ 4.7368e-03, -1.2325e-03,  4.9982e-03,  ...,  1.8480e-04,\n",
            "            4.8090e-03, -5.0384e-04],\n",
            "          [ 6.6706e-06,  7.6296e-03, -1.7512e-04,  ...,  7.1597e-03,\n",
            "           -2.3503e-03,  5.3472e-03],\n",
            "          ...,\n",
            "          [ 6.4633e-03, -5.2464e-04,  7.5186e-03,  ...,  1.7349e-03,\n",
            "            6.5316e-03,  1.2248e-03],\n",
            "          [-6.1741e-04,  8.4070e-03, -7.8715e-05,  ...,  8.3924e-03,\n",
            "           -1.6219e-03,  6.2902e-03],\n",
            "          [ 7.8684e-03, -1.1760e-03,  8.0330e-03,  ..., -1.5761e-04,\n",
            "            6.7182e-03, -9.4174e-05]],\n",
            "\n",
            "         [[-3.4813e-02, -2.6245e-02, -3.4927e-02,  ..., -2.5408e-02,\n",
            "           -3.4591e-02, -2.5787e-02],\n",
            "          [-3.0168e-02, -3.5444e-02, -3.0175e-02,  ..., -3.5313e-02,\n",
            "           -3.0272e-02, -3.6008e-02],\n",
            "          [-3.3693e-02, -2.6517e-02, -3.3873e-02,  ..., -2.6411e-02,\n",
            "           -3.3873e-02, -2.6430e-02],\n",
            "          ...,\n",
            "          [-3.0320e-02, -3.5405e-02, -3.1708e-02,  ..., -3.5837e-02,\n",
            "           -3.1822e-02, -3.7347e-02],\n",
            "          [-3.4550e-02, -2.7183e-02, -3.3311e-02,  ..., -2.9367e-02,\n",
            "           -3.3043e-02, -2.8744e-02],\n",
            "          [-3.1749e-02, -3.5481e-02, -3.3378e-02,  ..., -3.6817e-02,\n",
            "           -3.3679e-02, -3.7848e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.9350e-02,  4.8739e-02,  3.8754e-02,  ...,  4.9545e-02,\n",
            "            3.9486e-02,  5.0592e-02],\n",
            "          [ 4.5086e-02,  4.7711e-02,  4.5515e-02,  ...,  4.7543e-02,\n",
            "            4.6000e-02,  4.8721e-02],\n",
            "          [ 3.9820e-02,  4.8076e-02,  3.9929e-02,  ...,  4.9574e-02,\n",
            "            4.1657e-02,  5.0271e-02],\n",
            "          ...,\n",
            "          [ 4.5693e-02,  4.6074e-02,  4.4268e-02,  ...,  4.6218e-02,\n",
            "            4.5718e-02,  4.7102e-02],\n",
            "          [ 3.9485e-02,  4.8473e-02,  4.1264e-02,  ...,  4.9343e-02,\n",
            "            4.2300e-02,  4.9511e-02],\n",
            "          [ 4.5876e-02,  4.5386e-02,  4.3937e-02,  ...,  4.6026e-02,\n",
            "            4.4569e-02,  4.6895e-02]],\n",
            "\n",
            "         [[ 4.1639e-02,  2.7479e-02,  4.0958e-02,  ...,  2.8703e-02,\n",
            "            3.9731e-02,  2.8351e-02],\n",
            "          [ 3.1743e-02,  3.9356e-02,  3.2885e-02,  ...,  3.9764e-02,\n",
            "            3.1856e-02,  3.7943e-02],\n",
            "          [ 4.3056e-02,  2.7678e-02,  4.2048e-02,  ...,  2.7878e-02,\n",
            "            3.9977e-02,  2.8239e-02],\n",
            "          ...,\n",
            "          [ 3.1101e-02,  4.0118e-02,  3.1737e-02,  ...,  4.0534e-02,\n",
            "            3.1000e-02,  3.8326e-02],\n",
            "          [ 4.2508e-02,  2.7901e-02,  4.1662e-02,  ...,  2.7776e-02,\n",
            "            4.0013e-02,  2.8271e-02],\n",
            "          [ 3.0274e-02,  3.9358e-02,  3.1127e-02,  ...,  3.9693e-02,\n",
            "            3.1212e-02,  3.8422e-02]],\n",
            "\n",
            "         [[-7.7424e-03, -1.5994e-02, -7.6572e-03,  ..., -1.7297e-02,\n",
            "           -7.4373e-03, -1.5908e-02],\n",
            "          [-1.2131e-02, -1.0986e-02, -1.2634e-02,  ..., -1.2206e-02,\n",
            "           -1.4414e-02, -1.2592e-02],\n",
            "          [-8.1618e-03, -1.5989e-02, -7.1287e-03,  ..., -1.6747e-02,\n",
            "           -6.5114e-03, -1.5227e-02],\n",
            "          ...,\n",
            "          [-1.0080e-02, -1.1453e-02, -1.1313e-02,  ..., -1.2328e-02,\n",
            "           -1.4916e-02, -1.2392e-02],\n",
            "          [-9.2698e-03, -1.5932e-02, -6.8100e-03,  ..., -1.6228e-02,\n",
            "           -6.3881e-03, -1.5113e-02],\n",
            "          [-1.0624e-02, -1.3158e-02, -1.1647e-02,  ..., -1.3444e-02,\n",
            "           -1.5105e-02, -1.2935e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.7592e-04, -4.5921e-03, -7.6590e-04,  ..., -3.6890e-03,\n",
            "           -2.9615e-05, -3.4486e-03],\n",
            "          [-1.1871e-02,  4.4249e-03, -1.2182e-02,  ...,  4.2301e-03,\n",
            "           -1.1314e-02,  3.2576e-03],\n",
            "          [-1.1578e-03, -5.2398e-03, -2.1364e-03,  ..., -5.0193e-03,\n",
            "           -1.0701e-03, -3.7990e-03],\n",
            "          ...,\n",
            "          [-1.1991e-02,  4.6655e-03, -1.3260e-02,  ...,  5.4887e-03,\n",
            "           -1.1711e-02,  5.0192e-03],\n",
            "          [-6.3350e-04, -5.9040e-03, -1.6044e-03,  ..., -7.3271e-03,\n",
            "           -9.7696e-04, -6.1683e-03],\n",
            "          [-1.1142e-02,  4.2490e-03, -1.1670e-02,  ...,  5.3097e-03,\n",
            "           -1.1007e-02,  5.6082e-03]],\n",
            "\n",
            "         [[-2.6375e-04,  7.1856e-03, -2.0582e-04,  ...,  7.4315e-03,\n",
            "           -2.3048e-03,  6.7809e-03],\n",
            "          [ 4.7431e-03, -1.2306e-03,  4.9984e-03,  ...,  1.9093e-04,\n",
            "            4.8181e-03, -5.0330e-04],\n",
            "          [ 9.6359e-06,  7.6319e-03, -1.6094e-04,  ...,  7.1441e-03,\n",
            "           -2.3632e-03,  5.3468e-03],\n",
            "          ...,\n",
            "          [ 6.4818e-03, -5.4314e-04,  7.5532e-03,  ...,  1.7445e-03,\n",
            "            6.5237e-03,  1.2503e-03],\n",
            "          [-6.3837e-04,  8.3865e-03, -8.4925e-05,  ...,  8.3874e-03,\n",
            "           -1.6136e-03,  6.2931e-03],\n",
            "          [ 7.8889e-03, -1.1834e-03,  8.0396e-03,  ..., -1.4962e-04,\n",
            "            6.7219e-03, -8.8428e-05]],\n",
            "\n",
            "         [[-3.4809e-02, -2.6239e-02, -3.4924e-02,  ..., -2.5397e-02,\n",
            "           -3.4595e-02, -2.5784e-02],\n",
            "          [-3.0170e-02, -3.5449e-02, -3.0170e-02,  ..., -3.5308e-02,\n",
            "           -3.0270e-02, -3.5997e-02],\n",
            "          [-3.3689e-02, -2.6515e-02, -3.3856e-02,  ..., -2.6391e-02,\n",
            "           -3.3875e-02, -2.6427e-02],\n",
            "          ...,\n",
            "          [-3.0322e-02, -3.5375e-02, -3.1727e-02,  ..., -3.5830e-02,\n",
            "           -3.1829e-02, -3.7359e-02],\n",
            "          [-3.4566e-02, -2.7159e-02, -3.3287e-02,  ..., -2.9367e-02,\n",
            "           -3.3028e-02, -2.8739e-02],\n",
            "          [-3.1749e-02, -3.5473e-02, -3.3358e-02,  ..., -3.6813e-02,\n",
            "           -3.3686e-02, -3.7846e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.9348e-02,  4.8739e-02,  3.8744e-02,  ...,  4.9544e-02,\n",
            "            3.9487e-02,  5.0592e-02],\n",
            "          [ 4.5088e-02,  4.7714e-02,  4.5510e-02,  ...,  4.7545e-02,\n",
            "            4.5996e-02,  4.8718e-02],\n",
            "          [ 3.9818e-02,  4.8083e-02,  3.9924e-02,  ...,  4.9570e-02,\n",
            "            4.1659e-02,  5.0270e-02],\n",
            "          ...,\n",
            "          [ 4.5695e-02,  4.6078e-02,  4.4265e-02,  ...,  4.6223e-02,\n",
            "            4.5720e-02,  4.7103e-02],\n",
            "          [ 3.9479e-02,  4.8472e-02,  4.1259e-02,  ...,  4.9343e-02,\n",
            "            4.2301e-02,  4.9510e-02],\n",
            "          [ 4.5879e-02,  4.5386e-02,  4.3933e-02,  ...,  4.6026e-02,\n",
            "            4.4571e-02,  4.6897e-02]],\n",
            "\n",
            "         [[ 4.1641e-02,  2.7476e-02,  4.0963e-02,  ...,  2.8709e-02,\n",
            "            3.9725e-02,  2.8349e-02],\n",
            "          [ 3.1747e-02,  3.9362e-02,  3.2892e-02,  ...,  3.9766e-02,\n",
            "            3.1865e-02,  3.7943e-02],\n",
            "          [ 4.3062e-02,  2.7675e-02,  4.2066e-02,  ...,  2.7887e-02,\n",
            "            3.9980e-02,  2.8230e-02],\n",
            "          ...,\n",
            "          [ 3.1103e-02,  4.0123e-02,  3.1737e-02,  ...,  4.0541e-02,\n",
            "            3.0996e-02,  3.8334e-02],\n",
            "          [ 4.2514e-02,  2.7907e-02,  4.1667e-02,  ...,  2.7775e-02,\n",
            "            4.0012e-02,  2.8266e-02],\n",
            "          [ 3.0275e-02,  3.9359e-02,  3.1129e-02,  ...,  3.9702e-02,\n",
            "            3.1204e-02,  3.8427e-02]],\n",
            "\n",
            "         [[-7.7435e-03, -1.6001e-02, -7.6528e-03,  ..., -1.7303e-02,\n",
            "           -7.4354e-03, -1.5914e-02],\n",
            "          [-1.2130e-02, -1.0979e-02, -1.2635e-02,  ..., -1.2209e-02,\n",
            "           -1.4410e-02, -1.2593e-02],\n",
            "          [-8.1681e-03, -1.5993e-02, -7.1348e-03,  ..., -1.6750e-02,\n",
            "           -6.5091e-03, -1.5234e-02],\n",
            "          ...,\n",
            "          [-1.0078e-02, -1.1452e-02, -1.1317e-02,  ..., -1.2326e-02,\n",
            "           -1.4925e-02, -1.2392e-02],\n",
            "          [-9.2756e-03, -1.5942e-02, -6.8205e-03,  ..., -1.6227e-02,\n",
            "           -6.3840e-03, -1.5116e-02],\n",
            "          [-1.0623e-02, -1.3149e-02, -1.1648e-02,  ..., -1.3443e-02,\n",
            "           -1.5113e-02, -1.2934e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.7268e-04, -4.5886e-03, -7.6755e-04,  ..., -3.6912e-03,\n",
            "           -3.1834e-05, -3.4443e-03],\n",
            "          [-1.1876e-02,  4.4283e-03, -1.2194e-02,  ...,  4.2225e-03,\n",
            "           -1.1316e-02,  3.2573e-03],\n",
            "          [-1.1557e-03, -5.2422e-03, -2.1282e-03,  ..., -5.0178e-03,\n",
            "           -1.0751e-03, -3.7986e-03],\n",
            "          ...,\n",
            "          [-1.1997e-02,  4.6601e-03, -1.3263e-02,  ...,  5.4898e-03,\n",
            "           -1.1708e-02,  5.0224e-03],\n",
            "          [-6.3500e-04, -5.9072e-03, -1.6020e-03,  ..., -7.3274e-03,\n",
            "           -9.8083e-04, -6.1700e-03],\n",
            "          [-1.1145e-02,  4.2462e-03, -1.1670e-02,  ...,  5.3116e-03,\n",
            "           -1.1004e-02,  5.6080e-03]],\n",
            "\n",
            "         [[-2.6295e-04,  7.1835e-03, -2.0588e-04,  ...,  7.4372e-03,\n",
            "           -2.3029e-03,  6.7791e-03],\n",
            "          [ 4.7427e-03, -1.2323e-03,  4.9973e-03,  ...,  1.9244e-04,\n",
            "            4.8146e-03, -5.0534e-04],\n",
            "          [ 1.7603e-05,  7.6312e-03, -1.4894e-04,  ...,  7.1488e-03,\n",
            "           -2.3573e-03,  5.3460e-03],\n",
            "          ...,\n",
            "          [ 6.4835e-03, -5.5009e-04,  7.5553e-03,  ...,  1.7460e-03,\n",
            "            6.5243e-03,  1.2455e-03],\n",
            "          [-6.4316e-04,  8.3895e-03, -8.5942e-05,  ...,  8.3844e-03,\n",
            "           -1.6192e-03,  6.2906e-03],\n",
            "          [ 7.8879e-03, -1.1838e-03,  8.0327e-03,  ..., -1.4988e-04,\n",
            "            6.7221e-03, -9.2245e-05]],\n",
            "\n",
            "         [[-3.4807e-02, -2.6234e-02, -3.4924e-02,  ..., -2.5404e-02,\n",
            "           -3.4596e-02, -2.5781e-02],\n",
            "          [-3.0169e-02, -3.5449e-02, -3.0168e-02,  ..., -3.5303e-02,\n",
            "           -3.0270e-02, -3.5997e-02],\n",
            "          [-3.3683e-02, -2.6515e-02, -3.3845e-02,  ..., -2.6394e-02,\n",
            "           -3.3876e-02, -2.6426e-02],\n",
            "          ...,\n",
            "          [-3.0326e-02, -3.5358e-02, -3.1732e-02,  ..., -3.5831e-02,\n",
            "           -3.1827e-02, -3.7353e-02],\n",
            "          [-3.4568e-02, -2.7154e-02, -3.3279e-02,  ..., -2.9365e-02,\n",
            "           -3.3034e-02, -2.8736e-02],\n",
            "          [-3.1749e-02, -3.5467e-02, -3.3357e-02,  ..., -3.6813e-02,\n",
            "           -3.3682e-02, -3.7845e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.9353e-02,  4.8736e-02,  3.8768e-02,  ...,  4.9545e-02,\n",
            "            3.9486e-02,  5.0592e-02],\n",
            "          [ 4.5081e-02,  4.7709e-02,  4.5519e-02,  ...,  4.7543e-02,\n",
            "            4.6002e-02,  4.8720e-02],\n",
            "          [ 3.9824e-02,  4.8057e-02,  3.9928e-02,  ...,  4.9575e-02,\n",
            "            4.1656e-02,  5.0272e-02],\n",
            "          ...,\n",
            "          [ 4.5693e-02,  4.6076e-02,  4.4266e-02,  ...,  4.6217e-02,\n",
            "            4.5717e-02,  4.7101e-02],\n",
            "          [ 3.9481e-02,  4.8472e-02,  4.1261e-02,  ...,  4.9343e-02,\n",
            "            4.2300e-02,  4.9511e-02],\n",
            "          [ 4.5878e-02,  4.5386e-02,  4.3934e-02,  ...,  4.6025e-02,\n",
            "            4.4569e-02,  4.6894e-02]],\n",
            "\n",
            "         [[ 4.1637e-02,  2.7484e-02,  4.0955e-02,  ...,  2.8702e-02,\n",
            "            3.9732e-02,  2.8351e-02],\n",
            "          [ 3.1731e-02,  3.9344e-02,  3.2876e-02,  ...,  3.9763e-02,\n",
            "            3.1853e-02,  3.7942e-02],\n",
            "          [ 4.3042e-02,  2.7687e-02,  4.2019e-02,  ...,  2.7878e-02,\n",
            "            3.9977e-02,  2.8239e-02],\n",
            "          ...,\n",
            "          [ 3.1103e-02,  4.0119e-02,  3.1739e-02,  ...,  4.0533e-02,\n",
            "            3.1001e-02,  3.8325e-02],\n",
            "          [ 4.2511e-02,  2.7904e-02,  4.1664e-02,  ...,  2.7776e-02,\n",
            "            4.0014e-02,  2.8272e-02],\n",
            "          [ 3.0275e-02,  3.9358e-02,  3.1129e-02,  ...,  3.9692e-02,\n",
            "            3.1213e-02,  3.8422e-02]],\n",
            "\n",
            "         [[-7.7401e-03, -1.5985e-02, -7.6599e-03,  ..., -1.7297e-02,\n",
            "           -7.4383e-03, -1.5907e-02],\n",
            "          [-1.2135e-02, -1.0993e-02, -1.2628e-02,  ..., -1.2205e-02,\n",
            "           -1.4415e-02, -1.2592e-02],\n",
            "          [-8.1462e-03, -1.5981e-02, -7.1150e-03,  ..., -1.6747e-02,\n",
            "           -6.5116e-03, -1.5226e-02],\n",
            "          ...,\n",
            "          [-1.0079e-02, -1.1453e-02, -1.1313e-02,  ..., -1.2328e-02,\n",
            "           -1.4914e-02, -1.2392e-02],\n",
            "          [-9.2719e-03, -1.5937e-02, -6.8144e-03,  ..., -1.6227e-02,\n",
            "           -6.3890e-03, -1.5113e-02],\n",
            "          [-1.0624e-02, -1.3154e-02, -1.1650e-02,  ..., -1.3444e-02,\n",
            "           -1.5103e-02, -1.2934e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.8383e-04, -4.5985e-03, -7.6563e-04,  ..., -3.6882e-03,\n",
            "           -2.9052e-05, -3.4499e-03],\n",
            "          [-1.1864e-02,  4.4201e-03, -1.2168e-02,  ...,  4.2309e-03,\n",
            "           -1.1313e-02,  3.2579e-03],\n",
            "          [-1.1613e-03, -5.2303e-03, -2.1460e-03,  ..., -5.0195e-03,\n",
            "           -1.0701e-03, -3.7994e-03],\n",
            "          ...,\n",
            "          [-1.1993e-02,  4.6627e-03, -1.3265e-02,  ...,  5.4891e-03,\n",
            "           -1.1711e-02,  5.0186e-03],\n",
            "          [-6.3430e-04, -5.9057e-03, -1.6043e-03,  ..., -7.3281e-03,\n",
            "           -9.7626e-04, -6.1690e-03],\n",
            "          [-1.1143e-02,  4.2482e-03, -1.1673e-02,  ...,  5.3092e-03,\n",
            "           -1.1006e-02,  5.6077e-03]],\n",
            "\n",
            "         [[-2.6071e-04,  7.1915e-03, -2.0881e-04,  ...,  7.4307e-03,\n",
            "           -2.3046e-03,  6.7811e-03],\n",
            "          [ 4.7436e-03, -1.2280e-03,  5.0000e-03,  ...,  1.9146e-04,\n",
            "            4.8191e-03, -5.0309e-04],\n",
            "          [-5.9518e-06,  7.6314e-03, -1.8733e-04,  ...,  7.1451e-03,\n",
            "           -2.3641e-03,  5.3470e-03],\n",
            "          ...,\n",
            "          [ 6.4838e-03, -5.4469e-04,  7.5566e-03,  ...,  1.7443e-03,\n",
            "            6.5232e-03,  1.2516e-03],\n",
            "          [-6.4158e-04,  8.3861e-03, -8.6078e-05,  ...,  8.3879e-03,\n",
            "           -1.6126e-03,  6.2937e-03],\n",
            "          [ 7.8893e-03, -1.1830e-03,  8.0379e-03,  ..., -1.4937e-04,\n",
            "            6.7217e-03, -8.8076e-05]],\n",
            "\n",
            "         [[-3.4819e-02, -2.6243e-02, -3.4928e-02,  ..., -2.5396e-02,\n",
            "           -3.4595e-02, -2.5785e-02],\n",
            "          [-3.0168e-02, -3.5452e-02, -3.0166e-02,  ..., -3.5308e-02,\n",
            "           -3.0269e-02, -3.5996e-02],\n",
            "          [-3.3709e-02, -2.6510e-02, -3.3878e-02,  ..., -2.6391e-02,\n",
            "           -3.3875e-02, -2.6428e-02],\n",
            "          ...,\n",
            "          [-3.0323e-02, -3.5367e-02, -3.1728e-02,  ..., -3.5830e-02,\n",
            "           -3.1829e-02, -3.7360e-02],\n",
            "          [-3.4567e-02, -2.7157e-02, -3.3282e-02,  ..., -2.9368e-02,\n",
            "           -3.3027e-02, -2.8740e-02],\n",
            "          [-3.1749e-02, -3.5470e-02, -3.3357e-02,  ..., -3.6813e-02,\n",
            "           -3.3686e-02, -3.7846e-02]]]], device='cuda:0',\n",
            "       grad_fn=<CudnnConvolutionTransposeBackward>)\u001b[0m\n",
            "\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Final 1x1 convolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 14.73 GiB total capacity; 13.25 GiB already allocated; 147.88 MiB free; 583.31 MiB cached)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH6p2mq3TG4D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "6509d639-f4b6-42b9-a1a8-a3592f0c08d4"
      },
      "source": [
        "#a, b = zip(*val_losses)\n",
        "plt.title('Loss')\n",
        "plt.plot(train_losses, label='train')\n",
        "#plt.plot(a, b, label='val')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3M3MKsjF3cysgv4PSF2KEtME+b0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QY4owfQwm-Ni"
      },
      "source": [
        "___\n",
        "\n",
        "### Part 1b\n",
        "Implement a cost function\n",
        "\n",
        "You should still use cross-entropy as your cost function, but you may need to think hard about how exactly to set this up – your network should output cancer/not-cancer probabilities for each pixel, which can be viewed as a two-class classification problem.\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Adapt CrossEntropyLoss for 2 class pixel classification\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XPgrP88aOtfy",
        "colab": {}
      },
      "source": [
        "# You'll probably want a function or something to test input / output sizes of the ConvTranspose2d layer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jq22IyKanxo_",
        "colab": {}
      },
      "source": [
        "# Since you will be using the output of one network in two places(convolution and maxpooling),\n",
        "# you can't use nn.Sequential.\n",
        "# Instead you will write up the network like normal variable assignment as the example shown below:\n",
        "# You are welcome (and encouraged) to use the built-in batch normalization and dropout layer.\n",
        "\n",
        "# TODO: You need to change this to fit the UNet structure!!!\n",
        "class CancerDetection(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CancerDetection, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3,64,kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.conv3 = nn.Conv2d(64,128,kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.relu4 = nn.ReLU()\n",
        " \n",
        "  def forward(self, input):\n",
        "    conv1_out = self.conv1(input)\n",
        "    relu2_out = self.relu2(conv1_out)\n",
        "    conv3_out = self.conv3(relu2_out)\n",
        "    relu4_out = self.relu4(conv3_out) \n",
        "    return relu4_out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NAjagHCdGNAh",
        "colab": {}
      },
      "source": [
        "# Create your datasets and neural network as you have before\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RkieTbwlYWPS",
        "colab": {}
      },
      "source": [
        "# This is what was talked about in the video for memory management\n",
        "\n",
        "def scope():\n",
        "  try:\n",
        "    #your code for calling dataset and dataloader\n",
        "    gc.collect()\n",
        "    print(torch.cuda.memory_allocated(0) / 1e9)\n",
        "    \n",
        "    #for epochs:\n",
        "    # Call your model, figure out loss and accuracy\n",
        "    \n",
        "  except:\n",
        "    __ITB__()\n",
        "    \n",
        "scope()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CZ062Jv1jIIu"
      },
      "source": [
        "\n",
        "___\n",
        "\n",
        "### Part 2\n",
        "\n",
        "Plot performance over time\n",
        "\n",
        "Please generate a plot that shows loss on the training set as a function of training time. Make sure your axes are labeled!\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Plot training loss as function of training time (not Epochs)\n",
        "\n",
        "**DONE:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mTg1jyIsYVZN",
        "colab": {}
      },
      "source": [
        "# Your plotting code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S4s92S2_jQOG"
      },
      "source": [
        "___\n",
        "\n",
        "### Part 3\n",
        "\n",
        "Generate a prediction on the pos_test_000072.png image\n",
        "\n",
        "Calculate the output of your trained network on the pos_test_000072.png image,\n",
        "then make a hard decision (cancerous/not-cancerous) for each pixel.\n",
        "The resulting image should be black-and-white, where white pixels represent things\n",
        "you think are probably cancerous.\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "**NOTE:**\n",
        "\n",
        "Guessing that the pixel is not cancerous every single time will give you an accuracy of ~ 85%.\n",
        "Your trained network should be able to do better than that (but you will not be graded on accuracy).\n",
        "This is the result I got after 1 hour or training.\n",
        "\n",
        "![](http://liftothers.org/dokuwiki/lib/exe/fetch.php?w=400&tok=d23e0b&media=cs501r_f2016:training_accuracy.png)\n",
        "![](http://liftothers.org/dokuwiki/lib/exe/fetch.php?w=400&tok=bb8e3c&media=cs501r_f2016:training_loss.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XXfG3wClh8an",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "# Code for testing prediction on an image\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}