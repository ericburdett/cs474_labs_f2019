{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Lab10",
      "provenance": [],
      "collapsed_sections": [
        "sBOvJdJfkXIL",
        "kFoEeTYHDq2s",
        "7z6g7a_Y84n0",
        "TBigIUFTukeJ",
        "zcz0JGXjxFGe",
        "2vJVbYcAJAf2",
        "gMOzGDND9FD1",
        "eOtl8z8G9wbr",
        "2Krh0eYy18R9",
        "aEDv_-H7BvM0",
        "YH5mQBaa-_0b",
        "XMKRI77_-8nc",
        "qBT5jgifC7Im",
        "_K7F19SPQo6U",
        "9YLXvK51RnuL",
        "AHmjSVf_FNHv",
        "gPXJkNubFyY6",
        "iD45m3IwF9hh",
        "usQE-rSPZq_X",
        "IoA0tZZCa_1k",
        "8pa5vFJ5EUjv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEwpv0GZ_CrT",
        "colab_type": "text"
      },
      "source": [
        "<a \n",
        "href=\"https://colab.research.google.com/github/ericburdett/cs474_labs_f2019/blob/master/DL_Lab10.ipynb\"\n",
        "  target=\"_parent\">\n",
        "  <img\n",
        "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "    alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnoEhAVvBcMj",
        "colab_type": "text"
      },
      "source": [
        "#Lab 10: Transfer Learning/Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBOvJdJfkXIL",
        "colab_type": "text"
      },
      "source": [
        "## Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiuvTUWOjtBC",
        "colab_type": "text"
      },
      "source": [
        "### Objective\n",
        "\n",
        "- Gain experience fine-tuning pre-trained models to domain-specific applications.\n",
        "\n",
        "### Deliverable\n",
        "\n",
        "For this lab you will submit an ipython notebook via learning suite. The bulk of the work is in modifying fine-tuning a pre-trained ResNet. Fine-tuning the GPT-2 language model is pretty easy. The provided code works as is; you will just have to swap in your own text dataset.\n",
        "\n",
        "### Grading\n",
        "\n",
        "- 35% Create a dataset class for your own dataset\n",
        "- 35% Create a network class that wraps a pretrained ResNet\n",
        "- 20% Implement unfreezing in the network class\n",
        "- 10% Fine-tune GPT-2 on your own dataset\n",
        "\n",
        "### Tips\n",
        "- Your life will be better if you download a dataset that already has the data in the expected format for ImageFolder (make sure to read the documentation!). The datasets recommended below are in the correct format.\n",
        "- Get the CNN working on the provided dataset (bird species classification) before swapping in your own.\n",
        "- For reference on freezing/unfreezing network weights, see [this github gist](https://gist.github.com/jcjohnson/6e41e8512c17eae5da50aebef3378a4c)\n",
        "- For training GPT-2, first try the medium-size (355M parameter) model. If your Colab instance doesn't have enough GPU space, you may need to switch to the small-size (124M parameter) model, but the results will be less impressive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKzRORuLBNLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision.models import resnet152\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "import os\n",
        "import sys\n",
        "from PIL import Image, ImageOps\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4R3D8Mr8b54",
        "colab_type": "text"
      },
      "source": [
        "## 1 Fine-tune a ResNet for image classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFoEeTYHDq2s",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Find a dataset to fine-tune on, and make a Dataset class (1 hr.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z6g7a_Y84n0",
        "colab_type": "text"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8NtFZRd5hcm",
        "colab_type": "text"
      },
      "source": [
        "- Inherit from torch.utils.data.Dataset (DONE)\n",
        "- Use a [torchvision.datasets.ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder) (DONE)\n",
        "- Don't spend too long finding another dataset. Some suggestions that you are free to use:\n",
        " - https://www.kaggle.com/akash2907/bird-species-classification\n",
        " - https://www.kaggle.com/jessicali9530/stanford-dogs-dataset\n",
        " - https://www.kaggle.com/puneet6060/intel-image-classification (DONE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBigIUFTukeJ",
        "colab_type": "text"
      },
      "source": [
        "#### Help for downloading kaggle datasets\n",
        "Downloading Kaggle datasets requires authentication, so you can't just download from a url. Here are some step-by-step instructions of how to get Kaggle datasets in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X29UC6CvwfQ",
        "colab_type": "text"
      },
      "source": [
        "1. Create an API key in Kaggle\n",
        "    - Click on profile photo\n",
        "    - Go to 'My Account'\n",
        "    - Scroll down to the API access section and click \"Create New API Token\"\n",
        "    - `kaggle.json` is now downloaded to your computer\n",
        "\n",
        "2. Upload the API key and install the Kaggle API client by running the next cell (run it again if it throws an error the first time). Also, `files.upload()` may not work in Firefox. One solution is to expand the Files banner (indicated by the '>' tab on the left side of the page) and use that to upload the key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mhjc0pM7jOoZ",
        "colab_type": "code",
        "outputId": "8b2d4ba1-6bce-4e46-fe06-f159758cd82e",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "# Run this cell and select the kaggle.json file downloaded\n",
        "# from the Kaggle account settings page.\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "# Next, install the Kaggle API client.\n",
        "!pip install -q kaggle\n",
        "# Let's make sure the kaggle.json file is present.\n",
        "!ls -lha kaggle.json\n",
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so move it there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-003b1979-b848-4e79-9ee6-13d5911f968e\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-003b1979-b848-4e79-9ee6-13d5911f968e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "-rw-r--r-- 1 root root 67 Dec 12 00:16 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGlIa4SIwEXB",
        "colab_type": "text"
      },
      "source": [
        "3. Copy the desired dataset locally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HtB-XdIr1EE",
        "colab_type": "code",
        "outputId": "12a9321f-437c-43b5-8356-bb4b2637604f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Download the intel-image-classification dataset from kaggle\n",
        "!kaggle datasets download -d puneet6060/intel-image-classification"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading intel-image-classification.zip to /content\n",
            " 97% 337M/346M [00:06<00:00, 57.4MB/s]\n",
            "100% 346M/346M [00:06<00:00, 53.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcz0JGXjxFGe",
        "colab_type": "text"
      },
      "source": [
        "#### Make the Dataset class\n",
        "See the implementation below for reference, and make your own."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lthPlsGeK4CX",
        "colab_type": "code",
        "outputId": "895295d6-dbfe-425f-bd75-f067daeac27b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "class BirdDataset(Dataset):\n",
        "    def __init__(self, zip_file='bird-species-classification.zip', size=256, train=True, upload=False):\n",
        "        super(BirdDataset, self).__init__()\n",
        "        \n",
        "        self.train = train\n",
        "        extract_dir = os.path.splitext(zip_file)[0]\n",
        "        if not os.path.exists(extract_dir):\n",
        "            os.makedirs(extract_dir)\n",
        "            self.extract_zip(zip_file, extract_dir)\n",
        "            # Resize the images - originally they are high resolution. We could do this\n",
        "            # in the DataLoader, but it will read the full-resolution files from disk\n",
        "            # every time before resizing them, making training slow\n",
        "            self.resize(extract_dir, size=size)\n",
        "\n",
        "        postfix = 'train' if train else 'test'\n",
        "            \n",
        "        if train:\n",
        "            # The bird-species dataset mistakenly has a train_data folder inside of train_data\n",
        "            self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'train_data', 'train_data'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "        else:\n",
        "            self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'test_data', 'test_data'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "    def extract_zip(self, zip_file, extract_dir):\n",
        "        print(\"Extracting\", zip_file)\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "\n",
        "    def resize(self, path, size=256):\n",
        "        \"\"\"Resizes all images in place\"\"\"\n",
        "        print(\"Resizing images\")\n",
        "        dirs = os.walk(path)\n",
        "        for root, dirs, files in os.walk(path):\n",
        "            for item in files:\n",
        "                name = os.path.join(root, item)\n",
        "                if os.path.isfile(name):\n",
        "                    im = Image.open(name)\n",
        "                    im = ImageOps.fit(im, (size, size))\n",
        "                    im.save(name[:-3] + 'bmp', 'BMP')\n",
        "                    os.remove(name)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.dataset_folder[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset_folder)\n",
        "\n",
        "bird_data = BirdDataset()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting bird-species-classification.zip\n",
            "Resizing images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9WQjsPKMYYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class IntelDataset(Dataset):\n",
        "  def __init__(self, zip_file='intel-image-classification.zip', size=256, train=True, upload=False):\n",
        "    super(IntelDataset, self).__init__()\n",
        "\n",
        "    self.train = train\n",
        "    extract_dir = os.path.splitext(zip_file)[0]\n",
        "    if not os.path.exists(extract_dir):\n",
        "      os.makedirs(extract_dir)\n",
        "      self.extract_zip(zip_file, extract_dir)\n",
        "      self.resize(extract_dir, size=size)\n",
        "\n",
        "    postfix = 'train' if train else 'test'\n",
        "\n",
        "    if train:\n",
        "      self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'seg_train', 'seg_train'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "    else:\n",
        "      self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'seg_test', 'seg_test'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "  def extract_zip(self, zip_file, extract_dir):\n",
        "    print(\"Extracting\", zip_file)\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "      zip_ref.extractall(extract_dir)\n",
        "\n",
        "  def resize(self, path, size=256):\n",
        "    \"\"\"Resizes all images in place\"\"\"\n",
        "    print(\"Resizing images\")\n",
        "    dirs = os.walk(path)\n",
        "    for root, dirs, files in os.walk(path):\n",
        "      for item in files:\n",
        "        name = os.path.join(root, item)\n",
        "        if os.path.isfile(name):\n",
        "          im = Image.open(name)\n",
        "          im = ImageOps.fit(im, (size, size))\n",
        "          im.save(name[:-3] + 'bmp', 'BMP')\n",
        "          os.remove(name)\n",
        "\n",
        "  def class_num(self):\n",
        "    return len(self.dataset_folder.class_to_idx)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    return self.dataset_folder[i]\n",
        "\n",
        "  def __len__(self):\n",
        "    return 150 #len(self.dataset_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vJVbYcAJAf2",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Wrap a pretrained ResNet in an `nn.Module` (30 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMOzGDND9FD1",
        "colab_type": "text"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLvmDHbl9IyG",
        "colab_type": "text"
      },
      "source": [
        "- Make a model class that inherits from `nn.Module` (DONE)\n",
        "- Wrap a pretrained ResNet and swap out the last layer of that network with a layer that maps to the number of classes in your new dataset (DONE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOtl8z8G9wbr",
        "colab_type": "text"
      },
      "source": [
        "#### Make your model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auv1S5pfeDRB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6bb2232a-13ab-43a7-dd74-f2be07b18dd5"
      },
      "source": [
        "!ls intel-image-classification/seg_train/seg_train"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "buildings  forest  glacier  mountain  sea  street\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY-XU4Mwas0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNetIntel(nn.Module):\n",
        "    def __init__(self, num_classes, start_frozen=False):\n",
        "        super(ResNetIntel, self).__init__()\n",
        "\n",
        "        # Part 1.2\n",
        "        # Load the model - make sure it is pre-trained\n",
        "        self.model = resnet152(pretrained=True)\n",
        "\n",
        "        # Part 1.4\n",
        "        if start_frozen:\n",
        "            self.model.requires_grad=False\n",
        "            # Turn off all gradients of the resnet\n",
        "        \n",
        "        # Part 1.2\n",
        "        # Look at the code of torchvision.models.resnet152 to find the name of the attribute to override (the last layer of the resnet)\n",
        "        # Override the last layer of the neural network to map to the correct number of classes. Note that this new layer has requires_grad = True\n",
        "        fc = nn.Linear(in_features=2048, out_features=num_classes)\n",
        "        fc.requires_grad=True\n",
        "        self.model.fc = fc\n",
        "\n",
        "        \n",
        "    def unfreeze(self, n_layers):\n",
        "        # Part 1.4\n",
        "        # Turn on gradients for the last n_layers\n",
        "        layers = [x for x in self.model.children()][::-1]\n",
        "        \n",
        "        for i in range(n_layers):\n",
        "          layers[i].requires_grad=True\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Part 1.2\n",
        "        # Pass x through the resnet\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Krh0eYy18R9",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 Read through and run this training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yOGrrw2gbIPf",
        "colab": {}
      },
      "source": [
        "def accuracy(y_hat, y_truth):\n",
        "    \"\"\"Gets average accuracy of a vector of predictions\"\"\"\n",
        "    \n",
        "    preds = torch.argmax(y_hat, dim=1)\n",
        "    acc = torch.mean((preds == y_truth).float())\n",
        "    return acc\n",
        "\n",
        "def evaluate(model, objective, val_loader, device):\n",
        "    \"\"\"Gets average accuracy and loss for the validation set\"\"\"\n",
        "\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "    # model.eval() so that batchnorm and dropout work in eval mode\n",
        "    model.eval()\n",
        "    # torch.no_grad() to turn off computation graph creation. This allows for temporal\n",
        "    # and spatial complexity improvements, which allows for larger validation batch \n",
        "    # sizes so it’s recommended\n",
        "    with torch.no_grad():\n",
        "        for x, y_truth in val_loader:\n",
        "\n",
        "            x, y_truth = x.to(device), y_truth.to(device)\n",
        "            y_hat = model(x)\n",
        "            val_loss = objective(y_hat, y_truth)\n",
        "            val_acc = accuracy(y_hat, y_truth)\n",
        "\n",
        "            val_losses.append(val_loss.item())\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return torch.mean(torch.Tensor(val_losses)), torch.mean(torch.Tensor(val_accs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKESMcKi2E_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(start_frozen=False, model_unfreeze=0):\n",
        "    \"\"\"Fine-tunes a CNN\n",
        "    Args:\n",
        "        start_frozen (bool): whether to start with the network weights frozen.\n",
        "        model_unfreeze (int): the maximum number of network layers to unfreeze\n",
        "    \"\"\"\n",
        "    epochs = 20\n",
        "    # Start with a very low learning rate\n",
        "    lr = .00005\n",
        "    val_every = 3\n",
        "    num_classes = 16\n",
        "    batch_size = 32\n",
        "    device = torch.device('cuda:0')\n",
        "\n",
        "    # Data\n",
        "    train_dataset = IntelDataset(upload=True, train=True)\n",
        "    val_dataset = IntelDataset(upload=True, train=False)\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                              shuffle=True,\n",
        "                              num_workers=8,\n",
        "                              batch_size=batch_size)\n",
        "    val_loader = DataLoader(val_dataset,\n",
        "                              shuffle=True,\n",
        "                              num_workers=8,\n",
        "                              batch_size=batch_size)\n",
        "    \n",
        "    # Model\n",
        "    model = ResNetIntel(num_classes, start_frozen=start_frozen).to(device)\n",
        "    \n",
        "    # Objective\n",
        "    objective = nn.CrossEntropyLoss()\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-1)\n",
        "\n",
        "    # Progress bar\n",
        "    pbar = tqdm(total=len(train_loader) * epochs)\n",
        "\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "    \n",
        "    cnt = 0\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Implement model unfreezing\n",
        "        if epoch < model_unfreeze:\n",
        "            # Part 1.4\n",
        "            # Unfreeze the last layers, one more each epoch\n",
        "            model.unfreeze(epoch)\n",
        "        \n",
        "        for x, y_truth in train_loader:\n",
        "        \n",
        "            x, y_truth = x.to(device), y_truth.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_hat = model(x)\n",
        "            train_loss = objective(y_hat, y_truth)\n",
        "            train_acc = accuracy(y_hat, y_truth)\n",
        "\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_accs.append(train_acc)\n",
        "            train_losses.append(train_loss.item())\n",
        "\n",
        "            if cnt % val_every == 0:\n",
        "                val_loss, val_acc = evaluate(model, objective, val_loader, device)\n",
        "                val_losses.append(val_loss)\n",
        "                val_accs.append(val_acc)\n",
        "\n",
        "            pbar.set_description('train loss:{:.4f}, train accuracy:{:.4f}.'.format(train_loss.item(), train_acc))\n",
        "            pbar.update(1)\n",
        "            cnt += 1\n",
        "\n",
        "    pbar.close()\n",
        "    plt.subplot(121)\n",
        "    plt.plot(np.arange(len(train_accs)), train_accs, label='Train Accuracy')\n",
        "    plt.plot(np.arange(len(train_accs), step=val_every), val_accs, label='Val Accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(122)\n",
        "    plt.plot(np.arange(len(train_losses)), train_losses, label='Train Loss')\n",
        "    plt.plot(np.arange(len(train_losses), step=val_every), val_losses, label='Val Loss')\n",
        "    plt.legend()\n",
        "    plt.show()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvnxeLotchiH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "880e1675-e8b6-4817-95dd-da66b3b3f768"
      },
      "source": [
        "train(start_frozen=False, model_unfreeze=0)  "
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "train loss:2.7658, train accuracy:0.0312.:   0%|          | 0/100 [00:04<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "train loss:2.7658, train accuracy:0.0312.:   1%|          | 1/100 [00:04<06:45,  4.09s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:2.6731, train accuracy:0.0312.:   1%|          | 1/100 [00:05<06:45,  4.09s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:2.6731, train accuracy:0.0312.:   2%|▏         | 2/100 [00:05<05:08,  3.15s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:2.5769, train accuracy:0.0938.:   2%|▏         | 2/100 [00:05<05:08,  3.15s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:2.5769, train accuracy:0.0938.:   3%|▎         | 3/100 [00:05<04:00,  2.48s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:2.4812, train accuracy:0.1875.:   3%|▎         | 3/100 [00:09<04:00,  2.48s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:2.4812, train accuracy:0.1875.:   4%|▍         | 4/100 [00:09<04:17,  2.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:2.3893, train accuracy:0.4091.:   4%|▍         | 4/100 [00:09<04:17,  2.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:2.3893, train accuracy:0.4091.:   5%|▌         | 5/100 [00:09<03:18,  2.09s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:2.2592, train accuracy:0.6875.:   5%|▌         | 5/100 [00:11<03:18,  2.09s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:2.2592, train accuracy:0.6875.:   6%|▌         | 6/100 [00:11<03:08,  2.01s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:2.1666, train accuracy:0.8125.:   6%|▌         | 6/100 [00:14<03:08,  2.01s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:2.1666, train accuracy:0.8125.:   7%|▋         | 7/100 [00:14<03:39,  2.36s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:2.0697, train accuracy:0.8438.:   7%|▋         | 7/100 [00:15<03:39,  2.36s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:2.0697, train accuracy:0.8438.:   8%|▊         | 8/100 [00:15<02:58,  1.94s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.9933, train accuracy:0.9688.:   8%|▊         | 8/100 [00:16<02:58,  1.94s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.9933, train accuracy:0.9688.:   9%|▉         | 9/100 [00:16<02:29,  1.64s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.8974, train accuracy:0.9545.:   9%|▉         | 9/100 [00:19<02:29,  1.64s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.8974, train accuracy:0.9545.:  10%|█         | 10/100 [00:19<03:02,  2.03s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.7840, train accuracy:1.0000.:  10%|█         | 10/100 [00:21<03:02,  2.03s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.7840, train accuracy:1.0000.:  11%|█         | 11/100 [00:21<02:54,  1.96s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.7088, train accuracy:1.0000.:  11%|█         | 11/100 [00:22<02:54,  1.96s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.7088, train accuracy:1.0000.:  12%|█▏        | 12/100 [00:22<02:26,  1.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.6273, train accuracy:1.0000.:  12%|█▏        | 12/100 [00:25<02:26,  1.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.6273, train accuracy:1.0000.:  13%|█▎        | 13/100 [00:25<03:04,  2.12s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.5313, train accuracy:1.0000.:  13%|█▎        | 13/100 [00:26<03:04,  2.12s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.5313, train accuracy:1.0000.:  14%|█▍        | 14/100 [00:26<02:32,  1.78s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.4459, train accuracy:1.0000.:  14%|█▍        | 14/100 [00:27<02:32,  1.78s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.4459, train accuracy:1.0000.:  15%|█▌        | 15/100 [00:27<02:03,  1.45s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.3497, train accuracy:1.0000.:  15%|█▌        | 15/100 [00:31<02:03,  1.45s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.3497, train accuracy:1.0000.:  16%|█▌        | 16/100 [00:31<03:08,  2.24s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.2824, train accuracy:1.0000.:  16%|█▌        | 16/100 [00:32<03:08,  2.24s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.2824, train accuracy:1.0000.:  17%|█▋        | 17/100 [00:32<02:34,  1.87s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.2094, train accuracy:1.0000.:  17%|█▋        | 17/100 [00:33<02:34,  1.87s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.2094, train accuracy:1.0000.:  18%|█▊        | 18/100 [00:33<02:10,  1.59s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.1413, train accuracy:1.0000.:  18%|█▊        | 18/100 [00:36<02:10,  1.59s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.1413, train accuracy:1.0000.:  19%|█▉        | 19/100 [00:36<02:48,  2.08s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.0588, train accuracy:1.0000.:  19%|█▉        | 19/100 [00:37<02:48,  2.08s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:1.0588, train accuracy:1.0000.:  20%|██        | 20/100 [00:37<02:13,  1.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.9981, train accuracy:1.0000.:  20%|██        | 20/100 [00:39<02:13,  1.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.9981, train accuracy:1.0000.:  21%|██        | 21/100 [00:39<02:15,  1.72s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.9272, train accuracy:1.0000.:  21%|██        | 21/100 [00:42<02:15,  1.72s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.9272, train accuracy:1.0000.:  22%|██▏       | 22/100 [00:42<02:48,  2.16s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.8621, train accuracy:1.0000.:  22%|██▏       | 22/100 [00:43<02:48,  2.16s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.8621, train accuracy:1.0000.:  23%|██▎       | 23/100 [00:43<02:18,  1.80s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.8057, train accuracy:1.0000.:  23%|██▎       | 23/100 [00:44<02:18,  1.80s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.8057, train accuracy:1.0000.:  24%|██▍       | 24/100 [00:44<01:57,  1.55s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.7551, train accuracy:1.0000.:  24%|██▍       | 24/100 [00:47<01:57,  1.55s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.7551, train accuracy:1.0000.:  25%|██▌       | 25/100 [00:47<02:26,  1.95s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.6914, train accuracy:1.0000.:  25%|██▌       | 25/100 [00:48<02:26,  1.95s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.6914, train accuracy:1.0000.:  26%|██▌       | 26/100 [00:48<02:21,  1.91s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.6449, train accuracy:1.0000.:  26%|██▌       | 26/100 [00:49<02:21,  1.91s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.6449, train accuracy:1.0000.:  27%|██▋       | 27/100 [00:49<01:58,  1.62s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.5963, train accuracy:1.0000.:  27%|██▋       | 27/100 [00:53<01:58,  1.62s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.5963, train accuracy:1.0000.:  28%|██▊       | 28/100 [00:53<02:30,  2.09s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.5527, train accuracy:1.0000.:  28%|██▊       | 28/100 [00:53<02:30,  2.09s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.5527, train accuracy:1.0000.:  29%|██▉       | 29/100 [00:53<02:03,  1.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.5154, train accuracy:1.0000.:  29%|██▉       | 29/100 [00:54<02:03,  1.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.5154, train accuracy:1.0000.:  30%|███       | 30/100 [00:54<01:40,  1.43s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.4814, train accuracy:1.0000.:  30%|███       | 30/100 [00:58<01:40,  1.43s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.4814, train accuracy:1.0000.:  31%|███       | 31/100 [00:58<02:31,  2.20s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.4432, train accuracy:1.0000.:  31%|███       | 31/100 [00:59<02:31,  2.20s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.4432, train accuracy:1.0000.:  32%|███▏      | 32/100 [00:59<02:03,  1.82s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.4096, train accuracy:1.0000.:  32%|███▏      | 32/100 [01:00<02:03,  1.82s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.4096, train accuracy:1.0000.:  33%|███▎      | 33/100 [01:00<01:44,  1.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.3794, train accuracy:1.0000.:  33%|███▎      | 33/100 [01:03<01:44,  1.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.3794, train accuracy:1.0000.:  34%|███▍      | 34/100 [01:03<02:13,  2.03s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.3529, train accuracy:1.0000.:  34%|███▍      | 34/100 [01:04<02:13,  2.03s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.3529, train accuracy:1.0000.:  35%|███▌      | 35/100 [01:04<01:45,  1.63s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.3302, train accuracy:1.0000.:  35%|███▌      | 35/100 [01:06<01:45,  1.63s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.3302, train accuracy:1.0000.:  36%|███▌      | 36/100 [01:06<01:47,  1.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.3075, train accuracy:1.0000.:  36%|███▌      | 36/100 [01:09<01:47,  1.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.3075, train accuracy:1.0000.:  37%|███▋      | 37/100 [01:09<02:14,  2.13s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.2842, train accuracy:1.0000.:  37%|███▋      | 37/100 [01:10<02:14,  2.13s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.2842, train accuracy:1.0000.:  38%|███▊      | 38/100 [01:10<01:49,  1.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.2715, train accuracy:1.0000.:  38%|███▊      | 38/100 [01:11<01:49,  1.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.2715, train accuracy:1.0000.:  39%|███▉      | 39/100 [01:11<01:32,  1.52s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.2643, train accuracy:1.0000.:  39%|███▉      | 39/100 [01:14<01:32,  1.52s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.2643, train accuracy:1.0000.:  40%|████      | 40/100 [01:14<01:56,  1.94s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.2380, train accuracy:1.0000.:  40%|████      | 40/100 [01:15<01:56,  1.94s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.2380, train accuracy:1.0000.:  41%|████      | 41/100 [01:15<01:51,  1.89s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.2225, train accuracy:1.0000.:  41%|████      | 41/100 [01:16<01:51,  1.89s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.2225, train accuracy:1.0000.:  42%|████▏     | 42/100 [01:16<01:32,  1.60s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.2114, train accuracy:1.0000.:  42%|████▏     | 42/100 [01:19<01:32,  1.60s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.2114, train accuracy:1.0000.:  43%|████▎     | 43/100 [01:19<01:57,  2.06s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1981, train accuracy:1.0000.:  43%|████▎     | 43/100 [01:20<01:57,  2.06s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1981, train accuracy:1.0000.:  44%|████▍     | 44/100 [01:20<01:36,  1.72s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1873, train accuracy:1.0000.:  44%|████▍     | 44/100 [01:21<01:36,  1.72s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1873, train accuracy:1.0000.:  45%|████▌     | 45/100 [01:21<01:17,  1.41s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1755, train accuracy:1.0000.:  45%|████▌     | 45/100 [01:25<01:17,  1.41s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1755, train accuracy:1.0000.:  46%|████▌     | 46/100 [01:25<01:58,  2.19s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1683, train accuracy:1.0000.:  46%|████▌     | 46/100 [01:26<01:58,  2.19s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1683, train accuracy:1.0000.:  47%|████▋     | 47/100 [01:26<01:36,  1.82s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1604, train accuracy:1.0000.:  47%|████▋     | 47/100 [01:27<01:36,  1.82s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1604, train accuracy:1.0000.:  48%|████▊     | 48/100 [01:27<01:20,  1.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1637, train accuracy:1.0000.:  48%|████▊     | 48/100 [01:30<01:20,  1.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1637, train accuracy:1.0000.:  49%|████▉     | 49/100 [01:30<01:43,  2.04s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1462, train accuracy:1.0000.:  49%|████▉     | 49/100 [01:31<01:43,  2.04s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1462, train accuracy:1.0000.:  50%|█████     | 50/100 [01:31<01:21,  1.63s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1430, train accuracy:1.0000.:  50%|█████     | 50/100 [01:33<01:21,  1.63s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1430, train accuracy:1.0000.:  51%|█████     | 51/100 [01:33<01:22,  1.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1360, train accuracy:1.0000.:  51%|█████     | 51/100 [01:36<01:22,  1.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1360, train accuracy:1.0000.:  52%|█████▏    | 52/100 [01:36<01:42,  2.14s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1284, train accuracy:1.0000.:  52%|█████▏    | 52/100 [01:37<01:42,  2.14s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1284, train accuracy:1.0000.:  53%|█████▎    | 53/100 [01:37<01:24,  1.79s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1235, train accuracy:1.0000.:  53%|█████▎    | 53/100 [01:38<01:24,  1.79s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1235, train accuracy:1.0000.:  54%|█████▍    | 54/100 [01:38<01:10,  1.54s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1353, train accuracy:1.0000.:  54%|█████▍    | 54/100 [01:41<01:10,  1.54s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1353, train accuracy:1.0000.:  55%|█████▌    | 55/100 [01:41<01:28,  1.96s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1248, train accuracy:1.0000.:  55%|█████▌    | 55/100 [01:43<01:28,  1.96s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1248, train accuracy:1.0000.:  56%|█████▌    | 56/100 [01:43<01:24,  1.92s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1136, train accuracy:1.0000.:  56%|█████▌    | 56/100 [01:43<01:24,  1.92s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1136, train accuracy:1.0000.:  57%|█████▋    | 57/100 [01:43<01:10,  1.63s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1085, train accuracy:1.0000.:  57%|█████▋    | 57/100 [01:47<01:10,  1.63s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1085, train accuracy:1.0000.:  58%|█████▊    | 58/100 [01:47<01:27,  2.08s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1055, train accuracy:1.0000.:  58%|█████▊    | 58/100 [01:48<01:27,  2.08s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1055, train accuracy:1.0000.:  59%|█████▉    | 59/100 [01:48<01:11,  1.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1052, train accuracy:1.0000.:  59%|█████▉    | 59/100 [01:48<01:11,  1.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1052, train accuracy:1.0000.:  60%|██████    | 60/100 [01:48<00:56,  1.42s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1038, train accuracy:1.0000.:  60%|██████    | 60/100 [01:52<00:56,  1.42s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1038, train accuracy:1.0000.:  61%|██████    | 61/100 [01:52<01:25,  2.19s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1023, train accuracy:1.0000.:  61%|██████    | 61/100 [01:53<01:25,  2.19s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.1023, train accuracy:1.0000.:  62%|██████▏   | 62/100 [01:53<01:08,  1.82s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0950, train accuracy:1.0000.:  62%|██████▏   | 62/100 [01:54<01:08,  1.82s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0950, train accuracy:1.0000.:  63%|██████▎   | 63/100 [01:54<00:57,  1.55s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0953, train accuracy:1.0000.:  63%|██████▎   | 63/100 [01:57<00:57,  1.55s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0953, train accuracy:1.0000.:  64%|██████▍   | 64/100 [01:57<01:13,  2.04s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0953, train accuracy:1.0000.:  64%|██████▍   | 64/100 [01:58<01:13,  2.04s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0953, train accuracy:1.0000.:  65%|██████▌   | 65/100 [01:58<00:57,  1.63s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0900, train accuracy:1.0000.:  65%|██████▌   | 65/100 [02:00<00:57,  1.63s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0900, train accuracy:1.0000.:  66%|██████▌   | 66/100 [02:00<00:56,  1.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0902, train accuracy:1.0000.:  66%|██████▌   | 66/100 [02:03<00:56,  1.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0902, train accuracy:1.0000.:  67%|██████▋   | 67/100 [02:03<01:09,  2.11s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0877, train accuracy:1.0000.:  67%|██████▋   | 67/100 [02:04<01:09,  2.11s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0877, train accuracy:1.0000.:  68%|██████▊   | 68/100 [02:04<00:56,  1.76s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0858, train accuracy:1.0000.:  68%|██████▊   | 68/100 [02:05<00:56,  1.76s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0858, train accuracy:1.0000.:  69%|██████▉   | 69/100 [02:05<00:46,  1.51s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0838, train accuracy:1.0000.:  69%|██████▉   | 69/100 [02:08<00:46,  1.51s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0838, train accuracy:1.0000.:  70%|███████   | 70/100 [02:08<00:58,  1.94s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0815, train accuracy:1.0000.:  70%|███████   | 70/100 [02:09<00:58,  1.94s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0815, train accuracy:1.0000.:  71%|███████   | 71/100 [02:09<00:55,  1.90s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0814, train accuracy:1.0000.:  71%|███████   | 71/100 [02:10<00:55,  1.90s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0814, train accuracy:1.0000.:  72%|███████▏  | 72/100 [02:10<00:45,  1.61s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0829, train accuracy:1.0000.:  72%|███████▏  | 72/100 [02:14<00:45,  1.61s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0829, train accuracy:1.0000.:  73%|███████▎  | 73/100 [02:14<00:56,  2.07s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0781, train accuracy:1.0000.:  73%|███████▎  | 73/100 [02:15<00:56,  2.07s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0781, train accuracy:1.0000.:  74%|███████▍  | 74/100 [02:15<00:45,  1.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0793, train accuracy:1.0000.:  74%|███████▍  | 74/100 [02:15<00:45,  1.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0793, train accuracy:1.0000.:  75%|███████▌  | 75/100 [02:15<00:35,  1.42s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0749, train accuracy:1.0000.:  75%|███████▌  | 75/100 [02:19<00:35,  1.42s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0749, train accuracy:1.0000.:  76%|███████▌  | 76/100 [02:19<00:53,  2.21s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0771, train accuracy:1.0000.:  76%|███████▌  | 76/100 [02:20<00:53,  2.21s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0771, train accuracy:1.0000.:  77%|███████▋  | 77/100 [02:20<00:42,  1.83s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0737, train accuracy:1.0000.:  77%|███████▋  | 77/100 [02:21<00:42,  1.83s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0737, train accuracy:1.0000.:  78%|███████▊  | 78/100 [02:21<00:34,  1.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0753, train accuracy:1.0000.:  78%|███████▊  | 78/100 [02:24<00:34,  1.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0753, train accuracy:1.0000.:  79%|███████▉  | 79/100 [02:24<00:42,  2.02s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0710, train accuracy:1.0000.:  79%|███████▉  | 79/100 [02:25<00:42,  2.02s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0710, train accuracy:1.0000.:  80%|████████  | 80/100 [02:25<00:32,  1.62s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0723, train accuracy:1.0000.:  80%|████████  | 80/100 [02:27<00:32,  1.62s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0723, train accuracy:1.0000.:  81%|████████  | 81/100 [02:27<00:31,  1.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0701, train accuracy:1.0000.:  81%|████████  | 81/100 [02:30<00:31,  1.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0701, train accuracy:1.0000.:  82%|████████▏ | 82/100 [02:30<00:38,  2.12s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0701, train accuracy:1.0000.:  82%|████████▏ | 82/100 [02:31<00:38,  2.12s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0701, train accuracy:1.0000.:  83%|████████▎ | 83/100 [02:31<00:30,  1.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0677, train accuracy:1.0000.:  83%|████████▎ | 83/100 [02:32<00:30,  1.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0677, train accuracy:1.0000.:  84%|████████▍ | 84/100 [02:32<00:24,  1.52s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0680, train accuracy:1.0000.:  84%|████████▍ | 84/100 [02:35<00:24,  1.52s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0680, train accuracy:1.0000.:  85%|████████▌ | 85/100 [02:35<00:29,  1.94s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0674, train accuracy:1.0000.:  85%|████████▌ | 85/100 [02:36<00:29,  1.94s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0674, train accuracy:1.0000.:  86%|████████▌ | 86/100 [02:36<00:26,  1.90s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0664, train accuracy:1.0000.:  86%|████████▌ | 86/100 [02:37<00:26,  1.90s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0664, train accuracy:1.0000.:  87%|████████▋ | 87/100 [02:37<00:20,  1.61s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0635, train accuracy:1.0000.:  87%|████████▋ | 87/100 [02:41<00:20,  1.61s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0635, train accuracy:1.0000.:  88%|████████▊ | 88/100 [02:41<00:25,  2.09s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0644, train accuracy:1.0000.:  88%|████████▊ | 88/100 [02:42<00:25,  2.09s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0644, train accuracy:1.0000.:  89%|████████▉ | 89/100 [02:42<00:19,  1.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0625, train accuracy:1.0000.:  89%|████████▉ | 89/100 [02:42<00:19,  1.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0625, train accuracy:1.0000.:  90%|█████████ | 90/100 [02:42<00:14,  1.43s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0609, train accuracy:1.0000.:  90%|█████████ | 90/100 [02:46<00:14,  1.43s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0609, train accuracy:1.0000.:  91%|█████████ | 91/100 [02:46<00:19,  2.22s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0604, train accuracy:1.0000.:  91%|█████████ | 91/100 [02:47<00:19,  2.22s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0604, train accuracy:1.0000.:  92%|█████████▏| 92/100 [02:47<00:14,  1.84s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0615, train accuracy:1.0000.:  92%|█████████▏| 92/100 [02:48<00:14,  1.84s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0615, train accuracy:1.0000.:  93%|█████████▎| 93/100 [02:48<00:11,  1.58s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0593, train accuracy:1.0000.:  93%|█████████▎| 93/100 [02:51<00:11,  1.58s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0593, train accuracy:1.0000.:  94%|█████████▍| 94/100 [02:51<00:12,  2.05s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0607, train accuracy:1.0000.:  94%|█████████▍| 94/100 [02:52<00:12,  2.05s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0607, train accuracy:1.0000.:  95%|█████████▌| 95/100 [02:52<00:08,  1.65s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0564, train accuracy:1.0000.:  95%|█████████▌| 95/100 [02:54<00:08,  1.65s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0564, train accuracy:1.0000.:  96%|█████████▌| 96/100 [02:54<00:06,  1.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0639, train accuracy:1.0000.:  96%|█████████▌| 96/100 [02:57<00:06,  1.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0639, train accuracy:1.0000.:  97%|█████████▋| 97/100 [02:57<00:06,  2.14s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0556, train accuracy:1.0000.:  97%|█████████▋| 97/100 [02:58<00:06,  2.14s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0556, train accuracy:1.0000.:  98%|█████████▊| 98/100 [02:58<00:03,  1.79s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0562, train accuracy:1.0000.:  98%|█████████▊| 98/100 [02:59<00:03,  1.79s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0562, train accuracy:1.0000.:  99%|█████████▉| 99/100 [02:59<00:01,  1.54s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0559, train accuracy:1.0000.:  99%|█████████▉| 99/100 [03:02<00:01,  1.54s/it]\u001b[A\u001b[A\n",
            "\n",
            "train loss:0.0559, train accuracy:1.0000.: 100%|██████████| 100/100 [03:02<00:00,  1.96s/it]\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nV0qpNkiTu1JKtUH/HwyNSZhFnETIAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEDv_-H7BvM0",
        "colab_type": "text"
      },
      "source": [
        "### 1.4 Implement Unfreezing (1 hr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH5mQBaa-_0b",
        "colab_type": "text"
      },
      "source": [
        "#### Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_YmE1pe-6LF",
        "colab_type": "text"
      },
      "source": [
        "Unfreezing is a technique that can be helpful when fine tuning a CNN for a more difficult task with a large amount of data.\n",
        "\n",
        "The idea is that if we allow the network to tweak the earliest layers immediately, before the last FCL has been trained at all, the earliest layers will forget all of the useful features that they learned in order  to provide features that are helpful for the (untrained) FCL.\n",
        "\n",
        "So, rather than training all of the model weights at once, we learn the last fully connected layer, then train that layer together with the second-to-last layer, gradually adding layers until we reach the first layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMKRI77_-8nc",
        "colab_type": "text"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaUc8BTYC1bz",
        "colab_type": "text"
      },
      "source": [
        "- Modify your model class by setting the `requires_grad` attribute of the ResNet to `False`. (but keep `requires_grad = True` for the last layer). (DONE)\n",
        "- Add a member function to you model class that allows the user to unfreeze weights in the training loop. See [this github gist](https://gist.github.com/jcjohnson/6e41e8512c17eae5da50aebef3378a4c) for reference. (DONE)\n",
        "- Modify your training loop to add logic that calls the `unfreeze` function of the model class (unfreeze one layer every epoch). (DONE)\n",
        "- Call your train function to fine-tune the ResNet on your dataset. (DONE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBT5jgifC7Im",
        "colab_type": "text"
      },
      "source": [
        "#### Call your train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg9ySEO_BNDx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "14c6ab22-2d96-40ba-9c02-834818a8f431"
      },
      "source": [
        "############################\n",
        "# train with unfreezing here (should be a single call to your train function)\n",
        "############################\n",
        "train(start_frozen=True, model_unfreeze=10)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:2.5368, train accuracy:0.1250.:   0%|          | 0/100 [00:03<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:2.5368, train accuracy:0.1250.:   1%|          | 1/100 [00:03<06:12,  3.77s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:2.4287, train accuracy:0.2812.:   1%|          | 1/100 [00:04<06:12,  3.77s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:2.4287, train accuracy:0.2812.:   2%|▏         | 2/100 [00:04<04:45,  2.91s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:2.3334, train accuracy:0.4375.:   2%|▏         | 2/100 [00:05<04:45,  2.91s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:2.3334, train accuracy:0.4375.:   3%|▎         | 3/100 [00:05<03:43,  2.31s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:2.2571, train accuracy:0.6250.:   3%|▎         | 3/100 [00:08<03:43,  2.31s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:2.2571, train accuracy:0.6250.:   4%|▍         | 4/100 [00:08<04:03,  2.54s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:2.1509, train accuracy:0.7727.:   4%|▍         | 4/100 [00:09<04:03,  2.54s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:2.1509, train accuracy:0.7727.:   5%|▌         | 5/100 [00:09<03:07,  1.98s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:2.0386, train accuracy:0.8750.:   5%|▌         | 5/100 [00:11<03:07,  1.98s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:2.0386, train accuracy:0.8750.:   6%|▌         | 6/100 [00:11<03:00,  1.92s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.9431, train accuracy:0.9688.:   6%|▌         | 6/100 [00:14<03:00,  1.92s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.9431, train accuracy:0.9688.:   7%|▋         | 7/100 [00:14<03:31,  2.28s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.8430, train accuracy:1.0000.:   7%|▋         | 7/100 [00:15<03:31,  2.28s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.8430, train accuracy:1.0000.:   8%|▊         | 8/100 [00:15<02:51,  1.87s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.7621, train accuracy:1.0000.:   8%|▊         | 8/100 [00:16<02:51,  1.87s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.7621, train accuracy:1.0000.:   9%|▉         | 9/100 [00:16<02:24,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.6763, train accuracy:1.0000.:   9%|▉         | 9/100 [00:18<02:24,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.6763, train accuracy:1.0000.:  10%|█         | 10/100 [00:18<02:56,  1.96s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.5768, train accuracy:1.0000.:  10%|█         | 10/100 [00:20<02:56,  1.96s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.5768, train accuracy:1.0000.:  11%|█         | 11/100 [00:20<02:49,  1.90s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.4884, train accuracy:1.0000.:  11%|█         | 11/100 [00:21<02:49,  1.90s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.4884, train accuracy:1.0000.:  12%|█▏        | 12/100 [00:21<02:21,  1.61s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.4059, train accuracy:1.0000.:  12%|█▏        | 12/100 [00:24<02:21,  1.61s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.4059, train accuracy:1.0000.:  13%|█▎        | 13/100 [00:24<02:57,  2.04s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.3265, train accuracy:1.0000.:  13%|█▎        | 13/100 [00:25<02:57,  2.04s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.3265, train accuracy:1.0000.:  14%|█▍        | 14/100 [00:25<02:26,  1.71s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.2568, train accuracy:1.0000.:  14%|█▍        | 14/100 [00:26<02:26,  1.71s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.2568, train accuracy:1.0000.:  15%|█▌        | 15/100 [00:26<01:58,  1.40s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.1667, train accuracy:1.0000.:  15%|█▌        | 15/100 [00:30<01:58,  1.40s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.1667, train accuracy:1.0000.:  16%|█▌        | 16/100 [00:30<03:02,  2.17s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.0900, train accuracy:1.0000.:  16%|█▌        | 16/100 [00:31<03:02,  2.17s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.0900, train accuracy:1.0000.:  17%|█▋        | 17/100 [00:31<02:29,  1.80s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.0279, train accuracy:1.0000.:  17%|█▋        | 17/100 [00:32<02:29,  1.80s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:1.0279, train accuracy:1.0000.:  18%|█▊        | 18/100 [00:32<02:05,  1.54s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.9552, train accuracy:1.0000.:  18%|█▊        | 18/100 [00:35<02:05,  1.54s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.9552, train accuracy:1.0000.:  19%|█▉        | 19/100 [00:35<02:43,  2.02s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.8912, train accuracy:1.0000.:  19%|█▉        | 19/100 [00:35<02:43,  2.02s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.8912, train accuracy:1.0000.:  20%|██        | 20/100 [00:35<02:09,  1.61s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.8217, train accuracy:1.0000.:  20%|██        | 20/100 [00:37<02:09,  1.61s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.8217, train accuracy:1.0000.:  21%|██        | 21/100 [00:37<02:11,  1.67s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.7729, train accuracy:1.0000.:  21%|██        | 21/100 [00:40<02:11,  1.67s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.7729, train accuracy:1.0000.:  22%|██▏       | 22/100 [00:40<02:43,  2.10s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.7093, train accuracy:1.0000.:  22%|██▏       | 22/100 [00:41<02:43,  2.10s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.7093, train accuracy:1.0000.:  23%|██▎       | 23/100 [00:41<02:14,  1.75s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.6628, train accuracy:1.0000.:  23%|██▎       | 23/100 [00:42<02:14,  1.75s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.6628, train accuracy:1.0000.:  24%|██▍       | 24/100 [00:42<01:54,  1.51s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.6263, train accuracy:1.0000.:  24%|██▍       | 24/100 [00:45<01:54,  1.51s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.6263, train accuracy:1.0000.:  25%|██▌       | 25/100 [00:45<02:20,  1.88s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.5699, train accuracy:1.0000.:  25%|██▌       | 25/100 [00:47<02:20,  1.88s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.5699, train accuracy:1.0000.:  26%|██▌       | 26/100 [00:47<02:17,  1.86s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.5204, train accuracy:1.0000.:  26%|██▌       | 26/100 [00:48<02:17,  1.86s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.5204, train accuracy:1.0000.:  27%|██▋       | 27/100 [00:48<01:55,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.4966, train accuracy:1.0000.:  27%|██▋       | 27/100 [00:51<01:55,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.4966, train accuracy:1.0000.:  28%|██▊       | 28/100 [00:51<02:27,  2.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.4624, train accuracy:1.0000.:  28%|██▊       | 28/100 [00:52<02:27,  2.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.4624, train accuracy:1.0000.:  29%|██▉       | 29/100 [00:52<02:02,  1.72s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.4341, train accuracy:1.0000.:  29%|██▉       | 29/100 [00:52<02:02,  1.72s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.4341, train accuracy:1.0000.:  30%|███       | 30/100 [00:52<01:38,  1.41s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.3935, train accuracy:1.0000.:  30%|███       | 30/100 [00:56<01:38,  1.41s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.3935, train accuracy:1.0000.:  31%|███       | 31/100 [00:56<02:31,  2.20s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.3658, train accuracy:1.0000.:  31%|███       | 31/100 [00:57<02:31,  2.20s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.3658, train accuracy:1.0000.:  32%|███▏      | 32/100 [00:57<02:04,  1.83s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.3371, train accuracy:1.0000.:  32%|███▏      | 32/100 [00:58<02:04,  1.83s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.3371, train accuracy:1.0000.:  33%|███▎      | 33/100 [00:58<01:44,  1.57s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.3148, train accuracy:1.0000.:  33%|███▎      | 33/100 [01:02<01:44,  1.57s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.3148, train accuracy:1.0000.:  34%|███▍      | 34/100 [01:02<02:15,  2.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.2953, train accuracy:1.0000.:  34%|███▍      | 34/100 [01:02<02:15,  2.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.2953, train accuracy:1.0000.:  35%|███▌      | 35/100 [01:02<01:46,  1.64s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.2755, train accuracy:1.0000.:  35%|███▌      | 35/100 [01:04<01:46,  1.64s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.2755, train accuracy:1.0000.:  36%|███▌      | 36/100 [01:04<01:48,  1.69s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.2497, train accuracy:1.0000.:  36%|███▌      | 36/100 [01:07<01:48,  1.69s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.2497, train accuracy:1.0000.:  37%|███▋      | 37/100 [01:07<02:14,  2.14s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.2449, train accuracy:1.0000.:  37%|███▋      | 37/100 [01:08<02:14,  2.14s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.2449, train accuracy:1.0000.:  38%|███▊      | 38/100 [01:08<01:51,  1.79s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.2270, train accuracy:1.0000.:  38%|███▊      | 38/100 [01:09<01:51,  1.79s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.2270, train accuracy:1.0000.:  39%|███▉      | 39/100 [01:09<01:34,  1.54s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.2133, train accuracy:1.0000.:  39%|███▉      | 39/100 [01:12<01:34,  1.54s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.2133, train accuracy:1.0000.:  40%|████      | 40/100 [01:12<01:57,  1.96s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.2016, train accuracy:1.0000.:  40%|████      | 40/100 [01:14<01:57,  1.96s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.2016, train accuracy:1.0000.:  41%|████      | 41/100 [01:14<01:53,  1.92s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1903, train accuracy:1.0000.:  41%|████      | 41/100 [01:15<01:53,  1.92s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1903, train accuracy:1.0000.:  42%|████▏     | 42/100 [01:15<01:34,  1.63s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1764, train accuracy:1.0000.:  42%|████▏     | 42/100 [01:18<01:34,  1.63s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1764, train accuracy:1.0000.:  43%|████▎     | 43/100 [01:18<02:00,  2.11s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1706, train accuracy:1.0000.:  43%|████▎     | 43/100 [01:19<02:00,  2.11s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1706, train accuracy:1.0000.:  44%|████▍     | 44/100 [01:19<01:38,  1.76s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1681, train accuracy:1.0000.:  44%|████▍     | 44/100 [01:20<01:38,  1.76s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1681, train accuracy:1.0000.:  45%|████▌     | 45/100 [01:20<01:19,  1.44s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1536, train accuracy:1.0000.:  45%|████▌     | 45/100 [01:24<01:19,  1.44s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1536, train accuracy:1.0000.:  46%|████▌     | 46/100 [01:24<01:58,  2.20s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1476, train accuracy:1.0000.:  46%|████▌     | 46/100 [01:25<01:58,  2.20s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1476, train accuracy:1.0000.:  47%|████▋     | 47/100 [01:25<01:36,  1.83s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1460, train accuracy:1.0000.:  47%|████▋     | 47/100 [01:26<01:36,  1.83s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1460, train accuracy:1.0000.:  48%|████▊     | 48/100 [01:26<01:21,  1.56s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1387, train accuracy:1.0000.:  48%|████▊     | 48/100 [01:29<01:21,  1.56s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1387, train accuracy:1.0000.:  49%|████▉     | 49/100 [01:29<01:44,  2.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1265, train accuracy:1.0000.:  49%|████▉     | 49/100 [01:30<01:44,  2.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1265, train accuracy:1.0000.:  50%|█████     | 50/100 [01:30<01:21,  1.64s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1261, train accuracy:1.0000.:  50%|█████     | 50/100 [01:31<01:21,  1.64s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1261, train accuracy:1.0000.:  51%|█████     | 51/100 [01:31<01:22,  1.69s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1266, train accuracy:1.0000.:  51%|█████     | 51/100 [01:34<01:22,  1.69s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1266, train accuracy:1.0000.:  52%|█████▏    | 52/100 [01:35<01:42,  2.13s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1190, train accuracy:1.0000.:  52%|█████▏    | 52/100 [01:35<01:42,  2.13s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1190, train accuracy:1.0000.:  53%|█████▎    | 53/100 [01:35<01:23,  1.78s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1140, train accuracy:1.0000.:  53%|█████▎    | 53/100 [01:36<01:23,  1.78s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1140, train accuracy:1.0000.:  54%|█████▍    | 54/100 [01:36<01:10,  1.52s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1098, train accuracy:1.0000.:  54%|█████▍    | 54/100 [01:39<01:10,  1.52s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1098, train accuracy:1.0000.:  55%|█████▌    | 55/100 [01:39<01:26,  1.92s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1035, train accuracy:1.0000.:  55%|█████▌    | 55/100 [01:41<01:26,  1.92s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1035, train accuracy:1.0000.:  56%|█████▌    | 56/100 [01:41<01:22,  1.88s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1052, train accuracy:1.0000.:  56%|█████▌    | 56/100 [01:42<01:22,  1.88s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1052, train accuracy:1.0000.:  57%|█████▋    | 57/100 [01:42<01:08,  1.60s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1047, train accuracy:1.0000.:  57%|█████▋    | 57/100 [01:45<01:08,  1.60s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1047, train accuracy:1.0000.:  58%|█████▊    | 58/100 [01:45<01:26,  2.07s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0981, train accuracy:1.0000.:  58%|█████▊    | 58/100 [01:46<01:26,  2.07s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0981, train accuracy:1.0000.:  59%|█████▉    | 59/100 [01:46<01:11,  1.73s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1014, train accuracy:1.0000.:  59%|█████▉    | 59/100 [01:47<01:11,  1.73s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.1014, train accuracy:1.0000.:  60%|██████    | 60/100 [01:47<00:56,  1.42s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0940, train accuracy:1.0000.:  60%|██████    | 60/100 [01:51<00:56,  1.42s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0940, train accuracy:1.0000.:  61%|██████    | 61/100 [01:51<01:25,  2.19s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0918, train accuracy:1.0000.:  61%|██████    | 61/100 [01:52<01:25,  2.19s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0918, train accuracy:1.0000.:  62%|██████▏   | 62/100 [01:52<01:09,  1.82s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0930, train accuracy:1.0000.:  62%|██████▏   | 62/100 [01:53<01:09,  1.82s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0930, train accuracy:1.0000.:  63%|██████▎   | 63/100 [01:53<00:57,  1.55s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0894, train accuracy:1.0000.:  63%|██████▎   | 63/100 [01:56<00:57,  1.55s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0894, train accuracy:1.0000.:  64%|██████▍   | 64/100 [01:56<01:12,  2.03s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0866, train accuracy:1.0000.:  64%|██████▍   | 64/100 [01:56<01:12,  2.03s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0866, train accuracy:1.0000.:  65%|██████▌   | 65/100 [01:56<00:56,  1.62s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0829, train accuracy:1.0000.:  65%|██████▌   | 65/100 [01:58<00:56,  1.62s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0829, train accuracy:1.0000.:  66%|██████▌   | 66/100 [01:58<00:56,  1.68s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0846, train accuracy:1.0000.:  66%|██████▌   | 66/100 [02:01<00:56,  1.68s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0846, train accuracy:1.0000.:  67%|██████▋   | 67/100 [02:01<01:10,  2.13s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0821, train accuracy:1.0000.:  67%|██████▋   | 67/100 [02:02<01:10,  2.13s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0821, train accuracy:1.0000.:  68%|██████▊   | 68/100 [02:02<00:56,  1.77s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0828, train accuracy:1.0000.:  68%|██████▊   | 68/100 [02:03<00:56,  1.77s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0828, train accuracy:1.0000.:  69%|██████▉   | 69/100 [02:03<00:47,  1.52s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0831, train accuracy:1.0000.:  69%|██████▉   | 69/100 [02:06<00:47,  1.52s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0831, train accuracy:1.0000.:  70%|███████   | 70/100 [02:06<00:58,  1.94s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0812, train accuracy:1.0000.:  70%|███████   | 70/100 [02:08<00:58,  1.94s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0812, train accuracy:1.0000.:  71%|███████   | 71/100 [02:08<00:54,  1.90s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0779, train accuracy:1.0000.:  71%|███████   | 71/100 [02:09<00:54,  1.90s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0779, train accuracy:1.0000.:  72%|███████▏  | 72/100 [02:09<00:45,  1.61s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0753, train accuracy:1.0000.:  72%|███████▏  | 72/100 [02:12<00:45,  1.61s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0753, train accuracy:1.0000.:  73%|███████▎  | 73/100 [02:12<00:55,  2.07s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0720, train accuracy:1.0000.:  73%|███████▎  | 73/100 [02:13<00:55,  2.07s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0720, train accuracy:1.0000.:  74%|███████▍  | 74/100 [02:13<00:45,  1.74s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0748, train accuracy:1.0000.:  74%|███████▍  | 74/100 [02:14<00:45,  1.74s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0748, train accuracy:1.0000.:  75%|███████▌  | 75/100 [02:14<00:35,  1.42s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0739, train accuracy:1.0000.:  75%|███████▌  | 75/100 [02:18<00:35,  1.42s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0739, train accuracy:1.0000.:  76%|███████▌  | 76/100 [02:18<00:53,  2.21s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0721, train accuracy:1.0000.:  76%|███████▌  | 76/100 [02:19<00:53,  2.21s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0721, train accuracy:1.0000.:  77%|███████▋  | 77/100 [02:19<00:42,  1.84s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0676, train accuracy:1.0000.:  77%|███████▋  | 77/100 [02:20<00:42,  1.84s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0676, train accuracy:1.0000.:  78%|███████▊  | 78/100 [02:20<00:34,  1.57s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0706, train accuracy:1.0000.:  78%|███████▊  | 78/100 [02:23<00:34,  1.57s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0706, train accuracy:1.0000.:  79%|███████▉  | 79/100 [02:23<00:43,  2.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0865, train accuracy:1.0000.:  79%|███████▉  | 79/100 [02:24<00:43,  2.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0865, train accuracy:1.0000.:  80%|████████  | 80/100 [02:24<00:32,  1.64s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0683, train accuracy:1.0000.:  80%|████████  | 80/100 [02:25<00:32,  1.64s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0683, train accuracy:1.0000.:  81%|████████  | 81/100 [02:25<00:32,  1.70s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0663, train accuracy:1.0000.:  81%|████████  | 81/100 [02:29<00:32,  1.70s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0663, train accuracy:1.0000.:  82%|████████▏ | 82/100 [02:29<00:38,  2.13s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0715, train accuracy:1.0000.:  82%|████████▏ | 82/100 [02:29<00:38,  2.13s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0715, train accuracy:1.0000.:  83%|████████▎ | 83/100 [02:29<00:30,  1.77s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0633, train accuracy:1.0000.:  83%|████████▎ | 83/100 [02:30<00:30,  1.77s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0633, train accuracy:1.0000.:  84%|████████▍ | 84/100 [02:30<00:24,  1.53s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0605, train accuracy:1.0000.:  84%|████████▍ | 84/100 [02:33<00:24,  1.53s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0605, train accuracy:1.0000.:  85%|████████▌ | 85/100 [02:33<00:28,  1.92s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0632, train accuracy:1.0000.:  85%|████████▌ | 85/100 [02:35<00:28,  1.92s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0632, train accuracy:1.0000.:  86%|████████▌ | 86/100 [02:35<00:26,  1.89s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0603, train accuracy:1.0000.:  86%|████████▌ | 86/100 [02:36<00:26,  1.89s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0603, train accuracy:1.0000.:  87%|████████▋ | 87/100 [02:36<00:20,  1.61s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0637, train accuracy:1.0000.:  87%|████████▋ | 87/100 [02:39<00:20,  1.61s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0637, train accuracy:1.0000.:  88%|████████▊ | 88/100 [02:39<00:24,  2.07s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0604, train accuracy:1.0000.:  88%|████████▊ | 88/100 [02:40<00:24,  2.07s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0604, train accuracy:1.0000.:  89%|████████▉ | 89/100 [02:40<00:19,  1.73s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0621, train accuracy:1.0000.:  89%|████████▉ | 89/100 [02:41<00:19,  1.73s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0621, train accuracy:1.0000.:  90%|█████████ | 90/100 [02:41<00:14,  1.42s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0608, train accuracy:1.0000.:  90%|█████████ | 90/100 [02:45<00:14,  1.42s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0608, train accuracy:1.0000.:  91%|█████████ | 91/100 [02:45<00:19,  2.22s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0568, train accuracy:1.0000.:  91%|█████████ | 91/100 [02:46<00:19,  2.22s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0568, train accuracy:1.0000.:  92%|█████████▏| 92/100 [02:46<00:14,  1.84s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0581, train accuracy:1.0000.:  92%|█████████▏| 92/100 [02:47<00:14,  1.84s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0581, train accuracy:1.0000.:  93%|█████████▎| 93/100 [02:47<00:10,  1.57s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0556, train accuracy:1.0000.:  93%|█████████▎| 93/100 [02:50<00:10,  1.57s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0556, train accuracy:1.0000.:  94%|█████████▍| 94/100 [02:50<00:12,  2.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0520, train accuracy:1.0000.:  94%|█████████▍| 94/100 [02:51<00:12,  2.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0520, train accuracy:1.0000.:  95%|█████████▌| 95/100 [02:51<00:08,  1.64s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0552, train accuracy:1.0000.:  95%|█████████▌| 95/100 [02:52<00:08,  1.64s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0552, train accuracy:1.0000.:  96%|█████████▌| 96/100 [02:52<00:06,  1.70s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0525, train accuracy:1.0000.:  96%|█████████▌| 96/100 [02:56<00:06,  1.70s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0525, train accuracy:1.0000.:  97%|█████████▋| 97/100 [02:56<00:06,  2.13s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0524, train accuracy:1.0000.:  97%|█████████▋| 97/100 [02:57<00:06,  2.13s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0524, train accuracy:1.0000.:  98%|█████████▊| 98/100 [02:57<00:03,  1.78s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0508, train accuracy:1.0000.:  98%|█████████▊| 98/100 [02:58<00:03,  1.78s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0508, train accuracy:1.0000.:  99%|█████████▉| 99/100 [02:58<00:01,  1.52s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0528, train accuracy:1.0000.:  99%|█████████▉| 99/100 [03:00<00:01,  1.52s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "train loss:0.0528, train accuracy:1.0000.: 100%|██████████| 100/100 [03:00<00:00,  1.93s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nh12URUVY/QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZItH2lX7k4Yt",
        "colab_type": "text"
      },
      "source": [
        "You may not see any improvement for your classification task, but unfreezing can help convergence for more difficult image classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAXHAUf3EEiE",
        "colab_type": "text"
      },
      "source": [
        "##2 Fine-tune a language model - (15 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu9usOxtjFHL",
        "colab_type": "text"
      },
      "source": [
        "In this section you will use the gpt-2-simple package [here](https://github.com/minimaxir/gpt-2-simple) to fine-tune the GPT-2 language model on a domain of your choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K7F19SPQo6U",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Generate text from an the pretrained GPT-2 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YLXvK51RnuL",
        "colab_type": "text"
      },
      "source": [
        "#### Run this code to generate text from a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDNOb_H5IRvH",
        "colab_type": "code",
        "outputId": "c74ba431-044f-4e05-b2a1-fe82e728b2db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "source": [
        "!pip install gpt-2-simple\n",
        "\n",
        "# the transformers package is built on top of Tensorflow, and the default TF version \n",
        "# for Colab will soon switch to 2.x. We remedy this with the following magic method\n",
        "%tensorflow_version 1.x \n",
        "\n",
        "import gpt_2_simple as gpt2\n",
        "import os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gpt-2-simple\n",
            "  Downloading https://files.pythonhosted.org/packages/3d/ee/bb41a7cc57e0626a0dfeea0f8fedc21e255103c888f5cab5e1f7fb00380b/gpt_2_simple-0.7.tar.gz\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/db/4b29a0adec5881542cd81cb5d1929b5c0787003c5740b3c921e627d9c2e5/regex-2019.12.9.tar.gz (669kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (1.17.4)\n",
            "Collecting toposort\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (3.0.4)\n",
            "Building wheels for collected packages: gpt-2-simple, regex\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.7-cp36-none-any.whl size=23557 sha256=7f20669376f7d6b9ed83bbf6af6f1a5517ace504af24bf002d39f8acae37448f\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/7f/89/1253cc7ae7fd1cdf130fa146ab17314fd2a5a6d48ccf21dec5\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.12.9-cp36-cp36m-linux_x86_64.whl size=609177 sha256=9f9452e278eba2c2b7735415d0c3cde513be6630919339cd3e044c85043f365d\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/fb/b3/a89169557229468c49ca64f6839418f22461f6ee0a74f342b1\n",
            "Successfully built gpt-2-simple regex\n",
            "Installing collected packages: regex, toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.7 regex-2019.12.9 toposort-1.5\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aRJ-c9uRMOa",
        "colab_type": "code",
        "outputId": "f79c7c3f-f014-4272-ab1d-0c4e598864ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# This line is necessary to be able to run a new tf session\n",
        "tf.reset_default_graph()\n",
        "# The medium-sized model. IF you run out of memory, try \"124M\" instead\n",
        "model_name = \"124M\"\n",
        "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "\tprint(f\"Downloading {model_name} model...\")\n",
        "\tgpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, model_name=model_name)\n",
        "gpt2.generate(sess, model_name=model_name)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 395Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 108Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 874Mit/s]                                                    "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading 124M model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:04, 113Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 316Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 161Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 166Mit/s]                                                       \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading pretrained model models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
            "Luis Avila, the lead director of Vivid Academics, a research group investigating the relationship between smoking and cancers, said it is \"impossible\" to predict the effect of smoking on lung cancer.\n",
            "\n",
            "He said it is possible that smoking increases the risk of lung cancer in people who smoke but not who do not.\n",
            "\n",
            "\"There is no reason to believe that the effect of smoking on lung cancer is linear,\" he added.\n",
            "\n",
            "Avila said it might be possible for a person to quit smoking at least once a month, or it might be necessary to avoid smoking at all for several years.\n",
            "\n",
            "\"It's complicated to say which way it is going to go in a way that is not harming anyone,\" he said.\n",
            "\n",
            "The study was published in the Journal of the American Medical Association.\n",
            "\n",
            "Topics: cancer, human-interest, lifestyle-and-behaviour, health, health-policy, health-policy-and-nutrition, health-policy-and-nutrition, health, australia\n",
            "\n",
            "First posted<|endoftext|>Claim: A woman in China claims this man was \"too stupid\" to confirm the existence of the bird.\n",
            "\n",
            "MIXTURE\n",
            "\n",
            "WHAT'S TRUE\n",
            "\n",
            "The man in question claims the woman who claimed he was too dumb to confirm the existence of the bird is \"too stupid\".\n",
            "\n",
            "WHAT'S FALSE\n",
            "\n",
            "The man in question claims the woman who claimed she was too stupid to confirm the existence of the bird is \"too stupid\" (the latter is true).\n",
            "\n",
            "ORIGIN\n",
            "\n",
            "Example: [Collected via e-mail, January 2017]\n",
            "\n",
            "\n",
            "A woman in China claimed that she had a bird \"that was too stupid for her to confirm\" to prove her existence. She was also \"too stupid\" to confirm this bird to prove her existence, since the bird's name was \"Dhenyang Feng\".\n",
            "\n",
            "\n",
            "Origins\n",
            "\n",
            "On 20 January 2017, a woman in China claimed that this man was \"too stupid\" to confirm the existence of the bird. She is an employee at a major auto company in Beijing, who claims the man in question was \"too stupid\" to confirm the existence of the bird. She also claims she \"was too stupid\" to confirm that he was \"too stupid\" to confirm the existence of the bird.\n",
            "\n",
            "This is about the same time as a woman in the US who claimed she was too stupid to confirm the existence of the bird, but was too dumb to confirm a bird's existence.\n",
            "\n",
            "This could be because she was too dumb to confirm the existence of the bird in question, or because she was too dumb to confirm that the bird was her own invention.\n",
            "\n",
            "It is not clear who the man in question is, or how this could be the case.\n",
            "\n",
            "A second claim from the same woman in China is that this man, \"too stupid\" to confirm the existence of the bird, is \"too stupid\".\n",
            "\n",
            "\n",
            "More details\n",
            "\n",
            "The claim is very common in China, and has become a popular post-facto trope as a result.\n",
            "\n",
            "\n",
            "This claim was made by the Chinese Social Media Users' Association (CSA), a group of groups with over 40,000 followers, and was first published in Computerworld's May 2017 issue.\n",
            "\n",
            "\n",
            "It is widely believed this claim is bogus, based on a number of well-known Chinese hoaxes, and not based on any real or real bird sightings.\n",
            "\n",
            "It is also known to have been made by an American man, who was asked to confirm that his wife had a bird, but did not. The claim is not based on any genuine bird sightings or any sightings of a bird from China, but on a fake bird sighting.\n",
            "\n",
            "The CSA claimed the claim was made by the Chinese Social Media Users' Association (CSA), a group of groups with over 40,000 followers, and was first published in Computerworld's May 2017 issue. It is widely believed this claim is bogus, based on a number of well-known Chinese hoaxes, and not based on any real or real bird sightings. The CSA claimed the claim was made by the Chinese Social Media Users' Association (CSA), a group of groups with over 40,000 followers, and was first published in Computerworld's May 2017 issue.The CSA claimed the claim was made by the Chinese Social Media Users' Association (CSA), a group of groups with over 40,000 followers, and was first published in Computerworld's May 2017 issue.\n",
            "\n",
            "This is a very common claim, and has been spreading because of the popularity of the CSA. It has been widely accepted that the claim is true, with many people believing it to be true.\n",
            "\n",
            "A second claim from the same woman in China is that this man was \"too stupid\" to confirm the existence of the bird.\n",
            "\n",
            "\n",
            "This is not the first time the CSA has used the claim. They first\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHmjSVf_FNHv",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Download a text dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPXJkNubFyY6",
        "colab_type": "text"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWkuRjbcFzwb",
        "colab_type": "text"
      },
      "source": [
        "- Use the provided functions to download your own text dataset (DONE)\n",
        "- [Project Gutenberg](https://www.gutenberg.org/) is a nice starting point for raw text corpora (DONE - Harry Potter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD45m3IwF9hh",
        "colab_type": "text"
      },
      "source": [
        "#### Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESltl2QM5nxw",
        "colab_type": "code",
        "outputId": "6380e35f-e57f-49e6-975b-97a439e34f93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "from torchvision import datasets\n",
        "\n",
        "def extract_zip(zip_path, remove_finished=True):\n",
        "    print('Extracting {}'.format(zip_path))\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(zip_path.replace('.zip', ''))\n",
        "    if remove_finished:\n",
        "        os.remove(zip_path)\n",
        "\n",
        "def download_dataset(url, root='../data'):\n",
        "    if not os.path.exists(os.path.join(root, 'text')):\n",
        "        os.makedirs(os.path.join(root))\n",
        "        datasets.utils.download_url(url, root, 'text.zip', None)\n",
        "        extract_zip(os.path.join(root, 'text.zip'))\n",
        "    return os.path.join(root, 'text')\n",
        "\n",
        "##########################################\n",
        "# Set the url for your dataset here,\n",
        "# move the dataset to the desired location\n",
        "##########################################\n",
        "url = 'https://www.gutenberg.org/files/30/30.zip'\n",
        "download_dataset(url)\n",
        "!mv /data/text/30.txt /data/text/bible.txt\n",
        "!ls ../data/text"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot stat '/data/text/30.txt': No such file or directory\n",
            "bible.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs18iIiSVFhz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6207e19e-430f-472d-f1e1-74008417cd20"
      },
      "source": [
        "# Download the Harry Potter Book 1 Dataset\n",
        "datasets.utils.download_url('https://www.linguistik.uzh.ch/dam/jcr:169bff5c-ac13-457b-9acb-4fe7f1ad5cb0/Harry%20Potter%20and%20the%20Sorcerer.txt', '../data/text', filename='hp.txt')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.linguistik.uzh.ch/dam/jcr:169bff5c-ac13-457b-9acb-4fe7f1ad5cb0/Harry%20Potter%20and%20the%20Sorcerer.txt to ../data/text/hp.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "450560it [00:01, 428340.60it/s]                            \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EHKLfdYVT4W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25f86783-b3fa-4a17-a4f4-c4c6fe06d112"
      },
      "source": [
        "!ls ../data/text"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bible.txt  hp.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usQE-rSPZq_X",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Fine-tune GPT-2 on your own dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoA0tZZCa_1k",
        "colab_type": "text"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoU6ML1mbgjP",
        "colab_type": "text"
      },
      "source": [
        "- Swap out the dataset parameter with the path to your dataset (DONE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pa5vFJ5EUjv",
        "colab_type": "text"
      },
      "source": [
        "#### Train on your dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuQ5snl4LuS0",
        "colab_type": "code",
        "outputId": "3692c7f2-e5fd-4d8e-c60f-7ce3288843c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# This line is necessary to be able to run a new tf session if one has already been run\n",
        "tf.reset_default_graph()\n",
        "# Start a session\n",
        "sess = gpt2.start_tf_sess()\n",
        "# Fine tune `model_name` on `data`\n",
        "###################################\n",
        "# Swap out the `dataset` parameter with the path to your text dataset\n",
        "###################################\n",
        "gpt2.finetune(sess,\n",
        "              dataset='../data/text/hp.txt',\n",
        "              model_name=model_name,\n",
        "              restore_from='latest',\n",
        "              steps=500)   # steps is max number of training steps\n",
        "\n",
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/run1/model-500\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 113781 tokens\n",
            "Training...\n",
            "======== SAMPLE 1 ========\n",
            " mirror it, so that all might dwell in it.\n",
            "\n",
            "44:003:005 And David opened the gate of the place of David's tent as the\n",
            "           gate of a court of the chief priests, and there he laid\n",
            "           the altar of the altar to the LORD his God, and to the house of\n",
            "           his father at Jerusalem. And there came three\n",
            "           people out together: and all the host rested, because there\n",
            "           was a man in the mount of Olives in the field of the\n",
            "           camp that was there:\n",
            "\n",
            "44:003:006 And David said, Thou hast opened mine heart also unto the\n",
            "           LORD my God.\n",
            "\n",
            "44:003:007 And Solomon had made a covenant with the LORD, after that he\n",
            "           had offered the burnt offerings, to offer the burnt\n",
            "           offerings among them.\n",
            "\n",
            "44:003:008 And David had two priests in his tent, and when he began to\n",
            "           prepare for supper a vessel of the LORD's sacrifice, and another\n",
            "           offering,\n",
            "\n",
            "44:003:009 And one offering of the burnt offering, and a burnt\n",
            "           bullock, and two vessels of oil, and two vessels of oil,\n",
            "           both of which were burnt, and of two vessels of oil, of two\n",
            "           vessels of pure gold, and of pure silver, and two vessels of\n",
            "           silver,\n",
            "\n",
            "44:003:010 And a golden vessel, and four vessels of gold, and of four\n",
            "           colors, of four parts, by the two kidneys, and\n",
            "           by the four flanks, and with a pair of blue eyes to see\n",
            "           the place where they had set them.\n",
            "\n",
            "44:003:011 And David set three sons in their tents that were about to go\n",
            "           into the land of Gad.\n",
            "\n",
            "44:003:012 And David said unto Solomon, Come, set yourselves before the\n",
            "           LORD one to another, and I will be with you:\n",
            "\n",
            "44:003:013 And I will make your house his, and shall give you two sons:\n",
            "           of the priests, and the Levites, and the priests of your\n",
            "           fathers' houses, and your Levites' houses; which shall be\n",
            "           holy unto the LORD: and I will bring them out out of\n",
            "           Shittim from Gad, and they shall dwell in their host; and\n",
            "           their kings and their priests shall be with them.\n",
            "\n",
            "44:003:014 And after that shall I prepare an altar of the LORD thy God, which\n",
            "           shall be of ten cubits by ten cubits.\n",
            "\n",
            "44:003:015 And they shall drink of the drink of the altar, and shall be\n",
            "           drunken before it, they and their fathers.\n",
            "\n",
            "44:003:016 And they shall make an altar of the LORD thy God, the first\n",
            "           of them that hate it, and the last of them that curse it: all\n",
            "           they that hate it shall be cut off from among them, and they\n",
            "           shall eat of the burnt offerings without a covering, without the\n",
            "           vessels that contain it in the holy place;\n",
            "\n",
            "44:003:017 But they shall be drunk before it, and there shall be no\n",
            "           covering from among them which bring out their sods, as they\n",
            "           should burn their blood on it like a fire.\n",
            "\n",
            "44:003:018 And they shall eat their bread of rams, and their fish, and their\n",
            "\n",
            "\n",
            "[501 | 20.88] loss=3.88 avg=3.88\n",
            "[502 | 22.15] loss=3.32 avg=3.60\n",
            "[503 | 23.42] loss=3.37 avg=3.52\n",
            "[504 | 24.68] loss=3.29 avg=3.47\n",
            "[505 | 25.94] loss=3.31 avg=3.43\n",
            "[506 | 27.21] loss=3.22 avg=3.40\n",
            "[507 | 28.48] loss=3.02 avg=3.34\n",
            "[508 | 29.74] loss=3.21 avg=3.33\n",
            "[509 | 30.99] loss=3.11 avg=3.30\n",
            "[510 | 32.25] loss=3.09 avg=3.28\n",
            "[511 | 33.52] loss=3.09 avg=3.26\n",
            "[512 | 34.80] loss=2.94 avg=3.23\n",
            "[513 | 36.06] loss=2.80 avg=3.20\n",
            "[514 | 37.33] loss=2.97 avg=3.18\n",
            "[515 | 38.60] loss=2.88 avg=3.16\n",
            "[516 | 39.86] loss=2.91 avg=3.14\n",
            "[517 | 41.13] loss=3.10 avg=3.14\n",
            "[518 | 42.38] loss=2.92 avg=3.13\n",
            "[519 | 43.64] loss=2.91 avg=3.11\n",
            "[520 | 44.91] loss=2.92 avg=3.10\n",
            "[521 | 46.18] loss=2.88 avg=3.09\n",
            "[522 | 47.45] loss=2.73 avg=3.07\n",
            "[523 | 48.72] loss=2.89 avg=3.06\n",
            "[524 | 49.98] loss=2.93 avg=3.06\n",
            "[525 | 51.24] loss=2.84 avg=3.05\n",
            "[526 | 52.51] loss=2.89 avg=3.04\n",
            "[527 | 53.76] loss=2.87 avg=3.03\n",
            "[528 | 55.02] loss=2.66 avg=3.02\n",
            "[529 | 56.28] loss=2.73 avg=3.01\n",
            "[530 | 57.56] loss=2.67 avg=2.99\n",
            "[531 | 58.81] loss=2.73 avg=2.98\n",
            "[532 | 60.07] loss=2.61 avg=2.97\n",
            "[533 | 61.32] loss=2.79 avg=2.96\n",
            "[534 | 62.58] loss=2.61 avg=2.95\n",
            "[535 | 63.84] loss=2.70 avg=2.94\n",
            "[536 | 65.12] loss=2.93 avg=2.94\n",
            "[537 | 66.38] loss=2.67 avg=2.93\n",
            "[538 | 67.65] loss=2.70 avg=2.93\n",
            "[539 | 68.91] loss=2.74 avg=2.92\n",
            "[540 | 70.17] loss=2.71 avg=2.91\n",
            "[541 | 71.44] loss=2.48 avg=2.90\n",
            "[542 | 72.70] loss=2.70 avg=2.90\n",
            "[543 | 73.95] loss=2.73 avg=2.89\n",
            "[544 | 75.21] loss=2.54 avg=2.88\n",
            "[545 | 76.46] loss=2.71 avg=2.88\n",
            "[546 | 77.72] loss=2.56 avg=2.87\n",
            "[547 | 78.97] loss=2.66 avg=2.86\n",
            "[548 | 80.23] loss=2.58 avg=2.86\n",
            "[549 | 81.50] loss=2.43 avg=2.84\n",
            "[550 | 82.77] loss=2.73 avg=2.84\n",
            "[551 | 84.02] loss=2.48 avg=2.83\n",
            "[552 | 85.28] loss=2.48 avg=2.82\n",
            "[553 | 86.54] loss=2.66 avg=2.82\n",
            "[554 | 87.81] loss=2.52 avg=2.81\n",
            "[555 | 89.09] loss=2.43 avg=2.80\n",
            "[556 | 90.35] loss=2.39 avg=2.79\n",
            "[557 | 91.61] loss=2.38 avg=2.78\n",
            "[558 | 92.86] loss=2.37 avg=2.78\n",
            "[559 | 94.12] loss=2.62 avg=2.77\n",
            "[560 | 95.39] loss=2.67 avg=2.77\n",
            "[561 | 96.64] loss=2.55 avg=2.76\n",
            "[562 | 97.90] loss=2.53 avg=2.76\n",
            "[563 | 99.15] loss=2.54 avg=2.75\n",
            "[564 | 100.41] loss=2.54 avg=2.75\n",
            "[565 | 101.67] loss=2.46 avg=2.74\n",
            "[566 | 102.92] loss=2.38 avg=2.74\n",
            "[567 | 104.19] loss=2.27 avg=2.73\n",
            "[568 | 105.45] loss=2.35 avg=2.72\n",
            "[569 | 106.72] loss=2.49 avg=2.71\n",
            "[570 | 107.99] loss=2.63 avg=2.71\n",
            "[571 | 109.25] loss=2.66 avg=2.71\n",
            "[572 | 110.50] loss=2.24 avg=2.70\n",
            "[573 | 111.76] loss=2.21 avg=2.69\n",
            "[574 | 113.03] loss=2.52 avg=2.69\n",
            "[575 | 114.29] loss=2.44 avg=2.69\n",
            "[576 | 115.55] loss=2.21 avg=2.68\n",
            "[577 | 116.80] loss=2.47 avg=2.67\n",
            "[578 | 118.07] loss=2.15 avg=2.66\n",
            "[579 | 119.32] loss=2.42 avg=2.66\n",
            "[580 | 120.59] loss=2.40 avg=2.65\n",
            "[581 | 121.84] loss=2.15 avg=2.65\n",
            "[582 | 123.11] loss=2.46 avg=2.64\n",
            "[583 | 124.36] loss=2.22 avg=2.63\n",
            "[584 | 125.62] loss=2.23 avg=2.63\n",
            "[585 | 126.88] loss=2.25 avg=2.62\n",
            "[586 | 128.13] loss=2.30 avg=2.62\n",
            "[587 | 129.39] loss=2.13 avg=2.61\n",
            "[588 | 130.65] loss=2.16 avg=2.60\n",
            "[589 | 131.91] loss=2.14 avg=2.59\n",
            "[590 | 133.16] loss=2.39 avg=2.59\n",
            "[591 | 134.42] loss=2.09 avg=2.58\n",
            "[592 | 135.68] loss=2.18 avg=2.57\n",
            "[593 | 136.94] loss=2.12 avg=2.57\n",
            "[594 | 138.19] loss=2.16 avg=2.56\n",
            "[595 | 139.45] loss=2.03 avg=2.55\n",
            "[596 | 140.72] loss=2.26 avg=2.55\n",
            "[597 | 141.97] loss=2.17 avg=2.54\n",
            "[598 | 143.24] loss=2.11 avg=2.53\n",
            "[599 | 144.49] loss=2.24 avg=2.53\n",
            "[600 | 145.76] loss=2.11 avg=2.52\n",
            "======== SAMPLE 1 ========\n",
            " and the twins were in the school.\" \n",
            "\n",
            "Harry nodded. \n",
            "\n",
            "\"It is not your business what you choose and neither should your enemies,\" he said. \"What would be your name if you had a boy like us? They will call you 'Harry the Wizard of Longbottom'.\" \n",
            "\n",
            "\"I suppose I will -\" \n",
            "\n",
            "\"You can always ask Ron, he's a nice boy.\" \n",
            "\n",
            "\"Right,\" said Ron. \"He's in his first year in the library. I'm teaching Harry's favorite book, \"The Sorcerer's Stone, \"A History of Magic and the Illusionary. \n",
            "\n",
            "Harry nodded in understanding. \n",
            "\n",
            "\"I thought you said you'd try that on him --\" he looked at Ron. \n",
            "\n",
            "\"I -- no -- I know I've been trying on him all summer and I'm still very sleepy,\" \n",
            "\n",
            "said Harry. \n",
            "\n",
            "\"Well, I thought he would sleep at night,\" said Ron, sounding more than pleased with the attention he was getting, \"and when he wakes up, I'll tell him what happened to me, because I'm his brother. I wouldn't even know who to trust -- he's been here the past few days. You needn't worry -- I wouldn't ask you this much.\" \n",
            "\n",
            "Harry's mind had wandered to the second subject. He tried not to wake the twins at all, but he couldn't. \n",
            "\n",
            "When they opened their eyes, it was obvious that Harry was smiling at Ron, his heart swimming in his fingers. \n",
            "\n",
            "\"Oh my goodness, are you hungry, Harry?\" said Ron. \"I've just seen a Crabbe, and he's hungry. \n",
            "\n",
            "I'm liking the smell of the food that's inside me. See?\" \n",
            "\n",
            "\"Yes,\" said Harry eagerly. \"Look, Harry, you are in perfect health. It's a spell you use to defeat flies. If you keep going down the drain, I've used another spell to control butterflies -- just keep going down the drain until you reach what's called a dew, Harry. As for tonight tonight, it all starts in the bedroom. You and Harry will have a little fun tonight.\" \n",
            "\n",
            "\"Thank you,\" Harry murmured. \n",
            "\n",
            "He was going to sit up at last. \n",
            "\n",
            "\"I'll see you tonight, Ron.\" \n",
            "\n",
            "\"Good morning, Ron.\" \n",
            "\n",
            "\"Morning, Harry.\" \n",
            "\n",
            "They were still talking when Harry left, leaving the table empty. He sat down, but Ron and Hermione were hanging onto his chair. \n",
            "\n",
            "Ron and Hermione were shaking hands. \n",
            "\n",
            "\"Is that so?\" said Harry, breath still on his ear. \n",
            "\n",
            "\"Of course, Ron,\" said Hermione. \n",
            "\n",
            "\"It's no trouble,\" said Harry, \"I know someone who can. \n",
            "\n",
            "But what does a duvet do?\" \n",
            "\n",
            "\"Well, it lets you put the books back on the bed,\" said Ron. He looked as if the hairs on his back had just been stubbled. \"And it forces you to take off the cloak, which means you're always out of sight, too. Well, you could do with yourself, Harry.\" \n",
            "\n",
            "Harry smiled up at this. \n",
            "\n",
            "\"What do they do if Dumbledore's here?\" \n",
            "\n",
            "\"They let him in now,\" said Ron. \"Let the whole family in.\" \n",
            "\n",
            "Harry couldn't believe it. \n",
            "\n",
            "He had never really felt so bad for Hermione before. They hadn't really been separated, and now he was. \n",
            "\n",
            "\"You may go and tell Ron and Hermione you'd gone to the toilet, they'll know.\" \n",
            "\n",
            "\"Are they going to give Neville a second chance?\" \n",
            "\n",
            "\"Absolutely,\" said Ron. \n",
            "\n",
            "\"What if Neville turns out to be a witch?\" \n",
            "\n",
            "Harry hadn't even looked at the reflection on his forehead. He didn't speak a word of his plan. He was still shaking hands with Ron and Hermione. \n",
            "\n",
            "\"I think you could tell him how to get past Flamel,\" said Ron, pointing into the distance. \n",
            "\n",
            "\"Harry -- Harry... you know he is.\" \n",
            "\n",
            "Harry kept staring at the mirror, which clearly showed him clearly. \n",
            "\n",
            "\"I know, I know,\" he said. \"I don't know what to tell Ron. He's going to die tomorrow, Harry. He can't be anything to Ron if he's in my spell. I just want him here. \n",
            "\n",
            "\"He may take him,\" said Hermione. \n",
            "\n",
            "\"But you will not let him in the school!\" \n",
            "\n",
            "\"But I won't,\" said Harry, because he was thinking too hard about how to best protect himself. \n",
            "\n",
            "\"So if you will, Harry, I will\n",
            "\n",
            "[601 | 157.76] loss=1.89 avg=2.51\n",
            "[602 | 159.02] loss=1.92 avg=2.50\n",
            "[603 | 160.27] loss=2.15 avg=2.50\n",
            "[604 | 161.53] loss=2.16 avg=2.49\n",
            "[605 | 162.79] loss=2.21 avg=2.49\n",
            "[606 | 164.05] loss=1.83 avg=2.48\n",
            "[607 | 165.30] loss=1.85 avg=2.47\n",
            "[608 | 166.56] loss=1.96 avg=2.46\n",
            "[609 | 167.81] loss=1.92 avg=2.45\n",
            "[610 | 169.07] loss=1.96 avg=2.44\n",
            "[611 | 170.32] loss=2.01 avg=2.44\n",
            "[612 | 171.58] loss=2.10 avg=2.43\n",
            "[613 | 172.83] loss=1.93 avg=2.43\n",
            "[614 | 174.09] loss=1.77 avg=2.42\n",
            "[615 | 175.35] loss=1.71 avg=2.41\n",
            "[616 | 176.62] loss=1.88 avg=2.40\n",
            "[617 | 177.87] loss=2.00 avg=2.39\n",
            "[618 | 179.14] loss=1.88 avg=2.39\n",
            "[619 | 180.39] loss=2.06 avg=2.38\n",
            "[620 | 181.65] loss=2.25 avg=2.38\n",
            "[621 | 182.91] loss=1.90 avg=2.37\n",
            "[622 | 184.16] loss=1.80 avg=2.36\n",
            "[623 | 185.42] loss=1.97 avg=2.36\n",
            "[624 | 186.69] loss=1.87 avg=2.35\n",
            "[625 | 187.96] loss=1.75 avg=2.34\n",
            "[626 | 189.21] loss=1.67 avg=2.33\n",
            "[627 | 190.48] loss=1.77 avg=2.33\n",
            "[628 | 191.74] loss=1.92 avg=2.32\n",
            "[629 | 193.00] loss=1.63 avg=2.31\n",
            "[630 | 194.26] loss=1.71 avg=2.30\n",
            "[631 | 195.51] loss=1.83 avg=2.30\n",
            "[632 | 196.76] loss=1.81 avg=2.29\n",
            "[633 | 198.01] loss=1.51 avg=2.28\n",
            "[634 | 199.26] loss=1.89 avg=2.27\n",
            "[635 | 200.51] loss=1.53 avg=2.26\n",
            "[636 | 201.77] loss=1.74 avg=2.26\n",
            "[637 | 203.02] loss=1.95 avg=2.25\n",
            "[638 | 204.27] loss=1.60 avg=2.24\n",
            "[639 | 205.52] loss=1.91 avg=2.24\n",
            "[640 | 206.77] loss=1.46 avg=2.23\n",
            "[641 | 208.02] loss=1.54 avg=2.22\n",
            "[642 | 209.27] loss=1.43 avg=2.21\n",
            "[643 | 210.52] loss=1.63 avg=2.20\n",
            "[644 | 211.77] loss=1.51 avg=2.19\n",
            "[645 | 213.03] loss=1.89 avg=2.19\n",
            "[646 | 214.30] loss=1.37 avg=2.18\n",
            "[647 | 215.56] loss=1.84 avg=2.17\n",
            "[648 | 216.82] loss=1.39 avg=2.16\n",
            "[649 | 218.07] loss=1.61 avg=2.16\n",
            "[650 | 219.33] loss=1.50 avg=2.15\n",
            "[651 | 220.58] loss=1.63 avg=2.14\n",
            "[652 | 221.83] loss=1.23 avg=2.13\n",
            "[653 | 223.09] loss=1.38 avg=2.12\n",
            "[654 | 224.34] loss=1.39 avg=2.11\n",
            "[655 | 225.59] loss=1.49 avg=2.10\n",
            "[656 | 226.84] loss=1.74 avg=2.10\n",
            "[657 | 228.10] loss=1.44 avg=2.09\n",
            "[658 | 229.35] loss=1.29 avg=2.08\n",
            "[659 | 230.61] loss=1.75 avg=2.08\n",
            "[660 | 231.87] loss=1.53 avg=2.07\n",
            "[661 | 233.12] loss=1.51 avg=2.06\n",
            "[662 | 234.38] loss=1.16 avg=2.05\n",
            "[663 | 235.64] loss=1.30 avg=2.04\n",
            "[664 | 236.89] loss=1.13 avg=2.03\n",
            "[665 | 238.14] loss=1.17 avg=2.02\n",
            "[666 | 239.39] loss=1.40 avg=2.01\n",
            "[667 | 240.65] loss=1.19 avg=2.00\n",
            "[668 | 241.92] loss=1.07 avg=1.99\n",
            "[669 | 243.18] loss=1.38 avg=1.98\n",
            "[670 | 244.43] loss=1.32 avg=1.98\n",
            "[671 | 245.70] loss=1.37 avg=1.97\n",
            "[672 | 246.95] loss=1.16 avg=1.96\n",
            "[673 | 248.19] loss=1.36 avg=1.95\n",
            "[674 | 249.45] loss=1.17 avg=1.94\n",
            "[675 | 250.70] loss=1.33 avg=1.93\n",
            "[676 | 251.96] loss=1.59 avg=1.93\n",
            "[677 | 253.22] loss=1.15 avg=1.92\n",
            "[678 | 254.48] loss=1.19 avg=1.91\n",
            "[679 | 255.73] loss=1.32 avg=1.90\n",
            "[680 | 256.98] loss=1.15 avg=1.90\n",
            "[681 | 258.25] loss=1.50 avg=1.89\n",
            "[682 | 259.52] loss=1.04 avg=1.88\n",
            "[683 | 260.78] loss=1.33 avg=1.87\n",
            "[684 | 262.04] loss=1.03 avg=1.86\n",
            "[685 | 263.31] loss=1.41 avg=1.86\n",
            "[686 | 264.56] loss=1.16 avg=1.85\n",
            "[687 | 265.82] loss=1.20 avg=1.84\n",
            "[688 | 267.07] loss=1.39 avg=1.84\n",
            "[689 | 268.33] loss=1.01 avg=1.83\n",
            "[690 | 269.58] loss=1.04 avg=1.82\n",
            "[691 | 270.83] loss=1.10 avg=1.81\n",
            "[692 | 272.08] loss=0.78 avg=1.80\n",
            "[693 | 273.34] loss=0.69 avg=1.79\n",
            "[694 | 274.61] loss=1.15 avg=1.78\n",
            "[695 | 275.88] loss=1.13 avg=1.77\n",
            "[696 | 277.14] loss=0.97 avg=1.76\n",
            "[697 | 278.39] loss=0.96 avg=1.75\n",
            "[698 | 279.64] loss=1.39 avg=1.75\n",
            "[699 | 280.89] loss=0.88 avg=1.74\n",
            "[700 | 282.14] loss=1.23 avg=1.73\n",
            "======== SAMPLE 1 ========\n",
            " that in the midst of all this was one who could offer a kind of consolation prize that no one else had.... I needn't have traveled so many times....\" \n",
            "\n",
            "He finished his letter shortly before noon. He was quite sure it was a very clever and clever thing that Voldemort was about. Although he hadn't yet received any owl calls. \n",
            "\n",
            "CHAPTER EIGHT \n",
            "\n",
            "VIRGINIA AND THE VIRGINIA FLY \n",
            "\n",
            "A YEAR OF GLORY \n",
            "\n",
            "Had Hagrid given Harry the red cloak and the golden egg? Yes, he did. Hagrid had been dethroned as Voldemort's second in command since his defeat at the hands of the dark wizard so that he was no longer held responsible for Voldemort's defeat. However, he had learned that lessons had to be learned and done to be truly powerful, even if that meant breaking new ground. \n",
            "\n",
            "On top of all the unexpected things Hagrid had done since he had been declared the enemy, Harry didn't have to deal with Voldemort again. Every morning as he stood outside the classroom he would press his nose to the counter to get a bag of ordinary Sn- foods. \n",
            "\n",
            "Even though he still remembered Malfoy and his face staring blankly out at the street lamps of that school in the empty tank top hats, he could now tell that Voldemort was looking forward to winning. As he strode down the corridor toward the dungeons they had built for Hagrid and Madam Malkin, he would have seen that the last entrance to this school had long since been taken away. On the contrary, this was the only entrance left open to anyone wishing to enter, and the only place where anyone could begieve death had been closed for almost fifty years. \n",
            "\n",
            "\"Welcome,\" said Hagrid, with large, white teeth. \"Welcome to the castle?\" \n",
            "\n",
            "\"Yes,\" said Harry. \n",
            "\n",
            "\"Welcome to the zoo?\" Hagrid grunted. \"Yes.... Yes, sit here.\" \n",
            "\n",
            "He sat down opposite Hagrid and started to introduce his new group. \n",
            "\n",
            "Harry looked carefully at his new zoo animal. It was a dog that liked to sit in the zoo. \n",
            "\n",
            "\"Well,\" said Hagrid, \"how else are you going to learn about the dragon ... and the troll?\" \n",
            "\n",
            "Harry couldn't help noticing that Hagrid's new zoo animal looked quite a bit like Hagrid's old zoo. \n",
            "\n",
            "\"Library books look a bit like that without the proper context,\" said Hagrid. \"I've read about the dragons and their uses and I thought I'd give Hagrid's zoo an A because of its dragons. Well, I suppose you're going to look at the troll and dragonkind a bit differently than you'd do about Hogwarts.\" \n",
            "\n",
            "And he went back to staring at the three animals. \n",
            "\n",
            "\"I'm not going to sit and learn all about the unicorns and the druids until I have a better idea of what their uses are,\" said Hagrid. \"The library already knows about the druids and their uses, so will you get a better idea of what they're good at? I'm sure I missed something, though, so feel free to let me know if you have any ideas.\" \n",
            "\n",
            "Harry took his glasses off and walked over to the other two. He wasn't sure he liked the look of the unicorn and the druids much anymore. He had a feeling they might not have been very interesting at all. He wandered around the library for a bit, hoping that if he lost his temper, he'd just give them a chance. \n",
            "\n",
            "He lay there watching as they took what little they had and moved on. For some reason, though, everything they had that day turned out to be just as important as what they'd gotten themselves. Harry didn't get what was happening in the druid hut. They had been finding unicorn horns and other valuable magical items all day. \n",
            "\n",
            "Suddenly, everyone in the library was surrounded by something that looked like a thousand human heads. Harry looked around. His heart hammered as he stared at the thousands of heads, he saw that nobody but the most determined troll had done anything with the unicorn horns or the druids planned the troll's next move. \n",
            "\n",
            "But there was something else lurking in the shadows above them; something that kept looking up, somewhere, even if it wasn't anyone else. \n",
            "\n",
            "When he looked up, it was Harry who was facing a towering rock that was overhead. It was dark and silent; he had just just stared when a voice spoke... \n",
            "\n",
            "A tall, thin, black-haired man.... Was it dead. \n",
            "\n",
            "Harry stood transfixed. There was no point in pretending he hadn't seen the body. Yet he couldn't stay here long. He had to find the Stone. He knew that finding the Stone was no easy task. It was not something\n",
            "\n",
            "[701 | 293.40] loss=1.11 avg=1.72\n",
            "[702 | 294.65] loss=0.79 avg=1.71\n",
            "[703 | 295.91] loss=1.01 avg=1.71\n",
            "[704 | 297.16] loss=0.89 avg=1.70\n",
            "[705 | 298.41] loss=0.80 avg=1.69\n",
            "[706 | 299.65] loss=1.25 avg=1.68\n",
            "[707 | 300.91] loss=0.91 avg=1.67\n",
            "[708 | 302.16] loss=0.88 avg=1.66\n",
            "[709 | 303.41] loss=0.90 avg=1.65\n",
            "[710 | 304.67] loss=0.57 avg=1.64\n",
            "[711 | 305.92] loss=0.93 avg=1.63\n",
            "[712 | 307.16] loss=0.89 avg=1.63\n",
            "[713 | 308.41] loss=0.56 avg=1.61\n",
            "[714 | 309.67] loss=0.63 avg=1.60\n",
            "[715 | 310.91] loss=1.17 avg=1.60\n",
            "[716 | 312.16] loss=0.72 avg=1.59\n",
            "[717 | 313.43] loss=0.90 avg=1.58\n",
            "[718 | 314.69] loss=0.92 avg=1.57\n",
            "[719 | 315.94] loss=0.93 avg=1.56\n",
            "[720 | 317.20] loss=0.51 avg=1.55\n",
            "[721 | 318.45] loss=0.77 avg=1.54\n",
            "[722 | 319.70] loss=0.72 avg=1.54\n",
            "[723 | 320.95] loss=0.87 avg=1.53\n",
            "[724 | 322.20] loss=0.87 avg=1.52\n",
            "[725 | 323.46] loss=0.85 avg=1.51\n",
            "[726 | 324.71] loss=0.90 avg=1.51\n",
            "[727 | 325.96] loss=0.66 avg=1.50\n",
            "[728 | 327.23] loss=0.68 avg=1.49\n",
            "[729 | 328.48] loss=0.61 avg=1.48\n",
            "[730 | 329.74] loss=0.50 avg=1.47\n",
            "[731 | 330.99] loss=1.01 avg=1.46\n",
            "[732 | 332.26] loss=0.66 avg=1.45\n",
            "[733 | 333.53] loss=0.54 avg=1.44\n",
            "[734 | 334.78] loss=0.78 avg=1.44\n",
            "[735 | 336.03] loss=0.91 avg=1.43\n",
            "[736 | 337.28] loss=0.72 avg=1.42\n",
            "[737 | 338.54] loss=0.95 avg=1.42\n",
            "[738 | 339.80] loss=0.81 avg=1.41\n",
            "[739 | 341.05] loss=0.92 avg=1.40\n",
            "[740 | 342.31] loss=0.71 avg=1.40\n",
            "[741 | 343.56] loss=0.68 avg=1.39\n",
            "[742 | 344.82] loss=0.64 avg=1.38\n",
            "[743 | 346.07] loss=0.77 avg=1.37\n",
            "[744 | 347.32] loss=0.55 avg=1.36\n",
            "[745 | 348.56] loss=0.56 avg=1.36\n",
            "[746 | 349.81] loss=0.79 avg=1.35\n",
            "[747 | 351.06] loss=0.80 avg=1.34\n",
            "[748 | 352.32] loss=0.92 avg=1.34\n",
            "[749 | 353.57] loss=0.62 avg=1.33\n",
            "[750 | 354.82] loss=0.41 avg=1.32\n",
            "[751 | 356.08] loss=0.39 avg=1.31\n",
            "[752 | 357.35] loss=0.60 avg=1.30\n",
            "[753 | 358.61] loss=0.52 avg=1.30\n",
            "[754 | 359.86] loss=0.59 avg=1.29\n",
            "[755 | 361.11] loss=0.59 avg=1.28\n",
            "[756 | 362.37] loss=0.38 avg=1.27\n",
            "[757 | 363.63] loss=0.48 avg=1.26\n",
            "[758 | 364.88] loss=0.57 avg=1.25\n",
            "[759 | 366.13] loss=0.51 avg=1.25\n",
            "[760 | 367.38] loss=0.56 avg=1.24\n",
            "[761 | 368.63] loss=0.49 avg=1.23\n",
            "[762 | 369.88] loss=0.43 avg=1.22\n",
            "[763 | 371.14] loss=0.49 avg=1.21\n",
            "[764 | 372.40] loss=0.52 avg=1.21\n",
            "[765 | 373.66] loss=0.54 avg=1.20\n",
            "[766 | 374.93] loss=0.35 avg=1.19\n",
            "[767 | 376.19] loss=0.36 avg=1.18\n",
            "[768 | 377.44] loss=0.57 avg=1.17\n",
            "[769 | 378.69] loss=0.30 avg=1.17\n",
            "[770 | 379.95] loss=0.52 avg=1.16\n",
            "[771 | 381.21] loss=0.41 avg=1.15\n",
            "[772 | 382.47] loss=0.37 avg=1.14\n",
            "[773 | 383.72] loss=0.53 avg=1.14\n",
            "[774 | 384.97] loss=0.39 avg=1.13\n",
            "[775 | 386.22] loss=0.38 avg=1.12\n",
            "[776 | 387.48] loss=0.35 avg=1.11\n",
            "[777 | 388.75] loss=0.44 avg=1.10\n",
            "[778 | 390.00] loss=0.39 avg=1.10\n",
            "[779 | 391.26] loss=0.35 avg=1.09\n",
            "[780 | 392.51] loss=0.36 avg=1.08\n",
            "[781 | 393.77] loss=0.37 avg=1.07\n",
            "[782 | 395.03] loss=0.42 avg=1.07\n",
            "[783 | 396.29] loss=0.45 avg=1.06\n",
            "[784 | 397.55] loss=0.45 avg=1.05\n",
            "[785 | 398.81] loss=0.27 avg=1.05\n",
            "[786 | 400.06] loss=0.51 avg=1.04\n",
            "[787 | 401.31] loss=0.38 avg=1.03\n",
            "[788 | 402.56] loss=0.37 avg=1.03\n",
            "[789 | 403.81] loss=0.46 avg=1.02\n",
            "[790 | 405.07] loss=0.39 avg=1.01\n",
            "[791 | 406.32] loss=0.37 avg=1.01\n",
            "[792 | 407.58] loss=0.42 avg=1.00\n",
            "[793 | 408.84] loss=0.29 avg=0.99\n",
            "[794 | 410.10] loss=0.41 avg=0.99\n",
            "[795 | 411.35] loss=0.42 avg=0.98\n",
            "[796 | 412.61] loss=0.24 avg=0.97\n",
            "[797 | 413.87] loss=0.23 avg=0.96\n",
            "[798 | 415.12] loss=0.20 avg=0.96\n",
            "[799 | 416.38] loss=0.22 avg=0.95\n",
            "[800 | 417.63] loss=0.36 avg=0.94\n",
            "======== SAMPLE 1 ========\n",
            " an emerald-shaped chamber with three doors that lead inside -- one on each side of the chamber. Behind the third door, a small window allowed passage only through a narrow passage that led upstairs to the second chamber. \n",
            "\n",
            "There, Harry could see some solid rock under the door. \n",
            "\n",
            "\"Shouldn't a giant be afraid of us?\" they bellowed. \n",
            "\n",
            "\"Not a giant, eh? Fear not, Fang, we are ready -- get off, start fighting!\" \n",
            "\n",
            "The three of them brought the fight to a halt. Fang had four fists in his wand and the stone was a foot high. His robes held nothing but dust and greasy crumbs, and his feet had to be dragged underneath him to stand on them. \n",
            "\n",
            "\"We have to join you, then, get off, fight!\" said the giant. \"Brilliant!\" said Harry. \n",
            "\n",
            "\"SO! COME, COME!\" \n",
            "\n",
            "Now they had to go through the three-headed dog; there were piles of what looked like rolled up cabbage and a few leaves that looked as if they were floating in midair. \n",
            "\n",
            "\"Can we just walk?\" said Harry. \n",
            "\n",
            "\"Certainly! It's very difficult to see, go straight -- but we'll shortly be able to see you. Oh, go on, enjoy yourselves!\" said the giant. \n",
            "\n",
            "\"I don't see how!\" said Harry. \"After what seems like ages, we've come so far! Ten minutes, Harry, go, enjoy yourselves!\" \n",
            "\n",
            "But the giant beckoned. \n",
            "\n",
            "\"Fang, here I go!\" \n",
            "\n",
            "The first thing Harry saw, his heart sink, was a small pile of clothes suddenly falling off the top of his head. He wriggled them tight around his neck and tried to pull himself up, but they held no movement -- he fell straight down and grabbed the first few clothes he'd been handed. His legs wouldn't move a single pair. \n",
            "\n",
            "\"Oy, Professor McGonagall!\" \n",
            "\n",
            "Fang, who was lying on top of Professor Quirrell, was positively steamrolling. His eyes were white with rage, but his lips were narrow and he was holding his breath. \n",
            "\n",
            "\"STOP TAKING HIM DOWN! I COULDN'T CURRENTLY HAVEN'T I CONVICTED YOU! I WANT TO GET MY OLD MAN OUT OF THE OLD MAN OLD VANISHMENT!\" \n",
            "\n",
            "finally yelled Professor Quirrell, \"GRYFFIC!\" \n",
            "\n",
            "It was Harry's turn to fall back, but Professor Quirrell leapt on top of him. He had almost forgotten that he was there, standing almost right up to Harry's face, with his hands raised over his shoulders as though he were about to be swallowed. He had a hundred percent decided that this was the moment when he'd lose his temper, left Harry's hand entirely on top of his trousers. \n",
            "\n",
            "\"BOOM!\" boomed Professor Quirrell, throwing a pair of bad-minton shoes at Harry. \n",
            "\n",
            "\"NOT GETOUTS!\" yelled Fang, who was following shortly afterward. \n",
            "\n",
            "\"RIDICULOUSLY COME AND FIND HIM!\" \n",
            "\n",
            "Now that Harry was back on the spot, Harry could say, \"Where's the broom?\" He'd forgotten that there were any. \n",
            "\n",
            "\"PHOENIX WHORE BORDERS!\" \n",
            "\n",
            "\"Wandering about in Harry's debt,\" Hagrid rummaged around in his tin can. \"What's the matter?\" he said to the Professor Quirrell was standing right next to him. \"Shirting his way around the corner for me, he left an owl in my face!\" \n",
            "\n",
            "Hagrid furrowed his brow. \"What did he say was 'silly as ever'?\" \n",
            "\n",
            "\"Rubbish,\" said the Professor Quirrell. \"He's nearly there, even with his good-for-nothing feet.\" \n",
            "\n",
            "\"He could sit and stare for hours,\" muttered Harry in his agony. \n",
            "\n",
            "Professor Quirrell pulled out a broomstick and kicked hard in the face. Harry swung it hard in the air, something tipped halfway into Professor Quirrell's face and it went straight at Harry, knocking him upside down. \n",
            "\n",
            "\"STOP FLYING FOR MYSTICAL DOGS!\" yelled Hagrid in rage. \"CARRIED OF THE WHEELS FOR 21 YEARS BOUNC'T YOU? YOU'VE SINCE!\" \n",
            "\n",
            "Harry landed three floors short of the forbidden forest and, for the first time in his life, began to feel strangely as though he'd been chased by a giant. \n",
            "\n",
            "\"Where are you?\" said the Professor Quirrell loudly. \n",
            "\n",
            "\n",
            "\n",
            "[801 | 428.93] loss=0.39 avg=0.94\n",
            "[802 | 430.18] loss=0.28 avg=0.93\n",
            "[803 | 431.45] loss=0.39 avg=0.92\n",
            "[804 | 432.72] loss=0.42 avg=0.92\n",
            "[805 | 433.98] loss=0.26 avg=0.91\n",
            "[806 | 435.24] loss=0.31 avg=0.91\n",
            "[807 | 436.50] loss=0.31 avg=0.90\n",
            "[808 | 437.76] loss=0.30 avg=0.89\n",
            "[809 | 439.01] loss=0.41 avg=0.89\n",
            "[810 | 440.27] loss=0.36 avg=0.88\n",
            "[811 | 441.52] loss=0.21 avg=0.88\n",
            "[812 | 442.79] loss=0.38 avg=0.87\n",
            "[813 | 444.04] loss=0.21 avg=0.86\n",
            "[814 | 445.30] loss=0.43 avg=0.86\n",
            "[815 | 446.56] loss=0.35 avg=0.85\n",
            "[816 | 447.82] loss=0.23 avg=0.85\n",
            "[817 | 449.07] loss=0.26 avg=0.84\n",
            "[818 | 450.32] loss=0.28 avg=0.84\n",
            "[819 | 451.58] loss=0.33 avg=0.83\n",
            "[820 | 452.85] loss=0.18 avg=0.82\n",
            "[821 | 454.11] loss=0.24 avg=0.82\n",
            "[822 | 455.37] loss=0.22 avg=0.81\n",
            "[823 | 456.63] loss=0.20 avg=0.80\n",
            "[824 | 457.89] loss=0.17 avg=0.80\n",
            "[825 | 459.15] loss=0.29 avg=0.79\n",
            "[826 | 460.41] loss=0.23 avg=0.79\n",
            "[827 | 461.67] loss=0.21 avg=0.78\n",
            "[828 | 462.92] loss=0.28 avg=0.78\n",
            "[829 | 464.19] loss=0.23 avg=0.77\n",
            "[830 | 465.45] loss=0.21 avg=0.76\n",
            "[831 | 466.71] loss=0.22 avg=0.76\n",
            "[832 | 467.96] loss=0.16 avg=0.75\n",
            "[833 | 469.22] loss=0.24 avg=0.75\n",
            "[834 | 470.47] loss=0.16 avg=0.74\n",
            "[835 | 471.73] loss=0.25 avg=0.74\n",
            "[836 | 473.00] loss=0.21 avg=0.73\n",
            "[837 | 474.26] loss=0.19 avg=0.72\n",
            "[838 | 475.52] loss=0.39 avg=0.72\n",
            "[839 | 476.77] loss=0.20 avg=0.72\n",
            "[840 | 478.03] loss=0.22 avg=0.71\n",
            "[841 | 479.29] loss=0.26 avg=0.71\n",
            "[842 | 480.54] loss=0.20 avg=0.70\n",
            "[843 | 481.80] loss=0.21 avg=0.70\n",
            "[844 | 483.05] loss=0.30 avg=0.69\n",
            "[845 | 484.32] loss=0.15 avg=0.69\n",
            "[846 | 485.58] loss=0.16 avg=0.68\n",
            "[847 | 486.85] loss=0.16 avg=0.68\n",
            "[848 | 488.11] loss=0.25 avg=0.67\n",
            "[849 | 489.36] loss=0.29 avg=0.67\n",
            "[850 | 490.62] loss=0.16 avg=0.66\n",
            "[851 | 491.88] loss=0.15 avg=0.66\n",
            "[852 | 493.15] loss=0.18 avg=0.65\n",
            "[853 | 494.40] loss=0.14 avg=0.65\n",
            "[854 | 495.67] loss=0.25 avg=0.64\n",
            "[855 | 496.92] loss=0.17 avg=0.64\n",
            "[856 | 498.17] loss=0.38 avg=0.63\n",
            "[857 | 499.43] loss=0.19 avg=0.63\n",
            "[858 | 500.68] loss=0.13 avg=0.62\n",
            "[859 | 501.93] loss=0.17 avg=0.62\n",
            "[860 | 503.20] loss=0.21 avg=0.62\n",
            "[861 | 504.45] loss=0.20 avg=0.61\n",
            "[862 | 505.70] loss=0.17 avg=0.61\n",
            "[863 | 506.96] loss=0.18 avg=0.60\n",
            "[864 | 508.22] loss=0.14 avg=0.60\n",
            "[865 | 509.48] loss=0.21 avg=0.59\n",
            "[866 | 510.74] loss=0.13 avg=0.59\n",
            "[867 | 512.00] loss=0.18 avg=0.58\n",
            "[868 | 513.25] loss=0.18 avg=0.58\n",
            "[869 | 514.51] loss=0.15 avg=0.58\n",
            "[870 | 515.76] loss=0.15 avg=0.57\n",
            "[871 | 517.01] loss=0.17 avg=0.57\n",
            "[872 | 518.27] loss=0.15 avg=0.56\n",
            "[873 | 519.52] loss=0.19 avg=0.56\n",
            "[874 | 520.77] loss=0.12 avg=0.56\n",
            "[875 | 522.03] loss=0.12 avg=0.55\n",
            "[876 | 523.29] loss=0.24 avg=0.55\n",
            "[877 | 524.53] loss=0.15 avg=0.54\n",
            "[878 | 525.79] loss=0.10 avg=0.54\n",
            "[879 | 527.04] loss=0.23 avg=0.54\n",
            "[880 | 528.29] loss=0.13 avg=0.53\n",
            "[881 | 529.55] loss=0.11 avg=0.53\n",
            "[882 | 530.80] loss=0.15 avg=0.52\n",
            "[883 | 532.06] loss=0.11 avg=0.52\n",
            "[884 | 533.32] loss=0.15 avg=0.52\n",
            "[885 | 534.57] loss=0.31 avg=0.51\n",
            "[886 | 535.84] loss=0.09 avg=0.51\n",
            "[887 | 537.10] loss=0.13 avg=0.51\n",
            "[888 | 538.37] loss=0.08 avg=0.50\n",
            "[889 | 539.62] loss=0.13 avg=0.50\n",
            "[890 | 540.88] loss=0.13 avg=0.49\n",
            "[891 | 542.14] loss=0.13 avg=0.49\n",
            "[892 | 543.39] loss=0.14 avg=0.49\n",
            "[893 | 544.65] loss=0.15 avg=0.48\n",
            "[894 | 545.91] loss=0.10 avg=0.48\n",
            "[895 | 547.17] loss=0.10 avg=0.47\n",
            "[896 | 548.43] loss=0.11 avg=0.47\n",
            "[897 | 549.69] loss=0.13 avg=0.47\n",
            "[898 | 550.95] loss=0.11 avg=0.46\n",
            "[899 | 552.21] loss=0.11 avg=0.46\n",
            "[900 | 553.48] loss=0.14 avg=0.46\n",
            "======== SAMPLE 1 ========\n",
            " couldn't have done it, he'd have had to go much quicker and kill it than he did here.\" \n",
            "\n",
            "\"It was worth it,\" said Dumbledore quietly. \"I bet he'd have found out everything he'd been looking for, had he got all his money's worth.\" \n",
            "\n",
            "\"You can't go looking for it on your own,\" said Ron airily. \"Come on.\" \n",
            "\n",
            "There was a heavy sighing in the man's voice, and Harry, Ron, and Hermione followed Harry into a small, spray-soaked clearing ahead. Firsar hadn't seen Harry on his own. He hurried back through the door and peered through the crack and splintered-away door, realizing his mistake immediately. \n",
            "\n",
            "\"Where did I go?\" he heard the scythe rustling behind him. \n",
            "\n",
            "\"I don't know,\" he said slowly, \"but I thought I'd have a harder time hiding it. In here, on the run from the Goyle,\" he added. \n",
            "\n",
            "Draco Malfoy wasn't going to give in without a good fight. He leapt at the moment's mention and leapt back before the flying dervishes could overtake him. \n",
            "\n",
            "\"Firsar!\" said Ron. \"Let's see... Sir -- Hermione --?\" \n",
            "\n",
            "It was Hermione -- what, hundreds of books? Ron looked impatient. \n",
            "\n",
            "\"Books? What are you reading about? There's owls, aren't we?\" \n",
            "\n",
            "\"Listen, if you ask me, they don't do astronomy or something,\" said Malfoy stiffly. \"But you don't want to go studying for History and Politics, do you? I'll take you to my library.\" \n",
            "\n",
            "\"I won't,\" said Ron panic-stricken. \"I know you're not going to come out of Gryffindor tower looking for History and Politics, but why worry about us? You'll be safe.\" \n",
            "\n",
            "There was no escaping the bookish manor, which was so bad it drew almost no air. Harry sank down next to the fire in his cupboard and started looking through it. Unfortunately, he found a large poster of Seeker blood on the mantelpiece. This was his first time in so many weeks. He pulled out his wand, put it behind his back, and muttered something to save himself. He couldn't see which was which, but he knew the look on his face when someone touched him if he touched a speck. \"You saved my life,\" he said. \"Showy as canayous.\" \n",
            "\n",
            "He pulled open the cover of Diagon Alley and caught a train to King's Cross. There were tables all over the walls, but Harry didn't open them. They stared at each other, thinking, Why aren't you all copying down the different ways of doing battle? And finally, he pulled the poster shut behind him, so that the people in the portrait weren't looking at him. \n",
            "\n",
            "\"Now what are you going to do, troll Hagrid?\" \n",
            "\n",
            "Harry thought for a while and then said, \"I'm going to be your troll -- a bit of both.\" \n",
            "\n",
            "\"How what?\" came a low voice from under his pillow. \n",
            "\n",
            "\"I don't know,\" said Harry, remembering what he had seen and felt. \"I don't think anyone gets paid but me. The Dursleys make us.\" \n",
            "\n",
            "He looked over his shoulder at the three of them, and saw another bear come swiftly toward him. He reached up and grasped Harry's arm, sending him over the edge. \n",
            "\n",
            "\"Don't,\" he said, \"I'm with the Devil -- and no, I don't want to be with Mr. Dark -- you can't follow my orders -- it's too late, now.\" \n",
            "\n",
            "He looked at Harry in disgust. \n",
            "\n",
            "\"I'll -- I'll -- kill you,\" he said finally. \n",
            "\n",
            "He stumbled backward. He hadn't thought to call it quits yet. He grabbed Harry by the arm and carried him out of the house. \n",
            "\n",
            "When he got to the castle, everyone was staring at him in shock. \n",
            "\n",
            "Madam Hooch sat with her mouth open. Cold fingers gripped the lace curtains. \n",
            "\n",
            "\"Madam Hooch, this is Rudyard Kipling,\" said a tall man sitting on the edge. \"The man I feared he might be.... He's not acting alone, is he?\" \n",
            "\n",
            "A funny thing happened. As the man walked toward them, he took a long drag out of his cloak. It was a handsome, black-haired man with a hooked nose. He looked distinctly ruddy. \n",
            "\n",
            "\"How well you of yeh?\" he asked. \n",
            "\n",
            "\"Augh -- a bit o' waitin' now, Mandy.\" \n",
            "\n",
            "He reached out a hand\n",
            "\n",
            "[901 | 564.75] loss=0.12 avg=0.45\n",
            "[902 | 566.02] loss=0.09 avg=0.45\n",
            "[903 | 567.29] loss=0.10 avg=0.45\n",
            "[904 | 568.55] loss=0.11 avg=0.44\n",
            "[905 | 569.81] loss=0.15 avg=0.44\n",
            "[906 | 571.09] loss=0.11 avg=0.44\n",
            "[907 | 572.35] loss=0.11 avg=0.43\n",
            "[908 | 573.62] loss=0.09 avg=0.43\n",
            "[909 | 574.88] loss=0.18 avg=0.43\n",
            "[910 | 576.15] loss=0.10 avg=0.42\n",
            "[911 | 577.40] loss=0.09 avg=0.42\n",
            "[912 | 578.66] loss=0.10 avg=0.42\n",
            "[913 | 579.92] loss=0.11 avg=0.41\n",
            "[914 | 581.17] loss=0.13 avg=0.41\n",
            "[915 | 582.43] loss=0.13 avg=0.41\n",
            "[916 | 583.69] loss=0.13 avg=0.41\n",
            "[917 | 584.94] loss=0.10 avg=0.40\n",
            "[918 | 586.20] loss=0.14 avg=0.40\n",
            "[919 | 587.46] loss=0.14 avg=0.40\n",
            "[920 | 588.75] loss=0.09 avg=0.39\n",
            "[921 | 590.01] loss=0.11 avg=0.39\n",
            "[922 | 591.27] loss=0.11 avg=0.39\n",
            "[923 | 592.52] loss=0.09 avg=0.39\n",
            "[924 | 593.77] loss=0.12 avg=0.38\n",
            "[925 | 595.04] loss=0.12 avg=0.38\n",
            "[926 | 596.30] loss=0.12 avg=0.38\n",
            "[927 | 597.55] loss=0.11 avg=0.37\n",
            "[928 | 598.82] loss=0.11 avg=0.37\n",
            "[929 | 600.08] loss=0.18 avg=0.37\n",
            "[930 | 601.34] loss=0.12 avg=0.37\n",
            "[931 | 602.59] loss=0.14 avg=0.37\n",
            "[932 | 603.85] loss=0.11 avg=0.36\n",
            "[933 | 605.13] loss=0.17 avg=0.36\n",
            "[934 | 606.38] loss=0.08 avg=0.36\n",
            "[935 | 607.64] loss=0.12 avg=0.36\n",
            "[936 | 608.90] loss=0.12 avg=0.35\n",
            "[937 | 610.15] loss=0.11 avg=0.35\n",
            "[938 | 611.42] loss=0.12 avg=0.35\n",
            "[939 | 612.68] loss=0.09 avg=0.35\n",
            "[940 | 613.94] loss=0.10 avg=0.34\n",
            "[941 | 615.20] loss=0.11 avg=0.34\n",
            "[942 | 616.47] loss=0.11 avg=0.34\n",
            "[943 | 617.73] loss=0.11 avg=0.34\n",
            "[944 | 618.98] loss=0.12 avg=0.33\n",
            "[945 | 620.24] loss=0.08 avg=0.33\n",
            "[946 | 621.49] loss=0.08 avg=0.33\n",
            "[947 | 622.76] loss=0.12 avg=0.33\n",
            "[948 | 624.02] loss=0.08 avg=0.32\n",
            "[949 | 625.28] loss=0.09 avg=0.32\n",
            "[950 | 626.53] loss=0.10 avg=0.32\n",
            "[951 | 627.79] loss=0.12 avg=0.32\n",
            "[952 | 629.05] loss=0.08 avg=0.32\n",
            "[953 | 630.31] loss=0.09 avg=0.31\n",
            "[954 | 631.57] loss=0.09 avg=0.31\n",
            "[955 | 632.83] loss=0.08 avg=0.31\n",
            "[956 | 634.08] loss=0.09 avg=0.31\n",
            "[957 | 635.35] loss=0.10 avg=0.30\n",
            "[958 | 636.60] loss=0.08 avg=0.30\n",
            "[959 | 637.87] loss=0.10 avg=0.30\n",
            "[960 | 639.14] loss=0.09 avg=0.30\n",
            "[961 | 640.40] loss=0.07 avg=0.30\n",
            "[962 | 641.66] loss=0.09 avg=0.29\n",
            "[963 | 642.91] loss=0.10 avg=0.29\n",
            "[964 | 644.16] loss=0.08 avg=0.29\n",
            "[965 | 645.42] loss=0.08 avg=0.29\n",
            "[966 | 646.69] loss=0.08 avg=0.29\n",
            "[967 | 647.94] loss=0.10 avg=0.28\n",
            "[968 | 649.19] loss=0.08 avg=0.28\n",
            "[969 | 650.45] loss=0.10 avg=0.28\n",
            "[970 | 651.71] loss=0.10 avg=0.28\n",
            "[971 | 652.98] loss=0.09 avg=0.28\n",
            "[972 | 654.25] loss=0.09 avg=0.27\n",
            "[973 | 655.50] loss=0.10 avg=0.27\n",
            "[974 | 656.76] loss=0.09 avg=0.27\n",
            "[975 | 658.02] loss=0.10 avg=0.27\n",
            "[976 | 659.29] loss=0.08 avg=0.27\n",
            "[977 | 660.54] loss=0.08 avg=0.26\n",
            "[978 | 661.80] loss=0.11 avg=0.26\n",
            "[979 | 663.07] loss=0.12 avg=0.26\n",
            "[980 | 664.33] loss=0.09 avg=0.26\n",
            "[981 | 665.58] loss=0.07 avg=0.26\n",
            "[982 | 666.84] loss=0.08 avg=0.26\n",
            "[983 | 668.10] loss=0.08 avg=0.25\n",
            "[984 | 669.35] loss=0.09 avg=0.25\n",
            "[985 | 670.62] loss=0.07 avg=0.25\n",
            "[986 | 671.88] loss=0.08 avg=0.25\n",
            "[987 | 673.14] loss=0.10 avg=0.25\n",
            "[988 | 674.40] loss=0.10 avg=0.25\n",
            "[989 | 675.66] loss=0.10 avg=0.24\n",
            "[990 | 676.92] loss=0.11 avg=0.24\n",
            "[991 | 678.18] loss=0.09 avg=0.24\n",
            "[992 | 679.45] loss=0.09 avg=0.24\n",
            "[993 | 680.70] loss=0.08 avg=0.24\n",
            "[994 | 681.96] loss=0.11 avg=0.24\n",
            "[995 | 683.22] loss=0.08 avg=0.24\n",
            "[996 | 684.48] loss=0.07 avg=0.23\n",
            "[997 | 685.73] loss=0.09 avg=0.23\n",
            "[998 | 686.98] loss=0.09 avg=0.23\n",
            "[999 | 688.24] loss=0.08 avg=0.23\n",
            "[1000 | 689.49] loss=0.09 avg=0.23\n",
            "Saving checkpoint/run1/model-1000\n",
            "Harry stared at Draco Malfoy, who looked as if he'd never been scared as he walked through the door. \n",
            "\n",
            "\"Well, there you are,\" Harry said, \"Snape's a Potter again.\" \n",
            "\n",
            "Snape's face fell. \"I'm sorry,\" he said slowly, \"but I don't have a twin. I'd better have a broomstick.\" \n",
            "\n",
            "He raised his fist. \"Er -- sorry.\" \n",
            "\n",
            "He glared at Harry. \n",
            "\n",
            "\"You don't mean -- well, I suppose you still think you're great, but I suppose you're just that vain --\" \n",
            "\n",
            "\"No, no, no, no. My twin's a natural. \n",
            "\n",
            "What's the difference --?\" \n",
            "\n",
            "\"Oh, your story's not that different. I suppose --\" \n",
            "\n",
            "\" -- why, you ask? Well, what difference --\" \n",
            "\n",
            "\" -- why? How did I get a twin? You asked me, because I'm great, didn't you? Why didn't you tell me? I didn't know what was going on. come on, come on.\" \n",
            "\n",
            "The door swung open at once. A bundle of walking sticks were sticking out of it. \n",
            "\n",
            "\"Well, there's -- yer -- second cousin, a deaconstick's at Hogwarts? How are you supposed to keep them from stabbing each other? Explain.\" \n",
            "\n",
            "Harry sat down next to the second cousin, who was wearing a monk's robe, and listened. About ten people came up the stairs. One of the monks was wearing a pointed hat, the other a frizzled wig. The other monk was talking loudly, himself, too. \n",
            "\n",
            "\"Second cousin,\" he said. \"He's a monk. A very famous one. I even got a letter from him, saying he's a monk. He doesn't think we should keep him in the dark about what's going on. \n",
            "\n",
            "You've got to keep in mind, though, that the monk's job is to keep the school safe. The school's not toFirst, second cousin -- the school's supposed to be a good place for students to get ideas and ideas of their own making, safe for work and for play. \n",
            "\n",
            "But second cousin -- what do they call people who come from far places to see the forbidden -- and he was talking about a teacher who was desperately trying to keep the school as safe as possible. \n",
            "\n",
            "\"Second cousin,\" said the monk, \"he was on the run from the school. He was last seen in London. He passed on.\" \n",
            "\n",
            "\"He was on the run?\" \n",
            "\n",
            "\"Passed on.\" \n",
            "\n",
            "\"Horrified me,\" said the monk, \"how could a teacher walk for hours on end and not be searched?\" \n",
            "\n",
            "A few people laughed. \n",
            "\n",
            "\"He was on the run from the school? Horror! Horror!\" \n",
            "\n",
            "\"Passed on!\" said the monk, \"but he wasn't on the run! He's now!\" \n",
            "\n",
            "\"Horrified!\" said the monk, \"how could a teacher walk for hours and not be searched?\" \n",
            "\n",
            "A few people shouted. \n",
            "\n",
            "\"Passed on!\" said the monk, \"but clearly not all teachers are like that. the school's being searched!\" \n",
            "\n",
            "\"Horrified!\" said the people who had been shouting. \"Passed on!\" said the monk, looking around rapidly to make sure no one was behind the locked doors. lots of people standing around him right now looking for him. he heard someone saying, \"He's gone dark, they've found him. He was on the run from the school.\" \n",
            "\n",
            "\"Yes,\" said the monk, his eyes glinting. \"Yes, but how?\" \n",
            "\n",
            "He looked around quickly and saw that everyone else was staring at him in shock. He saw the windows of the high-security school smashed into several angles, the distant rustling of shops and shops selling cloaks and mirror glasses. Houses had been destroyed in the crash. \n",
            "\n",
            "\"Yes,\" said the monk, his voice shaking. \"Yes, but not all of them. Some of them were worth hundreds of thousands of pounds.\" \n",
            "\n",
            "He saw a car crash on the outskirts of the Great Hall, on the corner of Privet Drive and Grand Trafalgar St. \n",
            "\n",
            "A few people managed to escape the attack by jumping on top of each other, and sticking their necks out of the way to stop the car from going off course, ripping apart and exploding. \n",
            "\n",
            "\"Yes, yes, that's us.\" \n",
            "\n",
            "\"There's more.\" \n",
            "\n",
            "\"Don't tell me you were going to hurt anyone, are we?\" said a small voice. \n",
            "\n",
            "\"Not to worry, little one, we're safe now.\" \n",
            "\n",
            "The\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}