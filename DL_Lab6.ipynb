{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Lab6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE8ElXCulbYL",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericburdett/cs474_labs_f2019/blob/master/DL_Lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cksgAH12XRjV"
      },
      "source": [
        "# Lab 6: Sequence-to-sequence models\n",
        "\n",
        "## Description:\n",
        "For this lab, you will code up the [char-rnn model of Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). This is a recurrent neural network that is trained probabilistically on sequences of characters, and that can then be used to sample new sequences that are like the original.\n",
        "\n",
        "This lab will help you develop several new skills, as well as understand some best practices needed for building large models. In addition, we'll be able to create networks that generate neat text!\n",
        "\n",
        "## There are two parts of this lab:\n",
        "###  1.   Wiring up a basic sequence-to-sequence computation graph\n",
        "###  2.   Implementing your own GRU cell.\n",
        "\n",
        "\n",
        "An example of my final samples are shown below (more detail in the\n",
        "final section of this writeup), after 150 passes through the data.\n",
        "Please generate about 15 samples for each dataset.\n",
        "\n",
        "<code>\n",
        "And ifte thin forgision forward thene over up to a fear not your\n",
        "And freitions, which is great God. Behold these are the loss sub\n",
        "And ache with the Lord hath bloes, which was done to the holy Gr\n",
        "And appeicis arm vinimonahites strong in name, to doth piseling \n",
        "And miniquithers these words, he commanded order not; neither sa\n",
        "And min for many would happine even to the earth, to said unto m\n",
        "And mie first be traditions? Behold, you, because it was a sound\n",
        "And from tike ended the Lamanites had administered, and I say bi\n",
        "</code>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c2i_QpSsWG4c"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 0: Readings, data loading, and high level training\n",
        "\n",
        "---\n",
        "\n",
        "There is a tutorial here that will help build out scaffolding code, and get an understanding of using sequences in pytorch.\n",
        "\n",
        "* Read the following\n",
        "\n",
        "> * [Pytorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
        "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l7bdZWxvJrsx",
        "outputId": "6f155b66-4fe4-4b7f-85c9-ac0be676abb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n",
        "! tar -xzf text_files.tar.gz\n",
        "! pip install unidecode\n",
        "! pip install torch\n",
        "\n",
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        " \n",
        "import pdb\n",
        " \n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)\n",
        "\n",
        "shake_file = unidecode.unidecode(open('./text_files/tiny_shakespeare.txt').read())\n",
        "shake_file_len = len(shake_file)\n",
        "print('shake_file_len=', shake_file_len)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-20 06:27:27--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n",
            "Resolving piazza.com (piazza.com)... 34.205.95.128, 52.45.119.166, 52.2.48.133, ...\n",
            "Connecting to piazza.com (piazza.com)|34.205.95.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n",
            "--2019-10-20 06:27:27--  https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n",
            "Resolving d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)... 13.224.217.34, 13.224.217.219, 13.224.217.42, ...\n",
            "Connecting to d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)|13.224.217.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1533290 (1.5M) [application/x-gzip]\n",
            "Saving to: ‘./text_files.tar.gz’\n",
            "\n",
            "\r./text_files.tar.gz   0%[                    ]       0  --.-KB/s               \r./text_files.tar.gz 100%[===================>]   1.46M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-10-20 06:27:27 (32.5 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n",
            "\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.0+cu100)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.5)\n",
            "file_len = 2579888\n",
            "shake_file_len= 1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TxBeKeNjJ0NQ",
        "outputId": "5eeb1348-7b2e-4596-fd3a-93b116243408",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "chunk_len = 200\n",
        " \n",
        "def random_chunk():\n",
        "  start_index = random.randint(0, file_len - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]\n",
        "\n",
        "def shake_random_chunk():\n",
        "  start_index = random.randint(0, shake_file_len - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return shake_file[start_index:end_index]\n",
        "  \n",
        "print(random_chunk())\n",
        "print(shake_random_chunk())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "d looking with disgust at the \n",
            "carved handle: it had been shaped like a hideous head with squinting eyes \n",
            "and leering mouth. \n",
            "\n",
            "'Well, here is the strangest riddle that we have yet found!' exclaimed \n",
            "Le\n",
            " Yet I well remember\n",
            "The favours of these men: were they not mine?\n",
            "Did they not sometime cry, 'all hail!' to me?\n",
            "So Judas did to Christ: but he, in twelve,\n",
            "Found truth in all but one: I, in twelve thou\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "On0_WitWJ99e",
        "outputId": "6ef1ee7f-e55a-49f0-e490-9c66d13da0de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "  tensor = torch.zeros(len(string)).long()\n",
        "  for c in range(len(string)):\n",
        "      tensor[c] = all_characters.index(string[c])\n",
        "  return tensor\n",
        "\n",
        "print(char_tensor('abcDEF'))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([10, 11, 12, 39, 40, 41])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CYJPTLcaYmfI"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: Creating your own GRU cell \n",
        "\n",
        "**(Come back to this later - its defined here so that the GRU will be defined before it is used)**\n",
        "\n",
        "---\n",
        "\n",
        "The cell that you used in Part 1 was a pre-defined Pytorch layer. Now, write your own GRU class using the same parameters as the built-in Pytorch class does.\n",
        "\n",
        "Please try not to look at the GRU cell definition. The answer is right there in the code, and in theory, you could just cut-and-paste it. This bit is on your honor!\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "**DONE:**\n",
        "* Create a custom GRU cell\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aavAv50ZKQ-F",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Each layer does the following:\n",
        "# r_t = sigmoid(W_ir*x_t + b_ir + W_hr*h_(t-1) + b_hr)\n",
        "# z_t = sigmoid(W_iz*x_t + b_iz + W_hz*h_(t-1) + b_hz)\n",
        "# n_t = tanh(W_in*x_t + b_in + r_t**(W_hn*h_(t-1) + b_hn))\n",
        "# h_(t) = (1 - z_t)**n_t + z_t**h_(t-1)\n",
        "# Where ** is hadamard product (not matrix multiplication, but elementwise multiplication)\n",
        "class GRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "    super(GRU, self).__init__()\n",
        "    \n",
        "    self.w_ir = nn.Linear(input_size, hidden_size)\n",
        "    self.w_hr = nn.Linear(hidden_size, hidden_size)\n",
        "    self.w_iz = nn.Linear(input_size, hidden_size)\n",
        "    self.w_hz = nn.Linear(hidden_size, hidden_size)\n",
        "    self.w_in = nn.Linear(input_size, hidden_size)\n",
        "    self.w_hn = nn.Linear(hidden_size, hidden_size)\n",
        "    \n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.tanh = nn.Tanh()\n",
        "  \n",
        "  def forward(self, inputs, hidden):\n",
        "    \n",
        "    r_t = self.sigmoid(self.w_ir(inputs) + self.w_hr(hidden))\n",
        "    z_t = self.sigmoid(self.w_iz(inputs) + self.w_hz(hidden))\n",
        "    \n",
        "    n_t = self.tanh(self.w_in(inputs) + torch.mul(r_t, self.w_hn(hidden)))\n",
        "    h_t = torch.mul(1 - z_t, n_t) + torch.mul(z_t, hidden)\n",
        "    \n",
        "    return h_t[-1:], h_t\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "77ihHoC0gcEn",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qtXdX-B_WiAY"
      },
      "source": [
        "---\n",
        "\n",
        "##  Part 1: Building a sequence to sequence model\n",
        "\n",
        "---\n",
        "\n",
        "Great! We have the data in a useable form. We can switch out which text file we are reading from, and trying to simulate.\n",
        "\n",
        "We now want to build out an RNN model, in this section, we will use all built in Pytorch pieces when building our RNN class.\n",
        "\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "**DONE:**\n",
        "* Create an RNN class that extends from nn.Module.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d6tNdEnzWj5F",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    \n",
        "    self.embed = nn.Embedding(n_characters, hidden_size)\n",
        "    self.gru = GRU(input_size=hidden_size, hidden_size=hidden_size, num_layers=n_layers)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear = nn.Linear(self.hidden_size, self.output_size)\n",
        "    \n",
        "    # more stuff here...\n",
        "\n",
        "  def forward(self, input_char, hidden):\n",
        "    embedding = self.embed(input_char).unsqueeze(0).unsqueeze(0)\n",
        "    out_decoded, hidden = self.gru(embedding, hidden)\n",
        "    linear_out = self.linear(out_decoded)\n",
        "    output = self.relu(linear_out)\n",
        "\n",
        "    # by reviewing the documentation, construct a forward function that properly uses the output\n",
        "    # of the GRU\n",
        "\n",
        "    # stuff here\n",
        "    \n",
        "    return output, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.n_layers, 1, self.hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hrhXghEPKD-5",
        "colab": {}
      },
      "source": [
        "def random_training_set():    \n",
        "  chunk = random_chunk()\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target\n",
        "\n",
        "def shake_random_training_set():\n",
        "  chunk = shake_random_chunk()\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZpiGObbBX0Mr"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 2: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "We now want to be able to train our network, and sample text after training.\n",
        "\n",
        "This function outlines how training a sequence style network goes. \n",
        "\n",
        "**TODO:**\n",
        "\n",
        "**DONE:**\n",
        "* Fill in the pieces.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ALC3Pf8Kbsi",
        "colab": {}
      },
      "source": [
        "input_size = n_characters\n",
        "hidden_size = 200\n",
        "output_size = n_characters\n",
        "n_layers = 3\n",
        "  \n",
        "decoder = RNN(input_size=input_size, hidden_size=hidden_size,\n",
        "              output_size=output_size, n_layers=n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n",
        "objective = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(inp, target):\n",
        "  # initialize hidden state\n",
        "  hidden = decoder.init_hidden()\n",
        "  decoder_optimizer.zero_grad()\n",
        "  \n",
        "  loss = 0\n",
        "  \n",
        "  for char, next_char in zip(inp, target):\n",
        "\n",
        "    # make prediction, get next hidden state\n",
        "    pred, hidden = decoder(char, hidden)\n",
        "\n",
        "    # compute the loss\n",
        "    loss += objective(pred.squeeze(0), next_char.unsqueeze(0))\n",
        "\n",
        "  # update the network\n",
        "  loss.backward()\n",
        "    \n",
        "  decoder_optimizer.step()\n",
        "    \n",
        "  return loss / len(inp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFFOB86v-2Ar",
        "colab_type": "code",
        "outputId": "c1cc70e8-17ba-489a-9bde-93c7c04116eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "769"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EN06NUu3YRlz"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 3: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "You can at this time, if you choose, also write out your train loop boilerplate that samples random sequences and trains your RNN. This will be helpful to have working before writing your own GRU class.\n",
        "\n",
        "If you are finished training, or during training, and you want to sample from the network you may consider using the following function. If your RNN model is instantiated as `decoder`then this will probabilistically sample a sequence of length `predict_len`\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "**DONE:**\n",
        "* Fill out the evaluate function to generate text frome a primed string\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B-bp-OZ1KjNh",
        "colab": {}
      },
      "source": [
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "  ## initialize hidden state, initialize other useful variables\n",
        "    # your code here\n",
        "  ## /\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    prime_tensor = char_tensor(prime_str)\n",
        "\n",
        "    # Priming\n",
        "    hidden = torch.zeros(n_layers, 1, hidden_size)\n",
        "    character = char_tensor('')\n",
        "    for char in prime_tensor:\n",
        "      character, hidden = decoder(char, hidden)\n",
        "\n",
        "    probs = F.softmax(character.squeeze(0).squeeze(0) / temperature, dim=0)\n",
        "    character = torch.multinomial(probs, 1).squeeze(0)\n",
        "\n",
        "    for _ in range(predict_len):\n",
        "      character, hidden = decoder(character, hidden)\n",
        "      probs = F.softmax(character.squeeze(0).squeeze(0) / temperature, dim=0)\n",
        "      character = torch.multinomial(probs, 1).squeeze(0)\n",
        "      prime_str += all_characters[character.item()]\n",
        "\n",
        "\n",
        "  return prime_str.replace('\\n', ' ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Du4AGA8PcFEW"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: (Create a GRU cell, requirements above)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GFS2bpHSZEU6"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Part 5: Run it and generate some text!\n",
        "\n",
        "---\n",
        "\n",
        "Assuming everything has gone well, you should be able to run the main function in the scaffold code, using either your custom GRU cell or the built in layer, and see output something like this. I trained on the “lotr.txt” dataset, using chunk_length=200, hidden_size=100 for 2000 epochs gave.\n",
        "\n",
        "**TODO:** \n",
        "\n",
        "**DONE:**\n",
        "* Create some cool output\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-nXFeCmdKodw",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import gc\n",
        "n_epochs = 4000\n",
        "print_every = 100\n",
        "plot_every = 10\n",
        "# hidden_size = 200\n",
        "# n_layers = 3\n",
        "# lr = 0.001\n",
        " \n",
        "# decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "# decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xKfozqw-6eqb",
        "outputId": "7d817dc3-a664-4efd-b9ae-fb4a6206c96f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(1, n_epochs + 1):\n",
        "  \n",
        "  loss_ = train(*random_training_set())       \n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "    print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "    print(evaluate('Wh', 100, .6), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      all_losses.append(loss_avg / plot_every)\n",
        "      loss_avg = 0"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[40.44374442100525 (100 2%) 2.3414]\n",
            "Wh the in ward tood 'ard wad of selars ther and sad the ther the       louth the suther sat are share  \n",
            "\n",
            "[76.23829865455627 (200 5%) 2.0883]\n",
            "Wht to that this the mame was stime shist in fore seain and thin in the hat the cac that in the las fe \n",
            "\n",
            "[112.11902451515198 (300 7%) 2.4698]\n",
            "Whs the his coutter sain of to he lurtey sais and the he hind ande the so the for the was has  it wish \n",
            "\n",
            "[148.0571267604828 (400 10%) 2.1934]\n",
            "Wh fror store. 'But stin~ine flook in the sur the din the said and of the coure the urin; sto he the s \n",
            "\n",
            "[183.99544072151184 (500 12%) 2.2435]\n",
            "Whre as more all all and and ale no so the now for here the fire and it and the in the wark, and the w \n",
            "\n",
            "[219.84898447990417 (600 15%) 2.0871]\n",
            "WhnC, and to reant of mes and  streen and head the can were have rom  has the han clound said flet Fro \n",
            "\n",
            "[255.75043559074402 (700 17%) 1.9547]\n",
            "Whn shall should and leace the swere has and lime rours, sordin the shold with in the shall. 'That wil \n",
            "\n",
            "[291.78208351135254 (800 20%) 2.0080]\n",
            "Wht the feel a still the                                                                      hainst o \n",
            "\n",
            "[327.7107284069061 (900 22%) 1.7991]\n",
            "Whd the end the laid us a lowmen the shadow said. 'Tany to the have that the down that the fer the som \n",
            "\n",
            "[363.8392837047577 (1000 25%) 2.1141]\n",
            "Whr hounsered of they with had a on hill  will worn and seement to his said ~own your a could a  sinki \n",
            "\n",
            "[400.1775233745575 (1100 27%) 1.8944]\n",
            "Whre the  the  share then the dil with had near wand the  while we still no steed the will him it stil \n",
            "\n",
            "[436.0824568271637 (1200 30%) 1.6031]\n",
            "Wh the will me foor the $lacked the sundall the (aramint and the vied of the my in the  losted the sto \n",
            "\n",
            "[472.23388838768005 (1300 32%) 1.8760]\n",
            "Wh steed all the many  yes of we  ?' said Frodow lonk and stride  mearin. dow the did it the most the  \n",
            "\n",
            "[508.536967754364 (1400 35%) 1.8558]\n",
            "Whre to the tran8       for, the left the would neaden the mist are to on the muchen too were was a lo \n",
            "\n",
            "[544.6137673854828 (1500 37%) 1.7726]\n",
            "Whre 'he came as the 2ad, and out was will           [ra+ss. Frodo, and was swent  to the sun of the t \n",
            "\n",
            "[580.710351228714 (1600 40%) 1.7273]\n",
            "Whn  was new they had not on a tood held the said and hores of the his could said- and darned   came o \n",
            "\n",
            "[616.9329791069031 (1700 42%) 1.5564]\n",
            "Whre stower strance  at are fermany, and the stead and firch they said Brom we said he shall sheered w \n",
            "\n",
            "[652.8844413757324 (1800 45%) 1.8380]\n",
            "Whnd. 'But you have he dow they                                                       of the word your \n",
            "\n",
            "[688.7999229431152 (1900 47%) 1.8964]\n",
            "Whre 9llow.   'But you went had a little many into the  from the will any are a time was the was and l \n",
            "\n",
            "[724.5436477661133 (2000 50%) 1.8070]\n",
            "Why shadows his  to the  road all one the son of the  could did not seemed the rince they sered the ma \n",
            "\n",
            "[760.5398466587067 (2100 52%) 1.7839]\n",
            "Wht under the roads and that he said sether stronce and to see all shall over for stand.   'There what \n",
            "\n",
            "[796.4524354934692 (2200 55%) 1.6687]\n",
            "Whre to you deen and the  mark is  shire like as  side as not was all they crowinJs of the  was it wit \n",
            "\n",
            "[832.3977043628693 (2300 57%) 1.7981]\n",
            "WhnI they call Xhan his may the  the 7id not alones of the sick.   'Gandalf, and the  ster the foraint \n",
            "\n",
            "[868.3791034221649 (2400 60%) 1.5772]\n",
            "Whre with hat he sent, they willow son, for the males, and the ever lookind have not sicker of they fi \n",
            "\n",
            "[904.3135344982147 (2500 62%) 1.7809]\n",
            "Whnk to the larded to he had a had all in the side with a sine and the moundinus and a sine was  hall  \n",
            "\n",
            "[940.3826849460602 (2600 65%) 1.6092]\n",
            "Whr been at ban the more on that has me touth of his  one of the us had and it with said and first the \n",
            "\n",
            "[976.3972661495209 (2700 67%) 1.7027]\n",
            "Whre was a trees and is if he would he said. 'The was looked and the horse was are you said it was see \n",
            "\n",
            "[1012.6960656642914 (2800 70%) 1.6105]\n",
            "Whr stood the filled the leady about the rain of still  walked in the star it was  be see the fies see \n",
            "\n",
            "[1048.8380811214447 (2900 72%) 1.8213]\n",
            "Whr before the >araned to the ]seen in the east. For all the one and there went that we  could them.   \n",
            "\n",
            "[1084.7547254562378 (3000 75%) 1.6969]\n",
            "Wh down a shall not when the hild and his some the think the land the once hord and hand the jadeness  \n",
            "\n",
            "[1120.7021090984344 (3100 77%) 1.9295]\n",
            "Wht hear have have and desire you should for the      for at litted on the first for be seem with and  \n",
            "\n",
            "[1156.7057650089264 (3200 80%) 1.7277]\n",
            "Wh collowed the by the land us eyes and the since mean cloud, to the more seemed the rose from a sword \n",
            "\n",
            "[1192.5867202281952 (3300 82%) 1.7518]\n",
            "Whn$s of the stormind. All the horse of on his will seemed lands the end the  there to a brow not stre \n",
            "\n",
            "[1228.504052400589 (3400 85%) 1.8249]\n",
            "Wht the to  falled he all he ansence and means to  that the  course far as he well  in  the  Kinin~eth \n",
            "\n",
            "[1264.5574283599854 (3500 87%) 1.5331]\n",
            "Whre lone of the store from the host as  the land to a read the rink him been the hilliness  from the  \n",
            "\n",
            "[1300.7172048091888 (3600 90%) 1.6288]\n",
            "Whre the stood after the  wondle of the now to him and  him, and a turned.   There was a have to the t \n",
            "\n",
            "[1336.7584664821625 (3700 92%) 1.7018]\n",
            "Whre Ladily of what the shall fear slowly before the fore of stood before the men the dead the stand t \n",
            "\n",
            "[1372.7461943626404 (3800 95%) 1.8989]\n",
            "Wht he had down  not and they were lazed follow me,'   they knaws to the dark in the sun behind it mar \n",
            "\n",
            "[1408.777048587799 (3900 97%) 1.4794]\n",
            "Whr and stood come to a courtains  to the men the stank the  best she any norther the could untwards t \n",
            "\n",
            "[1444.5954148769379 (4000 100%) 1.6182]\n",
            "Wht reams                                             3ittle of the day. 'the got you days and they sa \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1oJGHL_zdjo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "4e62b053-74d4-4b00-f4f3-d70ce6c14647"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Lord of the Rings Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot([i * plot_every for i in range(len(all_losses))], all_losses)\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl4VPW5wPHvmx2yEZIAgQBhR3YQ\nQcGFKiqCV1trXepSl+q12uvS9lq9dlGvVq+t1dpFq7V1rfu+4oaKsij7vu8ESMiekD3v/eOcOZkJ\nySSBTBKY9/M8eTJz5sw5b87Aeee3i6pijDHGAER0dADGGGM6D0sKxhhjPJYUjDHGeCwpGGOM8VhS\nMMYY47GkYIwxxmNJwXR6IvK5iPz4EN/7ExHZJyKlIpLagv2vEJGvDuVchxDbSSKyvj3OZUxLWVIw\nbUJEtonI9I6Ow5+IRAN/BM5Q1QRVzWvwepaIqIhEhej8d4pItZuQCkVknoic4HtdVeeq6rBQnDtI\nTO2W9MyRyZKC6TRCcHPuCcQBq9v4uK3xkqomAGnAHOCVDozFmGZZUjAhJyLXiMgmEckXkbdFpLff\nayoiN4jIRmCju+10EVknIkUi8hdAghw7VkQeFpFs9+dhd9tQwFc1UyginzXy9i/9Xi/1/xYvIn8Q\nkQIR2SoiZ/ltTxaRJ0Vkj4jsFpF7RCSyuWugqjXA80AfEUl3jzVNRHb5HXubiPxCRFa4f/tLIhLn\n9/qt7nmzReTH7rUb7L42U0TWiEiJG9cvmoupIRHp7X4++e7ndY3fa5NEZJGIFLvVcX90t8eJyHMi\nkueWhr4VkZ6tPbfpPCwpmJASkVOB+4ALgAxgO/Big92+C0wGRohIGvA68Cucb9ebgalBTnEHcDww\nDhgLTAJ+paobgJHuPt1U9dRG3nuy3+sJqjrffT4ZJ6GkAQ8AT4qILzE9BdQAg4HxwBlAs+0dIhID\nXA7kAQVBdr0AmAEMAMYAV7jvnwH8DJjunntag/c9CfynqiYCo4DGkmBzXgR2Ab2B84HfuZ8fwJ+A\nP6lqEjAIeNnd/iMgGegLpALXAeWHcG7TSVhSMKF2CfBPVV2iqpXA7cAJIpLlt899qpqvquXATGC1\nqr6qqtXAw8DeZo5/t6rmqGoucBdw2WHGvF1Vn1DVWuBpnGTW0/0GPBO4WVXLVDUHeAi4KMixLhCR\nQpwb5TXA+W6poSmPqGq2quYD7+AkO3CSxb9UdbWqHgDubPC+apykmqSqBaq6pDV/sIj0xUm+v1TV\nClVdBvwDJ5H5jj9YRNJUtVRVF/htTwUGq2qtqi5W1eLWnNt0LpYUTKj1xikdAKCqpTjflvv47bOz\nwf47/fbXBq8HPb77uHcT+7aUl4TcGzBAAtAfiAb2uFUlhcDfgR5BjvWyqnbDad9YBRzb0nMDB9zz\nQoPrwsHX5Ps4CWu7iHzhXxXWQr2BfFUt8du2nfrP6WpgKLDOrSI6293+LDAbeNGt1nrAbeA3RyhL\nCibUsnFupgCISDzON8vdfvv4T9W7B6cqwre/+D9v7vhAP3dbS7R2iuCdQCWQpqrd3J8kVR3Z3BtV\ndT9wLXCniGS08rzgXJdMv+cB10RVv1XVc3ES1JvUV++0VDbQXUQS/bb1w/2cVHWjql7sHv//gFdF\nJF5Vq1X1LlUdAUwBzqa+dGGOQJYUTFuKdhsefT9RwAvAlSIyTkRigd8BC1V1WxPHeA8YKSLnue+/\nEegV5JwvAL8SkXS3PeI3wHMtjDcXqAMGtmRnVd0DfAQ8KCJJIhIhIoNE5JQWvn89zrfqW1sYn7+X\nca7jMSLSFfi17wURiRGRS0Qk2a1yK8b5u5oiDT6nOFXdCcwD7nO3jcEpHTznvuFSEUlX1Tqg0D1O\nnYh8R0RGu43txTjVScHObTo5SwqmLb2PU3fu+7lTVT/BuYG9hvNtdxBB6uDdb9Q/AO7HqWYaAnwd\n5Jz3AIuAFcBKYIm7rVlu1dC9wNduddDxLXjb5UAMsAanwfhVnDaHlvo9cK2IBKtyaizWD4BHcLq1\nbgJ8dfqV7u/LgG0iUozT2HtJkMNNIfBzKncT8MVAFk6p4Q3gt+7nB07j92oRKcVpdL7IbQPqhXMN\nioG1wBc4VUrmCCW2yI4xRx4ROQanjSK2mYZrY1rFSgrGHCFE5HvuGIwUnHr9dywhmLZmScGYI8d/\nAjk4YzdqgZ90bDjmaGTVR8YYYzxWUjDGGOMJyeyQoZSWlqZZWVkdHYYxxhxRFi9evF9V05vb74hL\nCllZWSxatKijwzDGmCOKiGxvfi+rPjLGGOPHkoIxxhiPJQVjjDEeSwrGGGM8lhSMMcZ4LCkYY4zx\nWFIwxhjjCZuksH5vCQ9+tJ79pZXN72yMMWEqbJLCppxS/vzZJvLLqjo6FGOM6bTCJilEiPO7ts4m\nADTGmKaET1Jws0KdzQprjDFNCp+kIG5SsNVjjTGmSWGTFCLdv9RKCsYY07SwSQrilhRqLSkYY0yT\nwiYpRLpJwVaaM8aYpoVNUvC1KdRam4IxxjQpfJKCtSkYY0yzQp4URCRSRJaKyLuNvBYrIi+JyCYR\nWSgiWaGKo773kSUFY4xpSnuUFG4C1jbx2tVAgaoOBh4C/i9UQUR64xRCdQZjjDnyhTQpiEgmMAv4\nRxO7nAs87T5+FThNfN2E2pg3otmqj4wxpkmhLik8DNwKNNW82wfYCaCqNUARkNpwJxG5VkQWicii\n3NzcQwrEqz6ypGCMMU0KWVIQkbOBHFVdfLjHUtXHVXWiqk5MT08/pGNYm4IxxjQvlCWFqcA5IrIN\neBE4VUSea7DPbqAvgIhEAclAXiiCsTYFY4xpXsiSgqrerqqZqpoFXAR8pqqXNtjtbeBH7uPz3X1C\nctsWmyXVGGOaFdXeJxSRu4FFqvo28CTwrIhsAvJxkkdI+EoKNqLZGGOa1i5JQVU/Bz53H//Gb3sF\n8IP2iCHC5j4yxphmhc+IZrE2BWOMaU4YJQXnt/U+MsaYpoVNUvC1KVhDszHGNC1skoINXjPGmOaF\nT1KwNZqNMaZZ4ZMUfG0KlhOMMaZJYZMUIsXaFIwxpjlhkxTEluM0xphmhU1SsN5HxhjTvPBJCjZ4\nzRhjmhU2SUFsjWZjjGlW2CSFSBunYIwxzQqbpOBNiNfUGnDGGGPCKClY9ZExxjQrfJKCLcdpjDHN\nCpukYL2PjDGmeWGTFLzlOK36yBhjmhRGSUGIEBvRbIwxwYRNUgCnXcFGNBtjTNPCKylEiLUpGGNM\nEOGVFMS6pBpjTDBhlRQiRaxLqjHGBBFWSSFCxHofGWNMEOGVFCIEywnGGNO08EoKYuspGGNMMGGV\nFCIjxBqajTEmiLBKCiKWFIwxJpiwSgpO76OOjsIYYzqvsEoKEWJzHxljTDAhSwoiEici34jIchFZ\nLSJ3NbLPFSKSKyLL3J8fhyoe8I1otqRgjDFNiQrhsSuBU1W1VESiga9E5ANVXdBgv5dU9achjMMT\nYYPXjDEmqJAlBXWmIy11n0a7Px16R460uY+MMSaokLYpiEikiCwDcoCPVXVhI7t9X0RWiMirItK3\nieNcKyKLRGRRbm7uYcRjbQrGGBNMSJOCqtaq6jggE5gkIqMa7PIOkKWqY4CPgaebOM7jqjpRVSem\np6cfcjyRIraegjHGBNEuvY9UtRCYA8xosD1PVSvdp/8Ajg1lHLaegjHGBBfK3kfpItLNfdwFOB1Y\n12CfDL+n5wBrQxUP2HoKxhjTnFD2PsoAnhaRSJzk87KqvisidwOLVPVt4EYROQeoAfKBK0IYj7Oe\ngmUFY4xpUih7H60Axjey/Td+j28Hbg9VDA3Z3EfGGBNcWI1oFhFqLScYY0yTwiopRArW+8gYY4II\nq6RgvY+MMSa48EoK1qZgjDFBhVdSEGzqbGOMCSKskoL1PjLGmODCKilEiNjcR8YYE0TYJQVrZzbG\nmKaFWVKwEc3GGBNMWCUFa1MwxpjgwiopiI1TMMaYoMIqKTjrKXR0FMYY03mFVVKIiLCV14wxJpjw\nSgpibQrGGBNM+CUFa1MwxpgmhVVSiIywwWvGGBNMWCUFsbmPjDEmqLBKCpHWpmCMMUGFVVKwhmZj\njAkuvJJChFBr1UfGGNOk8EoKthynMcYEFVZJwXofGWNMcGGVFKIiIqiptaRgjDFNCaukEBcdQUV1\nbUeHYYwxnVZYJYXYqEhq6pQaa202xphGhVVSiIt2/tzKGksKxhjTmLBKCrFRlhSMMSaYsEoKcdGR\nAFTWWLuCMcY0JqySQqxbfVRRbSUFY4xpTMiSgojEicg3IrJcRFaLyF2N7BMrIi+JyCYRWSgiWaGK\nByAuykoKxhgTTChLCpXAqao6FhgHzBCR4xvsczVQoKqDgYeA/wthPFZSMMaYZoQsKaij1H0a7f40\nHDl2LvC0+/hV4DQRkVDF5JUUbKyCMcY0KqRtCiISKSLLgBzgY1Vd2GCXPsBOAFWtAYqA1EaOc62I\nLBKRRbm5uYccj1dSsN5HxhjTqJAmBVWtVdVxQCYwSURGHeJxHlfViao6MT09/ZDjibWSgjHGBNUu\nvY9UtRCYA8xo8NJuoC+AiEQByUBeqOKIs5KCMcYE1aKkICKDRCTWfTxNRG4UkW7NvCfdt4+IdAFO\nB9Y12O1t4Efu4/OBzzSEc1tbScEYY4JraUnhNaBWRAYDj+N8u/93M+/JAOaIyArgW5w2hXdF5G4R\nOcfd50kgVUQ2AT8Dbmv1X9AK1qZgjDHBRbVwvzpVrRGR7wF/VtU/i8jSYG9Q1RXA+Ea2/8bvcQXw\ng9YEfDi8Ec1WUjDGmEa1tKRQLSIX41T1vOtuiw5NSKFjcx8ZY0xwLU0KVwInAPeq6lYRGQA8G7qw\nQiMmMgIRKykYY0xTWlR9pKprgBsBRCQFSFTVkI4+DgURITYqwtoUjDGmCS3tffS5iCSJSHdgCfCE\niPwxtKGFRlx0pJUUjDGmCS2tPkpW1WLgPOAZVZ0MTA9dWKETGxVhbQrGGNOEliaFKBHJAC6gvqH5\niBQXHWnrNBtjTBNamhTuBmYDm1X1WxEZCGwMXVihYyUFY4xpWksbml8BXvF7vgX4fqiCCqXEuGiK\nyqs7OgxjjOmUWtrQnCkib4hIjvvzmohkhjq4UOiRGEtOSWVHh2GMMZ1SS6uP/oUzT1Fv9+cdd9sR\nJz0xllxLCsYY06iWJoV0Vf2Xqta4P08Bhz6HdQfqkRhLUXm1NTYbY0wjWpoU8kTkUnfRnEgRuZQQ\nTnEdSj0S4wCstGCMMY1oaVK4Cqc76l5gD84011eEKKaQSk+MBSC31JKCMcY01KKkoKrbVfUcVU1X\n1R6q+l2O0N5HvqSQU2xJwRhjGjqcldd+1mZRtKMeSU5S2Fdc0cGRGGNM53M4SUHaLIp2lBYfS0Zy\nHJ+ty+noUIwxptM5nKQQsmUzQykiQvjBsZl8uTGXPUXlHR2OMcZ0KkGTgoiUiEhxIz8lOOMVjkin\nDEtHFdbtKenoUIwxplMJOs2Fqia2VyDtqXe3LgDsLrSSgjHG+Duc6qMjVo/EOCIjxKqPjDGmgbBM\nCpERQq+kOLILrQeSMcb4C8ukANC7WxzZVn1kjDEBwjgpdCHbqo+MMSZA2CaFrNR4dheUM3v13o4O\nxRhjOo2wTQpXnTiA4b2S+N9316B6RA65MMaYNhe2SSG5SzQ/mtKfXQXlrM4u7uhwjDGmUwjbpABw\n+oheiGBTXhhjjCusk0L3+Bj6dOvCltzSjg7FGGM6hbBOCuA0OH+6Loe/ztlkbQvGmLAXsqQgIn1F\nZI6IrBGR1SJyUyP7TBORIhFZ5v78JlTxNKV/aldKKmr4/ez17C+tau/TG2NMpxJ07qPDVAP8XFWX\niEgisFhEPlbVNQ32m6uqZ4cwjqCyUuO9xzsLDniL8BhjTDgKWUlBVfeo6hL3cQmwFugTqvMdqsS4\n+ry4q8AGsxljwlu7tCmISBYwHljYyMsniMhyEflAREa2Rzz+Zo7JYNaYDAB25h9o79MbY0ynEvKk\nICIJwGvAzaracEDAEqC/qo4F/gy82cQxrhWRRSKyKDc3t03jS4qL5q8/nEBqfAy7CiwpGGPCW0iT\ngohE4ySE51X19Yavq2qxqpa6j98HokUkrZH9HlfViao6MT09PSSxZqZ0YWe+VR8ZY8JbKHsfCfAk\nsFZV/9jEPr3c/RCRSW48eaGKKZhB6Qms31di3VKNMWEtlCWFqcBlwKl+XU5nish1InKdu8/5wCoR\nWQ48AlykHXRXHtu3G7kllSzcms8DH66jrs6SgzEm/ISsS6qqfgVIM/v8BfhLqGJojbF9uwFw0eML\nAPju+D4M7XlUrkZqjDFNCvsRzT7HZCQSE1l/Oc546EtOffBz9pdWdmBUxhjTviwpuGKjIhnfr1vA\nti25ZXy0el8HRWSMMe3PkoKf47K6H7St4ED91BcV1bXklljJwRhz9LKk4GfK4NSDthWVV3uPr3lm\nEcfd+0l7hmSMMe3KkoKfKYPS+OCmk7znvZLiKCirLynM3bgfgMqa2naPzRhj2oMlhQaOyUji41tO\n5sVrj6db12gK/UoKPoUHDt5mjDFHg1DOknrEGtIzkSHgJIUDB0+nXXCgip5Jce0fmDHGhJiVFIJI\n6RrjlQpKKupLBwVlVlIwxhydLCkE0a1rNAVuUthdWD8vUlG5LcZjjDk6WfVREN26xlBUXsWHq/bw\n+fr62VkLrE3BGHOUsqQQRLcu0VTXKtc9twSAPt26sLuwPGDsgjHGHE2s+iiItARnac7U+BjOPzaT\nJy6fSGxUhPU+MsYctaykEMSsMRnERUcyfUQPYqMiAafx2X/sgjHGHE2spBBEXHQks8ZkeAkBICU+\nhpySSmpq6/jd+2v5+xebOzBCY4xpW5YUWmnygO7M35zHgx9v4PEvt/D72es7OiRjjGkzlhRa6dLj\n+1FVW8ejnzslhJT4mA6OyBhj2o4lhVYa3CORQenx3vP8sireWZ7NqX/4nAc+XMfW/WUdGJ0xxhwe\nSwqHYGTvZMDplVRbp/z3q8vZsr+Mv32+mcc+b7yN4dtt+WTd9h5LdxRQXGG9l4wxnZMlhUNwXFaK\n+9tZf6Giuo6HLxxH3+5dWLg1j3vfW0NtgzWe31i6G4Dv/W0eE+7+uH0DNsaYFrKkcAgumdyfZ66a\nxOVT+nvbpgxKZXSfZLblHeCJuVvZsK8k4D3+i1XX1ClFNtbBGNMJWVI4BBERwslD00l3B7cBpCfG\nkpHcxXu+JTd428KXG3ODvm6MMR3BBq8dhjS/pCAi9O5WnxRu+PcSnl+YSnpiLNeePJC1e4oD3rsq\nu4j/GNu73WI1xpiWsKRwGJK7RAMwNtNpeM5IDlxjYUtuGfM25/HWsuyD3ru/xEZFG2M6H6s+OgwR\nEcJHt5zMcz+eDEBCbGCO9W1vKCYygtzSSu58ezXXP7+4yeMv2JLHzD/NbXShH2OMCQUrKRymoT0T\nvcdTB6dxy/ShpCXGsLeogsE9Ehp9z+jMZHKKK1i7p5jckkqW7SxkXN9uAfuoKhc9vgCAjTmlXk+n\nUHpl0U76de/K5IGpIT+XMaZzspJCG4qMEG6aPoRLJvfn52cMA+BPF41jUoMb+pAeCazbW0JuSSVA\no2Mb/AfB+fdUqq6to6a2LhTh8+BHG3hmwfaQHNsYc2SwkkKInTuuD+eO68MHK/cwMas7XWMiAybR\n+86wdGav2cvm3FIGpdeXLPy7tPqv3zDt95+TGBfFhzef3OaxllbWUFZZ0+bHNcYcOayk0E7OGp1B\nemIs8bFRpCfW91q6//tjiImM4PEvtnjbPl27jxtfWOY9LzxQza6CA3zvb1+zu7CcdXsDx0C0hbo6\npbSyhtIKSwrGhDNLCh2gS4xTQJvQrxs9k+K4YGJfXl+6i/yyKmrrlKufXkSVW0UUGSEUHKji7eXZ\nLN1R6B3jsicXku23bnQwry/Zxfsr9wTdp6zKSQalVlIwJqxZUugAk7K6ExUh3DFrBAAXTOxLda3y\nydp9fLYux9svK7Ur3bpEU3Cgmn1FFQHHmLtxP1Pu/4wHPlznbTtQVdNoe8PPXl7O9c8vCRqTLxmU\nWEnBmLAWsqQgIn1FZI6IrBGR1SJyUyP7iIg8IiKbRGSFiEwIVTydSb/Urmz63UyO7e/MoTSqTxKZ\nKV3419fbuO+DtfTr3pW3fzqV5685nm5doykqr2Ld3hK6xkQyuk9ywLGemLuFovJqamrrGPGb2dz6\n6grA6b302uJdXmO2b1tTfNVGVlIwJryFsqRQA/xcVUcAxwM3iMiIBvucBQxxf64FHg1hPJ2WiPCT\naYNYt7eYrfvL+J+ZwxmT2Y0+3bq4y39Ws35fCeeO683tM4cHvLe6VvlkzT4+X+9Mm/H60t2oKquz\ni/n5K8u5/fWV3r57GpQ2/JVU1ieFYMnDGHN0C1nvI1XdA+xxH5eIyFqgD7DGb7dzgWfUuQstEJFu\nIpLhvjesXDK5P5MHpBIdKfRPrV+voVvXGD5Zuw+AsZlOG4S/mKgIPli1h65uO8XA9Hge+mQjj3y6\nEcB7L8Dm3FIA3ly2mzNG9AoYR+ErKdTWKRXVdXSJqV+C1BgTPtqlS6qIZAHjgYUNXuoD7PR7vsvd\nFpAURORanJIE/fr1C1WYHa6xwW4pXZ2pNOKiI5g5JoMIqZ9v9bThPejbvSv/XriD+FjnJr6/pJJn\n529r9PhvLs3m4zV7Ka6oYWd+Ob85ewSvLN7J5AGpAdVGJZXVXlLYnFvKF+tzueC4vgeN2DbGHH1C\n3tAsIgnAa8DNqlrc3P6NUdXHVXWiqk5MT09v2wA7uZOHptMrKY4rpw4gKS6ahNgoLj2+H/++ZjJP\nXnEc/zG2N1W1dRQcqCa5SzTFFTUkxjmJ5LFLj2WYO+I6KkJ4bckuqmrrSE+MZXNOKTe+uJTfvLWa\nv32+KaArqu+xqvKzl5dz97trAqqhjDFHr5B+9RORaJyE8Lyqvt7ILruBvn7PM91txvUfY3sfNJvq\nPd8d7T2e0K9+eowzRvTklcW72JF/gCumZDFjVC9iooTP1+fy7bYC1u4pZkBaAuP6JvPakt1U1Tg9\nlfLLqrw2BahvbJ6zPoflO51usKuzi5qNVVV5ZdEu1u0t4RdnDvWqtMAZoX3pPxby1JXHMcRvapC2\nsr+0kr1FFYxq0BBvjGmdUPY+EuBJYK2q/rGJ3d4GLnd7IR0PFIVje8LhEBEeu/RYBvdI4MyRvbzt\nvbs5bQ+nDu/J3eeOol93Z1rv/t27MrhHIlU1dcTHRHLCwFT2FlUcVFJQVR78aAP9U7ty+Qn92VNY\n4TVAb8oppaSRJUXfXbGHW19bwT+/3sr1zy9h0bZ89pdWMm/zfp6Yu4XdheW8unhXSK7DDx6bz9l/\n/soayY05TKEsKUwFLgNWiohveO7/AP0AVPUx4H1gJrAJOABcGcJ4jlozRvVixqhebPObL6lPt64B\n+/gar/undWWI23Zx6fH9qaiuZf6WPP72+SZv37yyKj5Zm8Pq7GIe+P4YiiuqKa+upai8mqS4aKb/\n8QuG9kzgo1tO8d6jqvzho/WM7J1E35SufLh6r9cjCpw2EXBKDLsLy+mdHIeI/3p0h8c3V1RuaSU9\nEuOa2dsY05SQlRRU9StVFVUdo6rj3J/3VfUxNyGgjhtUdZCqjlbVRaGKJxz0T61PBBndAm+Mfbs7\nr/XvHs/kgd25ZfpQrp82mJ7uGhCVNfWD3u56Zw3XPON8FP8xtre3olx2YQX7SpxurRv2lfLxmn28\nvMjpJ7App5TteQe4eFI/HrxgLK9fP4WbThviHbOi2jn+R2v2MfX+z/iT2zsKYOmOAo7/3afklTpj\nKmrrlOzCcs79y1fc9/5aVJWSimpufnEpuwoOBL0GF/59gVflZYxpPRvRfBQREd79rxOZNiyd4b0C\n6+19Dc7DeiUQGxXJTdOHkNw1mp5+36pvPG0I0ZHCfvfmfMfMY+gSE+klmD1F5QGzt17zzCJufXUF\nm3JKOePhLwE4dXgP4mOjmNAvhVtOH8qqu86kR2IsEQJ/vGAsEW7hYN6mPO84f52zib3FFXy1aT/g\nDMibcv9nLN9VxN+/3MK32wr4elMeby7L5ifPNT4yOyaqviRy5zurve0FZVV8uSGX3YXlZN32Hgu2\n5AW8b/ofv+CGfwcf7R1q76/c413zppRUVHP2n+eydEdBO0VlwpUlhaPMqD7JPHXlpIBGXoDjslJ4\n78YTObZ/4DTePZKcyfkGpcfzs9OHMrxXEgDXTxvENScPBKC3r6RQVMHmRtaefmb+NlSdUoX/kqTg\nLDz04c0n8+0d0zlvQiYb753JFVOyWJVdRG2dU//vmwvKl3DWZDud1K47ZZBz3sJytux3xlis3F3E\nAx+uY97m/QHnifcbV9Gve32J6Z731nL5P7/hyblbAXh2fuDU4JtySnlvRX0zVlVNXYvaJcoqa3jh\nmx2H3YZRVF7N9c8v4eqngxeSF2zJZ9XuYu57f13Q/TrKm0t3M/q3s73OC53Z7sLyZkuc4cySQpgQ\nEUb2PrhnTh/3Jn7xJGf8h2+J0WF+JY30xFhS42P49Zur+PWbq4DAVeZe+GYHp4/oyZ8vHt/oubvH\nx5DqrmcdGSGM7ZvMgapaNuU4N/p9xU6V1KOfb+a7f/2ad1Zkc1xWCtd/x0kKuSWVbNxX6h3vb59v\n5odP1A95UVXKKmu957FuqaGqps6bNPCfXztJoaau/qblP014dW0dOcUVDP3VBzy3cMdBf8P5j87j\nD7PXe8/vfX8tt7++knmb8w7atzV8q+ptzill/uY85jdxPN/Aw9jozvlf9s53VlNSWRMwzXtndccb\nK/nlays6OoxOq3P+CzPtZmB6AvNvP5WrTxwAwK/PHsFxWSlMG9rD2ycyQnjx2uM5a1R976bLT+jv\nPa6uVU4/pmeLz3lsP6e04pu51ddAXllTx7KdhahCz6Q4EmOjiIuOYNmuQt5YupupgwNXhPN9Ky2r\nqqWqto6BaU5jelF5NXV1ytBffcD8BtVFNbX13+z3FtdP+7F1fxkPfeK0czw5d0vAewrKqli0vYC/\nzNlESUU1qsoeN9k0NldU0YFqvt2W36JrkV/m3ESra+u4+IkFXPzEgkb3W+2WnvaXds6bbp1b6isu\nP7hXWmeTW1IZMCeYCWRJwZDMQTuNAAAb00lEQVSR3MXrCTSsVyKvXDeFZHcktc+Qnok8eumx/PvH\nk3n6qkn84oxh/PWH9fMXjs5s+fiAfqldmTGyF//8aiuLtuWTU1LJpcf349FLJtDX7TrbK8npnZSe\nGOtV7xzbL4XeyfVtIEN/9QGLtxew0V2Q6LppgzguK4Xi8hr2lzX+n/7TdTlc/dS3AAEzz36xPpfX\n3O6y+4orA6pB1u6tH3M54+G5TL3/M2rcm6B/V966OuWON1Zy5sNf8oPH5pNTfPBcUw2rmwrdVfV8\nxwO48O/zueONlQH7+saJbMkt9ardOhNfRIVHQFIorazpkNmA80or+f6j81o85X1HsaRgWmXK4DRO\nGZpOhFsN5OO/alxL3D5zOAlxUZz/2HwAThvek7NGZ5Dldp3t5d7809xqp0lZ3bnh1MG8d+NJ/OvK\n47zjfP/ReXzvb/MA6N41hqS4aIorqtnrd8Of6M5G6/Ppuhx25h/g2231jbb3vr+Wqto6/mfmcMqr\na/l4Tf2cUWv31C9qtLuwnOyiCuZudNo0/BuIv9iYy/MLd3glEF/Duc+bS3cz4Pb3ySutRFVZk13s\nVbf43+gXbs3n+YU72JRTytl/nsuKXYXsyDtAWkIslTV17MhvWX34vM372dnCfQ+XL38VHuj8SaGk\nomOSwutLdrN4ewFPfrW13c/dGpYUzCHzX0HO1/unpfqnxvM/M4/xnp8wyKka6uo2GKd0jQGg0u3K\n+t3xfYiNiiQlPobvDOvBv6+ZzH3njea74+pHeyd3jSapi5MUfDPCnn9sJrecPvSg8//XC0t56JMN\nADxy8XhioyL41axjuGrqAEZkJHHPe2vILankwr/P57XFu0hLiPHGd/jzTwovfbMz4LUvNuTyzdZ8\nLntyIRXVtdzysjNcZ/H2Ah79YjMzH5nLHL+xHA29sXQ3q3YXc+0zi6mpU84ekwHAsp3BeyCVV9Wy\ndEcBP3xiITPcXmE+T329lQvcRNxaqtpkkqlzs0LhIbQpVFTXcu97awLWIm/oP59dxM9fXt7qYzem\ntKKG0sqadi9x+YbldPbxlZYUzCGLjXJu4DGRh/bPaLrbDpGWEEtctHOsAWnOjdc3wV+ue9Md0Tsp\n4L1TBqVx8aR+PHzReJ668jj6du/C4PQEkuKiKC6v8UoKv5wx/KCbedeYSJb5jWU4Z2xv1tw9gx+f\nNJCoyAguPK4ve4oqeGvZbhZuzWfNnmJOH9GTaPfvjIyoH3TnX8fvv642wEer93HLS8uYu3E/zy3Y\n7t0Mrn12MQ986DRafxBkRby1e5xqK1/J47RjehAfE8k7y/dQUNb0zff3s9d7paeyqlqvWgzgznfW\n8M22fCqqa5t6e5NeW7Kbkx6Yw6JG2kt8SaHoEKqP3l6WzRNztwaMXWlo9up9vLZkF6rKrEfmcs5f\nvmr1ecBJQL5VDdt76VlfFW1dJ88KlhTMYXnzhql8ceu0Q3pvl5hI3v7pVN7+6VRv283Th/C7743m\njBFOo/b0Y5wG72FB5kuaNqwHc289lZT4GJK6RFNSUU12UTnRkUJqfIxXohmTmcy2+2cFzI/0wjXH\nA4E3el+7hm/a8WE9E/nFGcO87b5v7OA0WvrWoPBvuB7aM4Hy6lp2u/XHD3/S+A2vJsi31UXbA0sE\nWanxDOqRwGfrcvjlayt47IvNnPL7OagqdXXqTT2yMScwOf38leVsyS0N2LaroPX12uvcJNWwh5Sq\neg34h1J9dMBdCra6kVUDG9pVUM7q7GJW7Gp+Li5/ziJUH/LUvG3etuJGpmppSxXVtUz+3Se8tcyZ\nzs33L6yzT8ViScEclnF9u3kjng/FmMxuAWMb4qIj+eHkfkS4N+m7zhnFgttPa/H6Dklx0dSp08Wz\nZ1IcERGCiPDJz07h2asnA5DhtlcMTI/3qq38ZaY44xwWbMnnmIwkZt9yMqkJsdx33hj+99yRzBxd\nnxS+2rSfUb+dzZNfbeVAVf2375OGpAe0Zfh6KZ01qhci8NK1x/Mjvx5cDfVMij2o3jsjOY4bT3VG\nic9Zn8P9H6xje94BdhWUc98Haxl950cUlVd754qMEP7zFGesScNSTGvaGg5U1bC/tJKUeKdKL7vB\nYk2llTVecissd0owD328gcuebDhTPuSUVLCwYY8w973+idmff6mmuV5d//xqK/P82nLW7S3mzIe+\nZMO+Ug5U1XL/B/XjPPLKqlo00SM4CauuldVN2/LK2FdcyYItTszl7t/RCfsJBLCkYDq1mKgIr9G5\nJZK6OOMnPlmb443BAGetCt8YjF7uQkU9/NpE/Pm/b0RGfbVV9/gYLjshi/HuzLTd3ZskwNPztwH1\nA+fiY6P43XmjyUzpwiWT69cA+csPJ7D27hlMHpjKDyb6TxDsOC4rhTeun0JfNzFlpXYlKS6KmKgI\noiIjmD6iJ//73VFU+3Wt/Xx9Dk+4g/PeWLKLLbllfHdcbxbdMd2bamTjvlIqa+pvrlc+9S2frdtH\nY5xxH/UJ6T+fXczEez7xupvuyA8cwFhQVv+N21dSWL6rkAVb8g5aM/wHj83nwscXBGz3JT9fnXt5\nVS3PL9zOp25Jzb/0sWp3fU+wht+4N+wr4e531/DDf9Qnoz/M3sD6fSW8sfTgiRgvf3Ihsx75qtkl\naKtq6ph4zyfc9NKyoPs9O39bQLvHFnegp2+Mie+a+n8OnZElBXNUSYit70p721nDG93Hl2Sim2gL\nifcbmDelkZJEj8Q4tt0/i9k3n8xtZw0nJjKCnflOdYyvFIIqQ3sm8tUvT+Wi4+qTQmSEeO0no/ok\nkxQXxRkj6sd43H3uKMb3S2Fs325eLPNvP415t53q7TO+b/106QC/fms1URFCYmwU97y3lqLyasZk\ndiMlPoauMVFkpnRhY06pN1jQ56qnFrF8ZyF1dcpby3Z7Da8PfrSBkb+dzaacErJue8/raTVnfQ7g\njJlYsCXPq67K8+v+u25vCXV1SkFZFdW1ys6CclSVlxftZN6m/WzPc0oo+/zGCfje70sOby/fzR1v\nrOLqpxehqt5YDoAlftN8NLyZP+1XNZRbUsnvZ6/zqgD3Fh/cRbnYPV9jYxb+9vkmZj0yF4DnF26n\nqLyad5ZnH7Sfv5cX7QpY6dBXZef77Yu3I3o+tYYlBXNUGdevGycNSePDm09ifL+URvfxlRRa0uA3\ny6/9oKH0xFiuO2UQV56Y5W0b447X8I3gBhjaq+nuukt+fTp/v+xY73k3d3zIzNFOm8rq7GLiY6O8\nrrkAx2Qkcenx/bhj5jFMynIGAl4/bRBP/GiiVxUzMav+bx/SI4G3l2cz65HAxtmkuCgufmIBN720\njJteXMYz87eRX1bFX+Y4M+a+viRwaZMN7qjywgPVXPT4Au56x1lZ19du0q97VzbllPLwpxvJK6sf\nqf3M/O3c+uqKgG/wu/yqr/LcxvpXF+/ilN/PCVhLfG9xBSt2OZ0CslK7BnQQyG/Q2D5vc57X7vPW\nst38dc5m77Vg1UR5pZW8tWy3V021r7iCBz5cz+rsYqpq6li/16l6y/Arsd704lJmr94LwI68A+wr\nrmDNnmJKKqq9aiZfSWF/aRX5ZVVevCUVNfxj7ham3v8ZK1vZNtIebH1Fc1Tp062L13bQlK5uSaAu\nSLvmv648jqqaOu9bfTD+04fcNH0oQ3omct74Pt622KhI7j9vNEN7HdxYHtWgtOKr4hrfN4W0hFiu\nc9sE/EVGiLfQ0oWT+lJQVuVNjf7E5RPpmRTLmMz60sQPJ/f3ur7GREXwzk9PJKNbHM8v2MH/fbjO\n+wZ81ztrvBs9wJcbD+4uOzYzmeXujeyTtfuoq1Nv3MRjlx7LzEfmsia72LsBrt1TzNPzt5HcJTqg\nZ9JVT33LM1dPZnNOKR+s2utt3553IKD949731vKuO3jxhEGpbMvzSybu3723qIKqmjq27i/j9rOG\n89m6HB79vD4hQP0NujHvr9zLP7/eyuUn9Ocn0wZxwn2fea9lF5Z7JYm8sipUlYID1by1LJu3lmXz\nzR2ncfLv55AYG+WVtMqqnNUP1+8roUt0JOXVtUz4348D4r7nvbUAzF69t1UDP9uDJQUTdsZmJpMY\nG8V/nTa4yX2+M6xHk681NGNkLy6Z3I/iihoSYqO4oJG2gosmBV9b/KbThvCnTzfSxU1CERHCol9N\nb/bcSXHRJMXVV5mdPuLg6UZOH9GTj285mZT4GFLjY7yukT+ZNoi9ReU83WCSQHC6CfvX3/t0j4/h\n05+fwvsr9vDgxxtYnV3MzvwDpCXEMKJ3EtOP6cmmnBKv0f2pedvIK6vimasm8bOXl3vjOsqqarnp\nxaWN9oJ6f+Ve0hNjyS2p9BICON2QX/AbC5JfWsVby3bz36+u8EagT8zqTp+ULvz030ubvXY+vhHG\nH63eR3pCYDvTroJyr1t0VU0dG3NK+fsX9dOg/PUzp1Tlv3JhUXk1B6pqWZ1dzM9OH0rBgSr+9fU2\n73VfV2NwGs6ra+v4w0frmTU6g/2llXTrGsOEJkq57cGSggk73brGsPKuM9vseDFREdz7vdHN7xjE\nLacPbXSQXVtpagnUprafMjSd15bUN876SggJcdEMSk/g9JE9efDjDWzPL2NH/gGvx1a/7l0D6tXz\nyqoQgUkDunPXOSMDpikP1i12SI8EBMjxq+8/eWjg+uwvfLODT9flMLxXIqP6JFNVU8eYzOSAyRpv\nOm0I763cc1B7CsAPjs3klcW72FPkxLG3uIIHP94QsM/OggMBbQ7XPbc4oNTx4eq9NFRcXuNVc505\nshfDeiWSGBfNIw3GYUwa0J1lOwtZtK2Av3+xJSDZbL1vZpsuQtUa1qZgTBhranqSPg0WaTrDXeq1\n3C0B+LoRZxeWsyP/gNfrylenD5Dq9s7KTOlCXHQks8ZksOGeswIWgwJnbErDEfHpibEBM/VCfdWa\nz6frcuidHMdbP53KH34wlkcuHk90ZETA8U8f0fOgXmYpXaPZ/LuZPHD+GJLiooJWLd3++kr2FFV4\n65M0HDS4r7iSU9xkNSi9fkLGj9fspV/3rgzt6Vxf/+ncff5jbG8qa+r4cNXBAxiXN2hrOFBV02wv\nqbZiScGYMDaoR3yj23sk1SeFbffPYrQ74M834CspLpqE2Ch+9/46duaXMzzDuWn63/x8jd3+iScm\nKoLZN5/MA+ePAZwb483Th3qDFM8c6VR/dY+PYahbijlpSBor7zzDicu9wWemOMnn5KHp3sh6H/92\noJ5JcazcXeTGEe9ti3THr6QlxAZU/TTFSwqNDM675fShxERFcM5Ypx1pb3E5X2/OY/oxPb1v+w0T\nYWJclBfPR2sO7hr8SYNtp//xS0b9dnazcbYFSwrGhLH0hFiuPnEAJw5OC9je8Nu1bzGmpLj6qhlf\nb5zJA7p7U68P9auOOs7tGTUwLbA0EhcdyTlje/M/M4dz/3lOtdt9543hX1ccx5RBThxllTXeKPa+\n3buS6LabvPtfJ/L0VZO4aqpzvskDAxeNaig1Poa7zhnJ2MxkZrhTv/vP2ZWaENPo+6YOTg2YBXhY\nr/rxKmeO7Ml7N54IwPfG92Fc324s+fXpnDfBSQrPLdhBVU1dQPuOf7K8dcYw7v3eaG/xqj1FFQdV\njS3ZUYCq8sZSp3rL18PLN/o7lKxNwZgwJiL8+uwR1NWpM0vsGys5Y0QveiYFVh8N65nIvd8bxZkj\ne/m91/k9Y1Qv79t63+5d+d9zR/LitzuZNqwH976/9qClYcFJDNeePMh7ntwlmu8M7+Etl5qR3MXr\nreU/mLBHUhw9kuI4eUgaw3olcsLAg8eRAJw7rjfvLM8mIkI4b0Im503I5LEvNnvn8klLOHgA45Ae\nCTz/Y2f6kw9WZfDuij0B3VF7JcUxsncyb1w/xevlleDX+2jx9gKOy0ph8oD6hOVrwP7vM4dx/TSn\ng4P/SO1xmcl8ucHp7XXa8B7M35LH8wt38Ks3V3HO2PpJH5ftKGRKgwTe1iwpGGOIiBDiIiL54wXj\nAAKmHgcneVwyOXBajuxCZx9ficDnshOyuOyELADeumEqx2QETmYYzPEDU3n26klMHpBKnSqnDe/h\n1dk3jGdqkJvjwxeO408XBa4E6JuB178h2lcNBc6ULQ9dOC6g9HDXOSPpHh/DmSN70TMpln3FlfR0\nE0TDcTCJfse98bQh3lQt4FzfbffPCtg/LjqStIQY9pdWBczHNXN0Bp+uy+Ge95zuwf5jLBZuzQ95\nUrDqI2PMQdKaqFbx94szhhIhNFoS8BmT2a3JkeNNOWlIOjFREcRFR/LkFccF3DBbqrGeO75v5v7r\nl/d1q3VS42N484apDEiLD+jim5oQy93njqJLTKSX3Bo2ePv4J4GGibIpvnnDRvVJ5lezjuGkIWlM\ncksYFdV1JMZFeeuipyXEBE2EbcWSgjHmIL5BdcFu+FdMHcCW+2YdNACvs/Kt450QW98Q7SsptGTG\nVN/4E99CUMG0ZNCj7/xpCTFkJMfx45MG8uzVk8lM6eK1e/jaagBuP+sYL2GEklUfGWMateD200iI\nO3puEedN6MML3+wImIjQN/Gg/wSDTZk5OoOvbzs1oI2jodevn0L3rs2Xsnx+ceYwCsqqAko2IsKx\n/VL4cPVevj8h05t2vXsLSm9t4ej5xI0xbao1s9MeCfqnxvPNHYGjxPuktG7a92AJAWj1SORB6Qlw\ncJMJV0zNYnCPBK96C+rHfYSaJQVjTNjytS/8cHLwaUja2/EDUzm+Qc+qlFaUQA6HJQVjTFjryCkl\nWiIuOoKK6romx1S0NUsKxpiw1pkTAsBbN5zIp+v2BfSaCiVLCsYY04kN65V40DxQoRSyvmQi8k8R\nyRGRVU28Pk1EikRkmfvzm1DFYowxpmVCWVJ4CvgL8EyQfeaq6tkhjMEYY0wrhKykoKpfAvmhOr4x\nxpi219FDEU8QkeUi8oGIjGxqJxG5VkQWicii3NyDlwg0xhjTNjoyKSwB+qvqWODPwJtN7aiqj6vq\nRFWdmJ7eyEgPY4wxbaLDkoKqFqtqqfv4fSBaREI/25MxxpgmdVhSEJFe4nYQFpFJbix5HRWPMcaY\nEPY+EpEXgGlAmojsAn4LRAOo6mPA+cBPRKQGKAcuUtXmZ6UyxhgTMnKk3YdFJBfYfohvTwP2t2E4\nbamzxmZxtY7F1ToWV+sdamz9VbXZRtkjLikcDhFZpKoTOzqOxnTW2Cyu1rG4Wsfiar1Qx9bRXVKN\nMcZ0IpYUjDHGeMItKTze0QEE0Vljs7hax+JqHYur9UIaW1i1KRhjjAku3EoKxhhjgrCkYIwxxhM2\nSUFEZojIehHZJCK3dcD5t4nISnftiEXutu4i8rGIbHR/p7jbRUQecWNdISIT2jCOg9a5OJQ4RORH\n7v4bReRHIYrrThHZ7bfmxky/125341ovImf6bW/Tz1lE+orIHBFZIyKrReQmd3uHXrMgcXXoNROR\nOBH5xp3ocrWI3OVuHyAiC91zvCQiMe72WPf5Jvf1rObiDUFsT4nIVr9rNs7d3p7//iNFZKmIvOs+\n77jrpapH/Q8QCWwGBgIxwHJgRDvHsA1Ia7DtAeA29/FtwP+5j2cCHwACHA8sbMM4TgYmAKsONQ6g\nO7DF/Z3iPk4JQVx3Ar9oZN8R7mcYCwxwP9vIUHzOQAYwwX2cCGxwz9+h1yxIXB16zdy/O8F9HA0s\ndK/DyzizFgA8BvzEfXw98Jj7+CLgpWDxHuZn2VRsTwHnN7J/e/77/xnwb+Bd93mHXa9wKSlMAjap\n6hZVrQJeBM7t4JjAieFp9/HTwHf9tj+jjgVANxHJaIsTauPrXLQ2jjOBj1U1X1ULgI+BGSGIqynn\nAi+qaqWqbgU24XzGbf45q+oeVV3iPi4B1gJ96OBrFiSuprTLNXP/7lL3abT7o8CpwKvu9obXy3cd\nXwVOExEJEu8hCxJbU9rlsxSRTGAW8A/3udCB1ytckkIfYKff810E/w8UCgp8JCKLReRad1tPVd3j\nPt4L9HQft3e8rY2jPeP7qVt0/6eviqaj4nKL6uNxvmF2mmvWIC7o4GvmVoUsA3JwbpibgUJVrWnk\nHN753deLgNRQxNVYbKrqu2b3utfsIRGJbRhbgxjaOraHgVuBOvd5Kh14vcIlKXQGJ6rqBOAs4AYR\nOdn/RXXKgB3eP7izxOF6FBgEjAP2AA92VCAikgC8BtysqsX+r3XkNWskrg6/Zqpaq6rjgEycb6vD\n2zuGpjSMTURGAbfjxHgcTpXQL9srHhE5G8hR1cXtdc7mhEtS2A309Xue6W5rN6q62/2dA7yB859l\nn69ayP2d4+7e3vG2No52iU9V97n/ieuAJ6gvDrdrXCISjXPjfV5VX3c3d/g1ayyuznLN3FgKgTnA\nCThVL75Zmf3P4Z3ffT0ZZwr9kP4b84tthlsVp6paCfyL9r1mU4FzRGQbTtXdqcCf6MjrdSgNEUfa\nD84U4VtwGmB8jWkj2/H88UCi3+N5OHWQvyewsfIB9/EsAhu4vmnjeLIIbNBtVRw436a24jSypbiP\nu4cgrgy/x7fg1JkCjCSwUW0LToNpm3/O7t/+DPBwg+0des2CxNWh1wxIB7q5j7sAc4GzgVcIbDi9\n3n18A4ENpy8Hi/cwP8umYsvwu6YPA/d30L//adQ3NHfY9WqzG01n/8HpSbABp37zjnY+90D3A1sO\nrPadH6cu8FNgI/CJ7x+W+4/wr26sK4GJbRjLCzjVCtU49Y5XH0ocwFU4jVmbgCtDFNez7nlXAG8T\neMO7w41rPXBWqD5n4EScqqEVwDL3Z2ZHX7MgcXXoNQPGAEvd868CfuP3f+Ab929/BYh1t8e5zze5\nrw9sLt4QxPaZe81WAc9R30Op3f79u8ecRn1S6LDrZdNcGGOM8YRLm4IxxpgWsKRgjDHGY0nBGGOM\nx5KCMcYYjyUFY4wxHksKxrhEpNZvpsxlhztjaINjZ4nfDLDGdFZRze9iTNgoV2cKBGPClpUUjGmG\nOGthPCDOehjfiMhgd3uWiHzmTqT2qYj0c7f3FJE33Hn7l4vIFPdQkSLyhDuX/0ci0sXd/0Zx1kVY\nISIvdtCfaQxgScEYf10aVB9d6PdakaqOBv6CMxUCwJ+Bp1V1DPA88Ii7/RHgC1Udi7NGxGp3+xDg\nr6o6EigEvu9uvw0Y7x7nulD9cca0hI1oNsYlIqWqmtDI9m3Aqaq6xZ2Ebq+qporIfpxpJKrd7XtU\nNU1EcoFMdSZY8x0jC2eq5iHu818C0ap6j4h8CJQCbwJvav2c/8a0OyspGNMy2sTj1qj0e1xLfZve\nLJw5diYA3/rNjmlMu7OkYEzLXOj3e777eB7OTJUAl+DMugnORHk/AW9Rl+SmDioiEUBfVZ2DM49/\nMnBQacWY9mLfSIyp18VdlcvnQ1X1dUtNEZEVON/2L3a3/RfwLxH5byAXuNLdfhPwuIhcjVMi+AnO\nDLCNiQSecxOHAI+oM9e/MR3C2hSMaYbbpjBRVfd3dCzGhJpVHxljjPFYScEYY4zHSgrGGGM8lhSM\nMcZ4LCkYY4zxWFIwxhjjsaRgjDHG8//60EOlDB0LlQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OimTh2rx7KM1",
        "colab_type": "text"
      },
      "source": [
        "## Lord of the Rings Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ee0so6aKJ5L8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "f01dc800-2ff8-4fb0-9ace-142957620e38"
      },
      "source": [
        "for i in range(10):\n",
        "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
        "  start = random.randint(0,len(start_strings)-1)\n",
        "  print(start_strings[start])\n",
        "#   all_characters.index(string[c])\n",
        "  print(evaluate(start_strings[start], 200), '\\n')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " lo\n",
            " log bore a my have if   thriends on it  come water. 'He as make a reed that was in discouting was wanged, and they went a mow have your drang  we have more got them, and could. The are use $ it others o \n",
            "\n",
            " ra\n",
            " rags to his than if a sheelest ever undelly.'   ') down to good as it a door to deem, and side. @be a looking to shifter and side have head and ready, and well followed, he with the Kings and tried of t \n",
            "\n",
            " G\n",
            " Gndalf remen than  a great now courselfuling that could as a stood stood the eard. The got flower the Brom they strees, and they came wall.'   That foven with a loed.   That hobbit in the  made holdshe \n",
            "\n",
            " he\n",
            " herter  any a followed that the ! must leavy his way for a dead, and stood going that  <urneys; and days of they surned to gragon; mey falling aboutly like enetcher. Then shut gone dirce a biright  take \n",
            "\n",
            " wh\n",
            " whte For mean to  me to reat norting in then to the +ar cragch. 0ear talks at a battle. Feady learned steeburce \\ank of it old dried-Brou%d and any tried he as one  and through Xame  was stood. The grin \n",
            "\n",
            " lo\n",
            " lok good a cloom and in with the battle and cry with  that you sill  of the been to the >arry, anQuess, and the end grower that good, he will me to day and lite got  at lettlanF   anyway to any made goo \n",
            "\n",
            " wh\n",
            " whtter four same would not sent of your Gondor dids and  that's this more a  river was brought belace ut helder his  moment of these heach back or they before to the          *ag all from the  ansark hi \n",
            "\n",
            " Th\n",
            " Th grassed not there a are to the diven that he weal.   'The find the garning astent   uneards that know make they  mings look a did name, and  we have said it  wish have night that my gowards to  they  \n",
            "\n",
            " wh\n",
            " wht id  as the east, and the better the did not  right after. Zore went at like a go men near way that way of  this flith any, and day, for the way. He says and all a ma`kly to we \felt alone the gleashi \n",
            "\n",
            " ca\n",
            " cae a  with between to mellien, and thoughtter tess aste found.   'Some 8   &own but  have no truads and that is dishrang to was sobar astomen  back, we go were some  off that found, 5            Any la \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YJhgDc2IauPE"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 6: Generate output on a different dataset\n",
        "\n",
        "---\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "* Choose a textual dataset. Here are some [text datasets](https://www.kaggle.com/datasets?tags=14104-text+data%2C13205-text+mining) from Kaggle \n",
        "\n",
        "* Generate some decent looking results and evaluate your model's performance (say what it did well / not so well)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2nkTDSNuOmV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cb90730b-fdbd-46bb-9f9a-98c0c951372b"
      },
      "source": [
        "input_size = n_characters\n",
        "hidden_size = 200\n",
        "output_size = n_characters\n",
        "n_layers = 3\n",
        "  \n",
        "decoder = RNN(input_size=input_size, hidden_size=hidden_size,\n",
        "              output_size=output_size, n_layers=n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n",
        "objective = nn.CrossEntropyLoss()\n",
        "\n",
        "shake_start = time.time()\n",
        "shake_all_losses = []\n",
        "shake_loss_avg = 0\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  \n",
        "  shake_loss_ = train(*shake_random_training_set())       \n",
        "  shake_loss_avg += shake_loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "    print('[%s (%d %d%%) %.4f]' % (time.time() - shake_start, epoch, epoch / n_epochs * 100, shake_loss_))\n",
        "    print(evaluate('Wh', 100, .6), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      shake_all_losses.append(shake_loss_avg / plot_every)\n",
        "      shake_loss_avg = 0"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom GRU\n",
            "[35.80942416191101 (100 2%) 2.4862]\n",
            "Whd sta, and the that therof pou the cot thats meat the tou thoust the bene me lance pon I int  IBA: 4 \n",
            "\n",
            "[71.38724660873413 (200 5%) 2.3574]\n",
            "Whr hen myo loen ano the you in thease be at the me the of he all sho mou saras that in of thou and fo \n",
            "\n",
            "[107.32283234596252 (300 7%) 2.1115]\n",
            "Whn, you and, you that the mose our beestine tor in iso the man the hean the fort thou the for be to l \n",
            "\n",
            "[142.74453139305115 (400 10%) 2.0327]\n",
            "Whd wein to mad of not thou st weat with the dethems the come the thee sir were an the you had soul th \n",
            "\n",
            "[178.83712458610535 (500 12%) 2.0804]\n",
            "Whr the the him.  COLUCELNIO: So comed that you been this the to me hat the dintose the to will for th \n",
            "\n",
            "[215.12371921539307 (600 15%) 2.0591]\n",
            "Wh houl my a your to the dout come and, we the shall day, Thuse hand shtis may of tre, Thet be my dead \n",
            "\n",
            "[250.76790189743042 (700 17%) 1.9315]\n",
            "Wht hath to so kind the hath the rounds the contristances That and that that the dist and and he crome \n",
            "\n",
            "[286.27036237716675 (800 20%) 2.0646]\n",
            "Whts chow canter it stay it it is all thouse the soan a do shouse unce and his leat hastered a contrer \n",
            "\n",
            "[321.76818656921387 (900 22%) 1.9849]\n",
            "Whster my to mast then ento therees my susted for plours, not resces  Seathinest.  KING RICHARD III: T \n",
            "\n",
            "[357.32065176963806 (1000 25%) 1.8753]\n",
            "Whuence of comelous: I se priend shall to hath hirs the prainks of and if and shall no lear the poor f \n",
            "\n",
            "[392.48130202293396 (1100 27%) 1.9318]\n",
            "Whre his to seed and strake I am then couse And dease could shall soul strence a bories.  LUCESTIO: Th \n",
            "\n",
            "[427.6337890625 (1200 30%) 1.7861]\n",
            "Wh praints your wasch be what wellow aid a dear enter.  ROMELO: No my stand hame of not, the are the s \n",
            "\n",
            "[463.2118489742279 (1300 32%) 2.0240]\n",
            "Wht I candery dead of the kind and as hath make the lown, The for the care sil mead, and the wercain   \n",
            "\n",
            "[499.19456028938293 (1400 35%) 1.8677]\n",
            "Wht what will no li6e.  ROMEO: So heir with make hen your bord, what fortuent him, flient, The strom h \n",
            "\n",
            "[534.9666395187378 (1500 37%) 2.0523]\n",
            "Wh% let my betured in me she man a would it it as her here as in a prookJ Tchis lords my him in the pa \n",
            "\n",
            "[570.4939527511597 (1600 40%) 1.8742]\n",
            "Whre it, a father.  CAULETHA: But her parts the came of ender, conther.  GRESTER: I fath like the cour \n",
            "\n",
            "[606.0637559890747 (1700 42%) 1.9110]\n",
            "Whch of the warder the senct a sire And me.  KING HERNCENTO: And with my brows the sany the mimented A \n",
            "\n",
            "[641.5293545722961 (1800 45%) 1.9986]\n",
            "Whre your are to peself.  PETRUCHIO: I this had prooman the proppets, and word.  PETRUCHIO: Then here  \n",
            "\n",
            "[677.1380732059479 (1900 47%) 1.7315]\n",
            "Wht and the bathin and here, and with heartion of him ment may Hould shall be this is the struen forth \n",
            "\n",
            "[712.6024041175842 (2000 50%) 1.7975]\n",
            "Wht, Be the most to this for my son fall the coursely, And my so thou lords the come it maYes of the b \n",
            "\n",
            "[748.1748950481415 (2100 52%) 1.9230]\n",
            "Whre ence, the perower that do father house stare The come not prife a wear be perity you father me fo \n",
            "\n",
            "[783.8486018180847 (2200 55%) 2.0001]\n",
            "Whth and and with the rest, her for the here I their be so thou make the crother the weres for the wil \n",
            "\n",
            "[819.7325193881989 (2300 57%) 2.1036]\n",
            "Whch for hence.  LEONTES: And the part of the forted they will the fail.  DUKE O, the prance and not f \n",
            "\n",
            "[855.1781311035156 (2400 60%) 1.7455]\n",
            "Wht I would thou are of think.  CORIO: Thou sir, what my lord: no should but in hath his should him sh \n",
            "\n",
            "[890.714076757431 (2500 62%) 1.8753]\n",
            "Whnd him, and recous as I asser of my father, and my secution, my come.  ERCENIUS: I art by the with a \n",
            "\n",
            "[926.0801150798798 (2600 65%) 1.9161]\n",
            "Wht eyerer here in propers.  LUCENTIO: And my so my make they waters, and fare the fire, And my were d \n",
            "\n",
            "[961.6209754943848 (2700 67%) 1.8572]\n",
            "Whn a more perin; The hearrs do and my do stand his hail I had hear a could and he honess and from wee \n",
            "\n",
            "[997.0262041091919 (2800 70%) 1.5098]\n",
            "Whm here was with say thee stards, And duke thou ha` I had fathers In he wear to this oft the breather \n",
            "\n",
            "[1032.3258302211761 (2900 72%) 1.9502]\n",
            "Whch hear think.  PETRUCHIO: God the tone for their feent the suck, That you chire were hand since you \n",
            "\n",
            "[1067.8999724388123 (3000 75%) 1.8990]\n",
            "Whrence thinks to shall deed.  ROMEO: You had stand it.  MENENIUS: That must shall the mistrant and si \n",
            "\n",
            "[1103.2512454986572 (3100 77%) 1.8899]\n",
            "Wht warry my long the grow of good, More and to the bading of shen and the more, I he had my gracter h \n",
            "\n",
            "[1138.9374170303345 (3200 80%) 1.5102]\n",
            "Wh, the since in the kings to my lord.  DUKE O son, streath, benour here the prown word, And resce, li \n",
            "\n",
            "[1174.4886548519135 (3300 82%) 1.6907]\n",
            "Wht with not bring so from them that thee, To him thee shall not may a fair of your counting, )et the  \n",
            "\n",
            "[1209.7758843898773 (3400 85%) 1.9736]\n",
            "Whre banish your house that I will God your ebperel hold me time assing. so here and speaks of these s \n",
            "\n",
            "[1245.0680801868439 (3500 87%) 1.5097]\n",
            "Wht word not didon: My lord the first to his before they And that the gone the booth of you are the st \n",
            "\n",
            "[1280.5595779418945 (3600 90%) 1.9635]\n",
            "Wht that be  lors, sir, by then a stones in this dife.  MENENIUS: Their child morrers me and that York \n",
            "\n",
            "[1316.1951806545258 (3700 92%) 1.3932]\n",
            "Whch with here, And garder for the sits of the propousing That an him him for a dear seater and but be \n",
            "\n",
            "[1351.7454087734222 (3800 95%) 1.8380]\n",
            "Whre that wear her giers.  CLIcking.  LADY DUKE O, so in the would to his word them the world dost str \n",
            "\n",
            "[1386.94305062294 (3900 97%) 1.6579]\n",
            "Whrefore in harm and face The word in the been thing of them all to your for were To place of him and  \n",
            "\n",
            "[1422.3103227615356 (4000 100%) 2.0628]\n",
            "Wh, be must time, And he dearthy the heart hear me onMy of the pasting to bean, But both, come, by thy \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzx2W74xyTpV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ea34722d-9a76-4502-87e8-9e2c51af6434"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Shakespeare Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot([i * plot_every for i in range(len(shake_all_losses))], shake_all_losses)\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-J2qKab67hj",
        "colab_type": "text"
      },
      "source": [
        "## Shakespeare Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24bFwNviyERA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "0214f1bc-8826-45f1-e3c0-2f51737d4142"
      },
      "source": [
        "for i in range(10):\n",
        "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
        "  start = random.randint(0,len(start_strings)-1)\n",
        "  print(start_strings[start])\n",
        "#   all_characters.index(string[c])\n",
        "  print(evaluate(start_strings[start], 200), '\\n')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ca\n",
            " canot it to rainst.  HENRY BOLINGART: But to gracious speak her heable me That my lord, I must if it wars nower.  GLOUCESTER: There dound his rest toushal ha/ry semseling him the wear2\tury.  Speen: I sh \n",
            "\n",
            " ca\n",
            " cases me goods not our trust Plight and proces and woman to prewour: Thou starible manglosely, but commanged.  RICHARD: No, so set three welkering to pertuches our looks, And her stain, appear to me awe \n",
            "\n",
            " G\n",
            " Geil say, consusing of the house 3i\"zen, Be made -a world beconsuly to bears as I his beances a sterain.  PETRUCHIO: And your your beding like that Let out, thou is not the miser, And it head, and rung \n",
            "\n",
            " lo\n",
            " lok at with Do tone me and you reads, Part thou are your had Grine throw me, when in the wears to make That part to millenderpelity opon The one you do seaten.  LUCIO: The, good blest in to me, appary.  \n",
            "\n",
            " ra\n",
            " raed to comporign.  PETRUCHIO: I would him is redius do good made To madise and of it instressured This discortal by what would towen your depared That such our wain that we ha_5 is must time That shar, \n",
            "\n",
            " G\n",
            " Gntleman: I had good know me, I come, her hast it well same to me: That at truinge to merein to may held son.  PETRUCHARELL: dos, I haR the better to is tony seeJ blood to them: I soulless you, send fo \n",
            "\n",
            " he\n",
            " heble foul his odes know the hand of your lord thou areming in me=ning of such me: And heady bloodZ&LI hood marcoused The beablour with that a primpess droshed In then warised their pince anding then.   \n",
            "\n",
            " G\n",
            " Gse is: the beary wembrity.  BISHER: It mather uncle secuntion: The hearts componted to be mothers it I depon are In to ditter her proson, and percesson, But this blity of his wared The partices had be \n",
            "\n",
            " I \n",
            " I ail double heir ways to me fliest ears of speet{end bock it mear/y.  Prompeful: I twill of heine shall it my fruth the honour to parted to must one hea: I home her well her and must bence bed their ma \n",
            "\n",
            " ca\n",
            " cae their such, In well and great in than my ore.  RICHARD: To all with hell, bes the haic, Some a may our send by healsE Thou forth the brother his fleshir3 me peacefulish ond make, thou Harcies The he \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkf6B8Fv0nPg",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "My Model is doing pretty well at organizing general sentence structure. It's even doing fairly well with punctuation. On the shakespeare dataset, it is creating names (in all caps), followed by a colon to denote a person speaking. This is pretty impressive considering that we are only producing one character at a time.\n",
        "\n",
        "Although the model is doing fairly well at creating actual words, including some punctuation and general sentence structure, it isn't really producing coherent sentences. Maybe with enough training, we may be able to see better results. In addition, it tends to leave out letters in words directly after priming. For example:\n",
        "\n",
        "* G -> Gntleman\n",
        "\n",
        "* Wh -> Whrefore\n",
        "\n",
        "* Wh -> Whch\n",
        "\n",
        "The priming seems to work, but it's not perfect directly after priming. The model needs a little more \"warm-up\" before it produces better words and sentence structure."
      ]
    }
  ]
}